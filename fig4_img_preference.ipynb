{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "from scipy.ndimage import label, sum as ndi_sum\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.image\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "folder_path = \"/path/to/your/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDANN response to synthetic shape & texture (ST) images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load TDANN final model from checkpoint\n",
    "def load_model_from_checkpoint(checkpoint_path: str, device: str):\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "    # drop the FC layer\n",
    "    model.fc = nn.Identity()\n",
    "    # load weights\n",
    "    ckpt = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    state_dict = ckpt[\"classy_state_dict\"][\"base_model\"][\"model\"][\"trunk\"]\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"base_model\") and \"fc.\" not in k:\n",
    "            remainder = k.split(\"base_model.\")[-1]\n",
    "            new_state_dict[remainder] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    # freeze all weights\n",
    "    for param in model.parameters(): param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "# obtain TDANN unit activation values to shape & texture images\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"available device: {DEVICE}\")\n",
    "checkpoint_path = folder_path + \"Fig4/TDANNfinal.torch\"\n",
    "model = load_model_from_checkpoint(checkpoint_path, DEVICE)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "# image preprocessing\n",
    "preprocess = torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)), torchvision.transforms.ToTensor()])\n",
    "# Define the layers of interest\n",
    "layers_of_interest = {\n",
    "    \"layer3.1\": model.layer3[1], # (256, 14, 14)\n",
    "    \"layer4.0\": model.layer4[0], # (512, 7, 7)\n",
    "}\n",
    "# to store all layers' all channels' central units' activation values to 50k images\n",
    "num_imgs = 366 + 448\n",
    "units = [np.zeros((256, 14, 14, num_imgs)), np.zeros((512, 7, 7, num_imgs))] \n",
    "for i in tqdm(range(num_imgs), desc=\"imgs...\", disable=False):\n",
    "    activations = {} # Dictionary to store activations\n",
    "    # Hook function to capture activations\n",
    "    def hook_fn(name):\n",
    "        def hook(module, input, output):\n",
    "            activations[name] = output.detach()  # Store detached tensor\n",
    "        return hook\n",
    "    # Register hooks\n",
    "    hooks = []\n",
    "    for name, layer in layers_of_interest.items():\n",
    "        hook = layer.register_forward_hook(hook_fn(name))\n",
    "        hooks.append(hook)\n",
    "    # Load and preprocess an image\n",
    "    if i < 366: # shape images\n",
    "        image_path = folder_path + \"V4DT/ShapeStimuli/Shape_\" + str(int(i + 1)) + \".jpg\"\n",
    "        input_tensor = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0) # [:, :3, :, :] # torch.Size([1, 3, 224, 224])\n",
    "    else: # texture images\n",
    "        image_path = folder_path + \"V4DT/TextureStimuli/Texture_\" + str(int(i - 366 + 1)) + \".jpg\"\n",
    "        input_tensor = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0) # .repeat(1, 3, 1, 1) # torch.Size([1, 3, 224, 224])\n",
    "    input_tensor = input_tensor.to(DEVICE, dtype=torch.float) # move to cuda\n",
    "    with torch.no_grad(): model(input_tensor) # Forward pass\n",
    "    for hook in hooks: hook.remove() # Remove hooks (to prevent memory issues)\n",
    "    for idx, (layer, activation) in enumerate(activations.items()):\n",
    "        activation = activation.detach().cpu().numpy().squeeze()\n",
    "        units[idx][:, :, :, i] = activation\n",
    "        # print(f\"Iteration {idx}: {layer} activation shape: {activation.shape}\")\n",
    "    # break # for testing purposes\n",
    "\n",
    "# save activation values\n",
    "np.savez(folder_path + \"Fig4/ShapeTextureActivation_TDANN.npz\", layer31=units[0], layer40=units[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDANN responses to theoretical receptive field masked ST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating receptive field shifted images based on CNN structural feature map location\n",
    "def get_receptive_field_box(feature_h, feature_w, rf_size=211, jump=16, start=0.5):\n",
    "    \"\"\"\n",
    "    Compute the ideal receptive field box (in floating point) in the target image space \n",
    "    for a given feature map location.\n",
    "    Parameters:\n",
    "      feature_h (int): vertical index of the feature.\n",
    "      feature_w (int): horizontal index of the feature.\n",
    "      rf_size (int): receptive field size (e.g., 147 for layer3.0 of ResNet18).\n",
    "      jump (int): the effective stride (e.g., 16 for layer3.0).\n",
    "      start (float): the offset for the center of the (0,0) feature.\n",
    "    Returns: left, top, right, bottom (floats): the coordinates of the ideal receptive field box.\n",
    "    \"\"\"\n",
    "    center_x = start + feature_w * jump\n",
    "    center_y = start + feature_h * jump\n",
    "    half_rf = rf_size / 2.0\n",
    "    left   = center_x - half_rf\n",
    "    top    = center_y - half_rf\n",
    "    right  = center_x + half_rf\n",
    "    bottom = center_y + half_rf\n",
    "    return left, top, right, bottom\n",
    "\n",
    "def visualize_rf_on_full_canvas(input_image, feature_h, feature_w, image_size=224, rf_size=211, jump=16, start=0.5):\n",
    "    \"\"\"\n",
    "    Create a new image (of dimensions image_size x image_size) that is white everywhere \n",
    "    except for the receptive field of the given feature unit. In this version, we assume\n",
    "    the original input image may not be 224x224. Therefore, we first resize the original \n",
    "    image to the given image_size using bilinear interpolation, then compute the receptive \n",
    "    field box and copy only the valid, in-bound portions from the resized image.\n",
    "    The valid pixels from the resized image are pasted into the canvas at the same coordinates,\n",
    "    so that if the receptive field is, for example, at the top-left, you see the top-left part of the\n",
    "    resized image in the corresponding canvas region.\n",
    "    Parameters:\n",
    "      input_image (PIL.Image): the original input image (can be any size).\n",
    "      feature_h, feature_w (int): indices for the feature map location.\n",
    "      image_size (int): the target size of the image (e.g., 224).\n",
    "      rf_size (int): the ideal receptive field size.\n",
    "      jump (int): effective stride.\n",
    "      start (float): center offset for the (0,0) feature.\n",
    "    Returns: canvas (PIL.Image): a new image with original image content only within the computed receptive field, with the rest of the image white.\n",
    "    \"\"\"\n",
    "    # First, resize the original image to the target size using bilinear interpolation.\n",
    "    resized_image = input_image.resize((image_size, image_size), resample=Image.BILINEAR)\n",
    "    # Compute the ideal receptive field box in the target coordinate system.\n",
    "    # Note: This box is computed in floating point and may extend outside [0, image_size].\n",
    "    left, top, right, bottom = get_receptive_field_box(feature_h, feature_w, rf_size, jump, start)\n",
    "    # Compute the valid (overlapping) area between the receptive field and the image boundaries.\n",
    "    valid_left = max(np.floor(left), 0)\n",
    "    valid_top  = max(np.floor(top), 0)\n",
    "    valid_right = min(np.ceil(right), image_size)\n",
    "    valid_bottom = min(np.ceil(bottom), image_size)\n",
    "    # Define integer coordinates for cropping from the resized image.\n",
    "    crop_box = (int(valid_left), int(valid_top), int(valid_right), int(valid_bottom))\n",
    "    # Extract the valid patch directly from the resized image.\n",
    "    valid_patch = resized_image.crop(crop_box)\n",
    "    # Create a white canvas of size image_size x image_size.\n",
    "    canvas = Image.new(\"RGB\", (image_size, image_size), \"white\")\n",
    "    # Paste the valid patch into the canvas at the exact coordinates.\n",
    "    # In this way, if the receptive field is on the top-left of the image,\n",
    "    # the top-left portion of the resized image is copied to the top-left of the canvas.\n",
    "    canvas.paste(valid_patch, (int(valid_left), int(valid_top)))\n",
    "    return canvas\n",
    "\n",
    "def visualize_rf_entire_image_rescaled(input_image, feature_h, feature_w,\n",
    "                                       image_size=224, rf_size=211, jump=16, start=0.5):\n",
    "    \"\"\"\n",
    "    Create a new 224x224 white canvas in which the entire original image (after bilinear interpolation)\n",
    "    is rescaled to exactly fill the valid receptive field region for a given feature unit.\n",
    "    In other words, the valid overlapping area between the ideal receptive field and the canvas\n",
    "    is determined, and then the entire original image is resized to that region's size (which can be rectangular)\n",
    "    and pasted at the corresponding location. Thus, even for a top-left unit (whose receptive field\n",
    "    covers only the top-left portion of the canvas), the entire original image is rescaled to fit that area.\n",
    "    Parameters:\n",
    "      input_image (PIL.Image.Image): the original image (of any size).\n",
    "      feature_h (int): vertical index of the feature map unit.\n",
    "      feature_w (int): horizontal index of the feature map unit.\n",
    "      image_size (int): size of the target canvas (e.g., 224).\n",
    "      rf_size (int): ideal receptive field size.\n",
    "      jump (int): effective stride.\n",
    "      start (float): offset for the (0,0) unit.\n",
    "    Returns:\n",
    "      canvas (PIL.Image.Image): a 224x224 image with a white background and the entire original image rescaled and pasted into the receptive field region.\n",
    "    \"\"\"\n",
    "    # Compute the ideal receptive field box in the canvas coordinate system.\n",
    "    left, top, right, bottom = get_receptive_field_box(feature_h, feature_w, rf_size, jump, start)\n",
    "    # Convert the floating point coordinates to integer positions for pasting.\n",
    "    paste_left = int(np.floor(left))\n",
    "    paste_top  = int(np.floor(top))\n",
    "    paste_right = int(np.ceil(right))\n",
    "    paste_bottom = int(np.ceil(bottom))\n",
    "    # Determine the valid area (clip the ideal box with the canvas boundaries [0, image_size])\n",
    "    clip_left = max(paste_left, 0)\n",
    "    clip_top  = max(paste_top, 0)\n",
    "    clip_right = min(paste_right, image_size)\n",
    "    clip_bottom = min(paste_bottom, image_size)\n",
    "    # The size of the region on the canvas into which we will paste the image.\n",
    "    region_width = clip_right - clip_left\n",
    "    region_height = clip_bottom - clip_top\n",
    "    # Resize the entire original image to exactly the size of this region.\n",
    "    resized_entire_image = input_image.resize((region_width, region_height), resample=Image.BILINEAR)\n",
    "    # Create a white canvas of size image_size x image_size.\n",
    "    canvas = Image.new(\"RGB\", (image_size, image_size), \"white\")\n",
    "    # Paste the rescaled entire image into the computed receptive field region.\n",
    "    canvas.paste(resized_entire_image, (clip_left, clip_top))\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked ST image examples visualization\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "image_path = folder_path + \"V4DT/TextureStimuli/Texture_156.jpg\"\n",
    "feature_h = [0, 3, 6]\n",
    "feature_w = [0, 3, 6]\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        img = visualize_rf_on_full_canvas(img, feature_h[i], feature_w[j], image_size=224, rf_size=307, jump=32, start=0.5) # <class 'PIL.Image.Image'>\n",
    "        # img.show()  # To view the image.\n",
    "        axes[i, j].imshow(img)\n",
    "        axes[i, j].axis('off')  # Hide axis ticks\n",
    "        rect = Rectangle(\n",
    "                (0, 0), 1, 1,  # coordinates in axis fraction\n",
    "                transform=axes[i, j].transAxes,\n",
    "                linewidth=2,\n",
    "                edgecolor='black',\n",
    "                facecolor='none'\n",
    "            )\n",
    "        axes[i, j].add_patch(rect)\n",
    "plt.tight_layout()\n",
    "# save the figure\n",
    "plt.savefig(folder_path + \"Fig4/TDANN40_image_rf.png\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_resnet18_rf_params(layer_tag):\n",
    "    \"\"\"\n",
    "    Compute and print the step-by-step receptive field parameters for a ResNet18 model.\n",
    "    \n",
    "    Supported layer tags:\n",
    "      - \"layer31\": corresponds to the output of the second block in layer3 (i.e. layer3.1)\n",
    "      - \"layer40\": corresponds to the output of the first block in layer4 (i.e. layer4.0)\n",
    "      \n",
    "    The function prints the intermediate steps and returns a dictionary containing:\n",
    "      - 'rf_size': final receptive field size in pixels.\n",
    "      - 'jump': effective stride (jump) between adjacent units.\n",
    "      - 'start': center offset for the (0,0) feature (assumed to be 0.5).\n",
    "    \"\"\"\n",
    "    # Initial values at the input (each pixel sees itself):\n",
    "    r, j = 1, 1  # receptive field and jump.\n",
    "    start = 0.5  # Assume the center of the first pixel is at 0.5\n",
    "    print(\"Initial input: receptive field = 1, jump = 1, start = 0.5\")\n",
    "    # --- conv1: 7×7 conv, stride 2, pad 3 ---\n",
    "    k, s = 7, 2\n",
    "    r = r + (k - 1) * j      # 1 + 6*1 = 7\n",
    "    j = j * s                # 1 * 2 = 2\n",
    "    print(\"After conv1 (7x7, stride 2): receptive field = {}, jump = {}\".format(r, j))\n",
    "    # --- maxpool: 3×3, stride 2, pad 1 ---\n",
    "    k, s = 3, 2\n",
    "    r = r + (k - 1) * j      # 7 + 2*2 = 11\n",
    "    j = j * s                # 2 * 2 = 4\n",
    "    print(\"After maxpool (3x3, stride 2): receptive field = {}, jump = {}\".format(r, j))\n",
    "    # --- Layer1: Two blocks (4 convolutions total), each conv: 3×3, stride 1 ---\n",
    "    for i in range(4):\n",
    "        k, s = 3, 1\n",
    "        r = r + (k - 1) * j  # Each conv adds 2*j.\n",
    "        print(\"After Layer1 conv {}: receptive field = {}, jump = {}\".format(i+1, r, j))\n",
    "    # At end of Layer1: r = 43, j = 4.\n",
    "    # --- Layer2: Two blocks ---\n",
    "    # Block1, Conv1 (3×3, stride 2)\n",
    "    k, s = 3, 2\n",
    "    r = r + (k - 1) * j   # 43 + 2*4 = 51\n",
    "    j = j * s             # 4 * 2 = 8\n",
    "    print(\"After Layer2 Block1 Conv1 (3x3, stride 2): receptive field = {}, jump = {}\".format(r, j))\n",
    "    # Block1, Conv2 (3×3, stride 1)\n",
    "    k, s = 3, 1\n",
    "    r = r + (k - 1) * j   # 51 + 2*8 = 67\n",
    "    print(\"After Layer2 Block1 Conv2 (3x3, stride 1): receptive field = {}, jump = {}\".format(r, j))\n",
    "    # Block2, Conv1 (3×3, stride 1)\n",
    "    k, s = 3, 1\n",
    "    r = r + (k - 1) * j   # 67 + 2*8 = 83\n",
    "    print(\"After Layer2 Block2 Conv1 (3x3, stride 1): receptive field = {}, jump = {}\".format(r, j))\n",
    "    # Block2, Conv2 (3×3, stride 1)\n",
    "    k, s = 3, 1\n",
    "    r = r + (k - 1) * j   # 83 + 2*8 = 99\n",
    "    print(\"After Layer2 Block2 Conv2 (3x3, stride 1): receptive field = {}, jump = {}\".format(r, j))\n",
    "    # Now: r = 99, j = 8.\n",
    "    # --- Layer3.0: First block of Layer3 ---\n",
    "    # Conv1: 3×3, stride 2\n",
    "    k, s = 3, 2\n",
    "    r = r + (k - 1) * j   # 99 + 2*8 = 115\n",
    "    j = j * s             # 8 * 2 = 16\n",
    "    print(\"After Layer3.0 Conv1 (3x3, stride 2): receptive field = {}, jump = {}\".format(r, j))\n",
    "    # Conv2: 3×3, stride 1\n",
    "    k, s = 3, 1\n",
    "    r = r + (k - 1) * j   # 115 + 2*16 = 147\n",
    "    print(\"After Layer3.0 Conv2 (3x3, stride 1): receptive field = {}, jump = {}\".format(r, j))\n",
    "    # Now: for layer3.0, r = 147, j = 16.\n",
    "    # --- Layer3.1: Second block of Layer3 (always computed for both tags) ---\n",
    "    # Conv1: 3×3, stride 1\n",
    "    k, s = 3, 1\n",
    "    r = r + (k - 1) * j   # 147 + 2*16 = 147 + 32 = 179\n",
    "    print(\"After Layer3.1 Conv1 (3x3, stride 1): receptive field = {}, jump = {}\".format(r, j))\n",
    "    # Conv2: 3×3, stride 1\n",
    "    k, s = 3, 1\n",
    "    r = r + (k - 1) * j   # 179 + 2*16 = 179 + 32 = 211\n",
    "    print(\"After Layer3.1 Conv2 (3x3, stride 1): receptive field = {}, jump = {}\".format(r, j))\n",
    "    # Now: for layer3.1, r = 211, j = 16.\n",
    "    if layer_tag == \"layer31\":\n",
    "        params = {\"rf_size\": r, \"jump\": j, \"start\": start}\n",
    "        print(\"\\nFinal parameters for layer3.1 (layer31):\", params)\n",
    "        return params\n",
    "    elif layer_tag == \"layer40\":\n",
    "        # --- Layer4.0: First block of Layer4 ---\n",
    "        # Conv1: 3×3, stride 2\n",
    "        k, s = 3, 2\n",
    "        r = r + (k - 1) * j   # 211 + 2*16 = 211 + 32 = 243\n",
    "        j = j * s             # 16 * 2 = 32\n",
    "        print(\"After Layer4.0 Conv1 (3x3, stride 2): receptive field = {}, jump = {}\".format(r, j))\n",
    "        # Conv2: 3×3, stride 1\n",
    "        k, s = 3, 1\n",
    "        r = r + (k - 1) * j   # 243 + 2*32 = 243 + 64 = 307\n",
    "        print(\"After Layer4.0 Conv2 (3x3, stride 1): receptive field = {}, jump = {}\".format(r, j))\n",
    "        params = {\"rf_size\": r, \"jump\": j, \"start\": start}\n",
    "        print(\"\\nFinal parameters for layer4.0 (layer40):\", params)\n",
    "        return params\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported layer tag. Use 'layer31' or 'layer40'.\")\n",
    "\n",
    "# Below are the computed theoretical receptive field parameters for ResNet18's layer3.0 and layer4.0.\n",
    "print(\"=== Computation for layer3.1 (layer31) ===\")\n",
    "params_layer31 = compute_resnet18_rf_params(\"layer31\") # {'rf_size': 211, 'jump': 16, 'start': 0.5}\n",
    "print(\"\\n=== Computation for layer4.0 (layer40) ===\")\n",
    "params_layer40 = compute_resnet18_rf_params(\"layer40\") # {'rf_size': 307, 'jump': 32, 'start': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load TDANN final model from checkpoint\n",
    "def load_model_from_checkpoint(checkpoint_path: str, device: str):\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "    # drop the FC layer\n",
    "    model.fc = nn.Identity()\n",
    "    # load weights\n",
    "    ckpt = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    state_dict = ckpt[\"classy_state_dict\"][\"base_model\"][\"model\"][\"trunk\"]\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"base_model\") and \"fc.\" not in k:\n",
    "            remainder = k.split(\"base_model.\")[-1]\n",
    "            new_state_dict[remainder] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    # freeze all weights\n",
    "    for param in model.parameters(): param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "# obtain TDANN unit activation values to receptive field masked shape & texture images\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"available device: {DEVICE}\")\n",
    "checkpoint_path = folder_path + \"Fig4/TDANNfinal.torch\"\n",
    "model = load_model_from_checkpoint(checkpoint_path, DEVICE)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "# image preprocessing\n",
    "preprocess = torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)), torchvision.transforms.ToTensor()])\n",
    "# Define the layers of interest\n",
    "layers_of_interest = {\n",
    "    \"layer3.1\": model.layer3[1], # (256, 14, 14)\n",
    "    \"layer4.0\": model.layer4[0], # (512, 7, 7)\n",
    "}\n",
    "# to store all layers' all channels' central units' activation values to 50k images\n",
    "num_imgs = 366 + 448\n",
    "units = [np.zeros((256, 14, 14, num_imgs)), np.zeros((512, 7, 7, num_imgs))] \n",
    "# TDANN layer4.0\n",
    "for feature_h in range(7):\n",
    "    for feature_w in tqdm(range(7), desc=\"layer4.0 feature_w...\", disable=False):\n",
    "        for i in tqdm(range(num_imgs), desc=\"imgs...\", disable=True):\n",
    "            activations = {} # Dictionary to store activations\n",
    "            # Hook function to capture activations\n",
    "            def hook_fn(name):\n",
    "                def hook(module, input, output):\n",
    "                    activations[name] = output.detach()  # Store detached tensor\n",
    "                return hook\n",
    "            # Register hook\n",
    "            hook = model.layer4[0].register_forward_hook(hook_fn(\"layer4.0\"))\n",
    "            # Load and preprocess an image\n",
    "            if i < 366: # shape images\n",
    "                image_path = folder_path + \"V4DT/ShapeStimuli/Shape_\" + str(int(i + 1)) + \".jpg\"\n",
    "                img = Image.open(image_path).convert(\"RGB\")\n",
    "                img = visualize_rf_entire_image_rescaled(img, feature_h, feature_w, image_size=224, rf_size=307, jump=32, start=0.5)\n",
    "            else: # texture images\n",
    "                image_path = folder_path + \"V4DT/TextureStimuli/Texture_\" + str(int(i - 366 + 1)) + \".jpg\"\n",
    "                img = Image.open(image_path).convert(\"RGB\")\n",
    "                img = visualize_rf_entire_image_rescaled(img, feature_h, feature_w, image_size=224, rf_size=307, jump=32, start=0.5)\n",
    "            input_tensor = preprocess(img).unsqueeze(0).to(DEVICE, dtype=torch.float) # move to cuda\n",
    "            with torch.no_grad(): model(input_tensor) # Forward pass\n",
    "            hook.remove() # Remove hooks (to prevent memory issues)\n",
    "            for idx, (layer, activation) in enumerate(activations.items()):\n",
    "                activation = activation.detach().cpu().numpy().squeeze()\n",
    "                units[idx + 1][:, feature_h, feature_w, i] = activation[:, feature_h, feature_w] # store activation values\n",
    "            assert idx == 0\n",
    "# TDANN layer3.1\n",
    "print(\"layer3.1:\")\n",
    "for feature_h in range(14):\n",
    "    for feature_w in tqdm(range(14), desc=\"layer3.1 feature_w...\", disable=False):\n",
    "        for i in tqdm(range(num_imgs), desc=\"imgs...\", disable=True):\n",
    "            activations = {} # Dictionary to store activations\n",
    "            # Hook function to capture activations\n",
    "            def hook_fn(name):\n",
    "                def hook(module, input, output):\n",
    "                    activations[name] = output.detach()  # Store detached tensor\n",
    "                return hook\n",
    "            # Register hook\n",
    "            hook = model.layer3[1].register_forward_hook(hook_fn(\"layer3.1\"))\n",
    "            # Load and preprocess an image\n",
    "            if i < 366: # shape images\n",
    "                image_path = folder_path + \"V4DT/ShapeStimuli/Shape_\" + str(int(i + 1)) + \".jpg\"\n",
    "                img = Image.open(image_path).convert(\"RGB\")\n",
    "                img = visualize_rf_entire_image_rescaled(img, feature_h, feature_w, image_size=224, rf_size=211, jump=16, start=0.5)\n",
    "                # img.show()  # To view the image.\n",
    "            else: # texture images\n",
    "                image_path = folder_path + \"V4DT/TextureStimuli/Texture_\" + str(int(i - 366 + 1)) + \".jpg\"\n",
    "                img = Image.open(image_path).convert(\"RGB\")\n",
    "                img = visualize_rf_entire_image_rescaled(img, feature_h, feature_w, image_size=224, rf_size=211, jump=16, start=0.5)\n",
    "            input_tensor = preprocess(img).unsqueeze(0).to(DEVICE, dtype=torch.float) # move to cuda\n",
    "            with torch.no_grad(): model(input_tensor) # Forward pass\n",
    "            hook.remove() # Remove hooks (to prevent memory issues)\n",
    "            for idx, (layer, activation) in enumerate(activations.items()):\n",
    "                activation = activation.detach().cpu().numpy().squeeze()\n",
    "                units[idx][:, feature_h, feature_w, i] = activation[:, feature_h, feature_w] # store activation values\n",
    "            assert idx == 0\n",
    "\n",
    "# save activation values\n",
    "np.savez(folder_path + \"Fig4/ShapeTextureActivation_TDANN_rfs.npz\", layer31=units[0], layer40=units[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDANN responses to estimated receptive field masked ST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_scaled_image_on_feature_location(\n",
    "    input_image: Image.Image,\n",
    "    feature_h: int,\n",
    "    feature_w: int,\n",
    "    box_size: int,\n",
    "    canvas_size: int = 224,\n",
    "    feat_map_size: int = 14\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Create a white canvas of size canvas_size x canvas_size, then:\n",
    "      1. Resize the entire input_image to box_size x box_size via bilinear interpolation.\n",
    "      2. Compute the top-left corner (x, y) for a feature unit at (feature_h, feature_w)\n",
    "         in a feat_map_size x feat_map_size grid, so that:\n",
    "           - (0, 0) → top-left of canvas\n",
    "           - (0, feat_map_size-1) → top-right\n",
    "           - (feat_map_size-1, 0) → bottom-left\n",
    "           - (feat_map_size-1, feat_map_size-1) → bottom-right\n",
    "      3. Paste the resized image into that slot on the white canvas.\n",
    "    Parameters:\n",
    "        input_image: PIL.Image.Image, any size.\n",
    "        feature_h: int, row index in [0, feat_map_size-1].\n",
    "        feature_w: int, col index in [0, feat_map_size-1].\n",
    "        box_size: int, side length of the square slot.\n",
    "        canvas_size: int, size of the white canvas (default 224).\n",
    "        feat_map_size: int, spatial size of the feature map (default 14).\n",
    "    Returns:\n",
    "        A new PIL.Image.Image (canvas_size x canvas_size) with the resized input pasted into the slot corresponding to (feature_h, feature_w).\n",
    "    \"\"\"\n",
    "    # 1) Prepare canvas and resized patch\n",
    "    canvas = Image.new(\"RGB\", (canvas_size, canvas_size), \"white\")\n",
    "    patch = input_image.resize((box_size, box_size), resample=Image.BILINEAR)\n",
    "    # 2) Compute paste coordinates\n",
    "    max_offset = canvas_size - box_size\n",
    "    # Normalize feature indices to [0..1], then scale to [0..max_offset]\n",
    "    x = round((feature_w / (feat_map_size - 1)) * max_offset)\n",
    "    y = round((feature_h / (feat_map_size - 1)) * max_offset)\n",
    "    # 3) Paste and return\n",
    "    canvas.paste(patch, (x, y))\n",
    "    return canvas\n",
    "\n",
    "# load TDANN final model from checkpoint\n",
    "def load_model_from_checkpoint(checkpoint_path: str, device: str):\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "    # drop the FC layer\n",
    "    model.fc = nn.Identity()\n",
    "    # load weights\n",
    "    ckpt = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    state_dict = ckpt[\"classy_state_dict\"][\"base_model\"][\"model\"][\"trunk\"]\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"base_model\") and \"fc.\" not in k:\n",
    "            remainder = k.split(\"base_model.\")[-1]\n",
    "            new_state_dict[remainder] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    # freeze all weights\n",
    "    for param in model.parameters(): param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "# obtain TDANN unit activation values to receptive field masked shape & texture images\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"available device: {DEVICE}\")\n",
    "checkpoint_path = folder_path + \"Fig4/TDANNfinal.torch\"\n",
    "model = load_model_from_checkpoint(checkpoint_path, DEVICE)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "# image preprocessing\n",
    "preprocess = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "# to store targeted layers' all units' activation values to some images\n",
    "num_imgs = 366 + 448\n",
    "units_activation = np.zeros((256, 14, 14, num_imgs))\n",
    "# load estimated receptive field size\n",
    "scale = 1\n",
    "Sigma = np.array(h5py.File(folder_path + \"Fig4/layer31/Sigma.mat\")[\"Sigma\"]) # 2D gaussian fitted receptive field parametersof shape (2, 256)\n",
    "Sigma = scale * np.mean(Sigma, axis=0) # mean receptive field size: (2, 256) -> (256,)\n",
    "assert len(Sigma) == 256\n",
    "# TDANN layer3.1\n",
    "print(\"layer3.1:\")\n",
    "for h in range(14):\n",
    "    for w in tqdm(range(14), desc=\"layer3.1 feature_w...\", disable=True):\n",
    "        for c in tqdm(range(256), desc=\"given h, w, for all channels...\", disable=False): # iterate through all channels\n",
    "            for i in tqdm(range(num_imgs), desc=\"imgs...\", disable=True):\n",
    "                # Load and preprocess an image\n",
    "                if i < 366: # shape images\n",
    "                    image_path = folder_path + \"V4DT/ShapeStimuli/Shape_\" + str(int(i + 1)) + \".jpg\"\n",
    "                    img = Image.open(image_path).convert(\"RGB\")\n",
    "                    img = visualize_scaled_image_on_feature_location(img, feature_h=h, feature_w=w, box_size=int(Sigma[c]), canvas_size=224, feat_map_size=14)\n",
    "                    # img.show()  # To view the image.\n",
    "                else: # texture images\n",
    "                    image_path = folder_path + \"V4DT/TextureStimuli/Texture_\" + str(int(i - 366 + 1)) + \".jpg\"\n",
    "                    img = Image.open(image_path).convert(\"RGB\")\n",
    "                    img = visualize_scaled_image_on_feature_location(img, feature_h=h, feature_w=w, box_size=int(Sigma[c]), canvas_size=224, feat_map_size=14)\n",
    "                x = preprocess(img).unsqueeze(0).to(DEVICE, dtype=torch.float) # move to cuda\n",
    "                x = model.conv1(x)\n",
    "                x = model.bn1(x)\n",
    "                x = model.relu(x)\n",
    "                x = model.maxpool(x)\n",
    "                x = model.layer1(x)\n",
    "                x = model.layer2(x)\n",
    "                x = model.layer3(x) # up until layer3.1\n",
    "                # x = model.layer4[0](x) # up until layer4.0\n",
    "                units_activation[c, h, w, i] = x[:, c, h, w] # store activation values\n",
    "\n",
    "# save activation values\n",
    "np.save(folder_path + \"Fig4/ShapeTextureActivation_TDANN31_1erf.npy\", units_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked ST image according to estimated receptive field examples visualization\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "image_path = \"/Users/dunhan/Desktop/topoV4/V4DT/TextureStimuli/Texture_156.jpg\"\n",
    "feature_hs = [0, 7, 13]\n",
    "feature_ws = [0, 7, 13]\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        img = visualize_scaled_image_on_feature_location(img, feature_h=feature_hs[i], feature_w=feature_ws[j], box_size=60, canvas_size=224, feat_map_size=14)\n",
    "        # img.show()  # To view the image.\n",
    "        axes[i, j].imshow(img)\n",
    "        axes[i, j].axis('off')  # Hide axis ticks\n",
    "        rect = Rectangle(\n",
    "                (0, 0), 1, 1,  # coordinates in axis fraction\n",
    "                transform=axes[i, j].transAxes,\n",
    "                linewidth=2,\n",
    "                edgecolor='black',\n",
    "                facecolor='none'\n",
    "            )\n",
    "        axes[i, j].add_patch(rect)\n",
    "plt.tight_layout()\n",
    "# save the figure\n",
    "plt.savefig(folder_path + \"Fig4/TDANN31_image_erf.png\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDANN ST image preference map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of TDANN final network & positions, into a 2D 60 by 60 gridded map\n",
    "# combined_features = np.load(folder_path + \"Fig4/TDANN31_Rsp.npy\") # (50000, 25088) for layer4.1; (50000, 50176) for layer3.1\n",
    "# combined_features = np.load(folder_path + \"Fig4/ShapeTextureActivation_TDANN_rf.npz\")[\"layer40\"].reshape(-1, 814).T\n",
    "combined_features = np.load(folder_path + \"Fig4/TDANN40_2erf.npy\").reshape(-1, 814).T\n",
    "positions = np.load(folder_path + \"Fig4/TDANNfinal_positions/layer4.0.npz\")[\"coordinates\"]\n",
    "cortical_size = max(positions[:, 0]) - min(positions[:, 0]) # define the length of the 2D plane\n",
    "grid_num = 60 # each grid contains the most-preferred 9 images by the mean within-grid-units' response\n",
    "stmap = np.ones((grid_num, grid_num)) # 1 for shape preferring, 2 for texture preferring\n",
    "grids_count = int(grid_num ** 2)\n",
    "num_imgs_each_side = 3 # number of images on each side of the grid, total number of images in a grid should be squared\n",
    "# define the size of a single image\n",
    "img_size = 30\n",
    "line_width = 5\n",
    "# create a blank map of black color (R=0, G=0, B=0)\n",
    "map = np.zeros((grid_num * (img_size*3 + line_width) + line_width, \n",
    "                grid_num * (img_size*3 + line_width) + line_width, 3))\n",
    "# To store each grid's agregated response to all 50K images, with an additional roi\n",
    "# TDANN41_weight = np.zeros((grid_num, grid_num, 50000))\n",
    "# roi = np.zeros((grid_num, grid_num))\n",
    "for i in tqdm(range(grid_num), desc=\"map initialization...\"):\n",
    "    for j in range(grid_num):\n",
    "        # first find all units in this current grid\n",
    "        xmin_cortex = cortical_size / grid_num * i\n",
    "        xmax_cortex = cortical_size / grid_num * (i + 1)\n",
    "        ymin_cortex = cortical_size / grid_num * j\n",
    "        ymax_cortex = cortical_size / grid_num * (j + 1)\n",
    "        # find all units in this current grid\n",
    "        units_within_grid_indeices = np.where((positions[:, 0] >= xmin_cortex) & (positions[:, 0] < xmax_cortex) & (positions[:, 1] >= ymin_cortex) & (positions[:, 1] < ymax_cortex))[0]\n",
    "        if len(units_within_grid_indeices) > 0:\n",
    "            # roi[i, j] = 1\n",
    "            # compute the mean response of all units (neurons) within this grid to all 50K images (into a row vector)\n",
    "            mean_responses = np.mean(combined_features[:, units_within_grid_indeices], axis=1).T\n",
    "            # TDANN41_weight[i, j, :] = mean_responses\n",
    "            image_label = np.arange(814) + 1 # 1-indexed image names\n",
    "            # sort the mean responses (from small to large) and the image_label according to the order of mean responses\n",
    "            mean_responses, image_label = zip(*sorted(zip(mean_responses,image_label)))\n",
    "            image_label = np.flip(image_label[-int(num_imgs_each_side ** 2):]) # take the top nine images with largest mean response\n",
    "\n",
    "            # locate the top left corner of the current grid in the map\n",
    "            x = i * (img_size*3 + line_width) + line_width\n",
    "            y = j * (img_size*3 + line_width) + line_width\n",
    "            \n",
    "            # fill the map's current grid with the selected nine images\n",
    "            texture_img_count = 0\n",
    "            for row in range(3):\n",
    "                for col in range(3):\n",
    "                    # load the image\n",
    "                    if image_label[row*3+col] <= 366:\n",
    "                        path = folder_path + \"V4DT/ShapeStimuli/Shape_\"  + str(int(image_label[row*3+col])) + \".jpg\" # the image name is 1-indexed\n",
    "                    else:\n",
    "                        path = folder_path + \"V4DT/TextureStimuli/Texture_\" + str(int(image_label[row*3+col] - 366)) + \".jpg\"\n",
    "                        texture_img_count += 1\n",
    "                    img = np.array(Image.open(path).convert(\"RGB\"))\n",
    "                    # img = cv2.resize(img, (30, 30), interpolation=cv2.INTER_LINEAR)\n",
    "                    img = resize(img, (img_size, img_size, 3), anti_aliasing=True) # resize the image\n",
    "                    # put the image onto the map\n",
    "                    # if i > 10 and i < 50 and j > 15 and j < 50:\n",
    "                    # img_label.append(image_label[row*3+col])\n",
    "                    map[x + row * img_size : x + (row + 1) * img_size, \n",
    "                        y + col * img_size : y + (col + 1) * img_size, :] = img\n",
    "            if texture_img_count > 4: stmap[i, j] = 2 # the current pixel is texture preferring\n",
    "        else: # white out the grid if no units are in this grid\n",
    "            stmap[i, j] = 0\n",
    "            x = i * (img_size*3 + line_width)\n",
    "            y = j * (img_size*3 + line_width)\n",
    "            map[x : x + img_size*3 + line_width*2, \n",
    "                y : y + img_size*3 + line_width*2, :] = 1.0\n",
    "            \n",
    "print(\"texture preferring ratio:\", np.sum(stmap == 2) / np.sum(stmap != 0))\n",
    "matplotlib.image.imsave(folder_path + 'Fig4/TDANN40_ST_2erf.bmp', map)\n",
    "del map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDANN responses to 50k natural images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load TDANN final model from checkpoint\n",
    "def load_model_from_checkpoint(checkpoint_path: str, device: str):\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "    # drop the FC layer\n",
    "    model.fc = nn.Identity()\n",
    "    # load weights\n",
    "    ckpt = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    state_dict = ckpt[\"classy_state_dict\"][\"base_model\"][\"model\"][\"trunk\"]\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"base_model\") and \"fc.\" not in k:\n",
    "            remainder = k.split(\"base_model.\")[-1]\n",
    "            new_state_dict[remainder] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    # freeze all weights\n",
    "    for param in model.parameters(): param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "# obtain TDANN one specific layer all units' activations to 50k images\n",
    "def get_layer_activations():\n",
    "    LAYER = \"layer4.0\"\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"available device: {DEVICE}\")\n",
    "    checkpoint_path = folder_path + \"Fig4/TDANNfinal.torch\"\n",
    "    model = load_model_from_checkpoint(checkpoint_path, DEVICE)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    # image preprocessing\n",
    "    preprocess = torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)), torchvision.transforms.ToTensor()])\n",
    "    # Define the layers of interest\n",
    "    layers_of_interest = {\n",
    "        \"layer1.0\": model.layer1[0], # (64, 56, 56)\n",
    "        \"layer1.1\": model.layer1[1], # (64, 56, 56)\n",
    "        \"layer2.0\": model.layer2[0], # (128, 28, 28)\n",
    "        \"layer2.1\": model.layer2[1], # (128, 28, 28)\n",
    "        \"layer3.0\": model.layer3[0], # (256, 14, 14)\n",
    "        \"layer3.1\": model.layer3[1], # (256, 14, 14)\n",
    "        \"layer4.0\": model.layer4[0], # (512, 7, 7)\n",
    "        \"layer4.1\": model.layer4[1], # (512, 7, 7)\n",
    "    }\n",
    "    layer_of_interest = layers_of_interest[LAYER]\n",
    "    # to store all layers' all channels' central units' activation values to 50k images\n",
    "    num_imgs = 50000\n",
    "    if LAYER == \"layer1.0\" or LAYER == \"layer1.1\":\n",
    "        storage = np.zeros((num_imgs, 64, 56, 56))\n",
    "    elif LAYER == \"layer2.0\" or LAYER == \"layer2.1\":\n",
    "        storage = np.zeros((num_imgs, 128, 28, 28))\n",
    "    elif LAYER == \"layer3.0\" or LAYER == \"layer3.1\":\n",
    "        storage = np.zeros((num_imgs, 256, 14, 14))\n",
    "    elif LAYER == \"layer4.0\" or LAYER == \"layer4.1\":\n",
    "        storage = np.zeros((num_imgs, 512, 7, 7))\n",
    "    # iterate through all images\n",
    "    for i in tqdm(range(num_imgs), desc=\"50k imgs...\", disable=False):\n",
    "        # Hook function to capture activations\n",
    "        def hook_fn(module, input, output):\n",
    "            global activation\n",
    "            activation = output.detach()  # Detach to avoid computation graph issues\n",
    "        # Register hook\n",
    "        hook = layer_of_interest.register_forward_hook(hook_fn)\n",
    "        # Load and preprocess an image\n",
    "        image_path = folder_path + \"50K_Imgset/\" + str(int(i + 1)) + \".bmp\"\n",
    "        input_tensor = preprocess(Image.open(image_path)).unsqueeze(0) # torch.Size([1, 3, 224, 224])\n",
    "        input_tensor = input_tensor.to(DEVICE, dtype=torch.float) # move to cuda\n",
    "        with torch.no_grad(): model(input_tensor) # Forward pass\n",
    "        if activation is not None:\n",
    "            storage[i, :, :, :] = activation.detach().cpu().numpy().squeeze()  # Convert to NumPy\n",
    "        hook.remove()\n",
    "        # break # for testing purposes\n",
    "    # obtained layer units activation\n",
    "    storage = storage.reshape(storage.shape[0], -1) # of shape (n_images, n_units)\n",
    "    print(\"obtained activation matrix has shape:\", storage.shape)\n",
    "    # load layer units' positions on the TDANN simulated cortical map\n",
    "    positions = np.load(folder_path + \"Fig4/TDANNfinal_positions/\" + LAYER + \".npz\")[\"coordinates\"] # (num_units, 2)\n",
    "    cortical_size = max(positions[:, 0]) - min(positions[:, 0]) # define the length of the 2D plane\n",
    "    grid_num = 60 # each grid contains the most-preferred 9 images by the mean within-grid-units' response\n",
    "    # To store each grid's agregated response to all 50K images, with an additional roi\n",
    "    layer_map = np.zeros((grid_num, grid_num, int(num_imgs + 1))) # 50000 image activations + 1 last ROI dimension\n",
    "    for i in tqdm(range(grid_num), desc=\"map initialization...\", disable=False):\n",
    "        for j in range(grid_num):\n",
    "            # first find all units in this current grid\n",
    "            xmin_cortex = cortical_size / grid_num * i\n",
    "            xmax_cortex = cortical_size / grid_num * (i + 1)\n",
    "            ymin_cortex = cortical_size / grid_num * j\n",
    "            ymax_cortex = cortical_size / grid_num * (j + 1)\n",
    "            # find all units in this current grid\n",
    "            units_within_grid_indeices = np.where((positions[:, 0] >= xmin_cortex) & (positions[:, 0] < xmax_cortex) & (positions[:, 1] >= ymin_cortex) & (positions[:, 1] < ymax_cortex))[0]\n",
    "            if len(units_within_grid_indeices) > 0:\n",
    "                # compute the mean response of all units (neurons) within this grid to all 50K images (into a row vector)\n",
    "                layer_map[i, j, :num_imgs] = np.mean(storage[:, units_within_grid_indeices], axis=1).T\n",
    "                layer_map[i, j, -1] = 1\n",
    "    # save\n",
    "    np.save(folder_path + \"Fig4/TDANN40map.npy\", layer_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain TDANN every layer every channel central units' activation values to 50k images\n",
    "def get_activations():\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"available device: {DEVICE}\")\n",
    "    checkpoint_path = folder_path + \"Fig4/TDANNfinal.torch\"\n",
    "    model = load_model_from_checkpoint(checkpoint_path, DEVICE)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    # image preprocessing\n",
    "    preprocess = torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)), torchvision.transforms.ToTensor()])\n",
    "    # Define the layers of interest\n",
    "    layers_of_interest = {\n",
    "        \"layer1.0\": model.layer1[0], # (64, 56, 56)\n",
    "        \"layer1.1\": model.layer1[1], # (64, 56, 56)\n",
    "        \"layer2.0\": model.layer2[0], # (128, 28, 28)\n",
    "        \"layer2.1\": model.layer2[1], # (128, 28, 28)\n",
    "        \"layer3.0\": model.layer3[0], # (256, 14, 14)\n",
    "        \"layer3.1\": model.layer3[1], # (256, 14, 14)\n",
    "        \"layer4.0\": model.layer4[0], # (512, 7, 7)\n",
    "        \"layer4.1\": model.layer4[1], # (512, 7, 7)\n",
    "    }\n",
    "    # to store all layers' all channels' central units' activation values to 50k images\n",
    "    num_imgs = 50000\n",
    "    units = [np.zeros((64, num_imgs)), np.zeros((64, num_imgs)), # layer1 (64, 56, 56)\n",
    "             np.zeros((128, num_imgs)), np.zeros((128, num_imgs)), # layer2 (128, 28, 28)\n",
    "             np.zeros((256, num_imgs)), np.zeros((256, num_imgs)), # layer3 (256, 14, 14)\n",
    "             np.zeros((512, num_imgs)), np.zeros((512, num_imgs))] # layer4 (512, 7, 7)\n",
    "    index = np.array([28, 28, 14, 14, 7, 7, 3, 3])\n",
    "    for i in tqdm(range(num_imgs), desc=\"50k imgs...\", disable=False):\n",
    "        activations = {} # Dictionary to store activations\n",
    "        # Hook function to capture activations\n",
    "        def hook_fn(name):\n",
    "            def hook(module, input, output):\n",
    "                activations[name] = output.detach()  # Store detached tensor\n",
    "            return hook\n",
    "        # Register hooks\n",
    "        hooks = []\n",
    "        for name, layer in layers_of_interest.items():\n",
    "            hook = layer.register_forward_hook(hook_fn(name))\n",
    "            hooks.append(hook)\n",
    "        # Load and preprocess an image\n",
    "        image_path = folder_path + \"50K_Imgset/\" + str(int(i + 1)) + \".bmp\"\n",
    "        input_tensor = preprocess(Image.open(image_path)).unsqueeze(0) # torch.Size([1, 3, 224, 224])\n",
    "        input_tensor = input_tensor.to(DEVICE, dtype=torch.float) # move to cuda\n",
    "        with torch.no_grad(): model(input_tensor) # Forward pass\n",
    "        for hook in hooks: hook.remove() # Remove hooks (to prevent memory issues)\n",
    "        # Print activation shapes\n",
    "        for idx, (layer, activation) in enumerate(activations.items()):\n",
    "            activation = activation.detach().cpu().numpy().squeeze()\n",
    "            units[idx][:, i] = activation[:, index[idx], index[idx]] # store central units' activation values to corresponding arrays\n",
    "            # print(f\"Iteration {idx}: {layer} activation shape: {activation.shape}\")\n",
    "        # break # for testing purposes\n",
    "\n",
    "    # save activation values\n",
    "    np.savez(folder_path + \"Fig4/activation_TDANN.npz\", \n",
    "             layer10=units[0], layer11=units[1], \n",
    "             layer20=units[2], layer21=units[3], \n",
    "             layer30=units[4], layer31=units[5], \n",
    "             layer40=units[6], layer41=units[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDANN natural image preference map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of TDANN final network & positions, into a 2D 60 by 60 gridded map, from top left to bottom right\n",
    "features = np.load(folder_path + \"Fig4/layer40/TDANN40map.npy\")\n",
    "Imgset50K_path = folder_path + \"50K_Imgset/\"\n",
    "grid_num = 60 # each grid contains the most-preferred 9 images by the mean within-grid-units' response\n",
    "grids_count = int(grid_num ** 2)\n",
    "num_imgs_each_side = 3 # number of images on each side of the grid, total number of images in a grid should be squared\n",
    "# define the size of a single image\n",
    "img_size = 30\n",
    "line_width = 5\n",
    "# create a blank map of black color (R=0, G=0, B=0)\n",
    "map = np.zeros((grid_num * (img_size*3 + line_width) + line_width, \n",
    "                grid_num * (img_size*3 + line_width) + line_width, \n",
    "                3))\n",
    "for i in tqdm(range(grid_num), desc=\"map initialization...\"): # vertical, from top to bottom\n",
    "    for j in range(grid_num): # horizontal, from left to right\n",
    "        if features[i, j, -1] == 1: # if there are units in this grid\n",
    "            image_label = np.arange(50000) + 1 # 1-indexed image names\n",
    "            # sort the mean responses (from small to large) and the image_label according to the order of mean responses\n",
    "            mean_responses, image_label = zip(*sorted(zip(features[i, j, :50000],image_label)))\n",
    "            image_label = np.flip(image_label[-int(num_imgs_each_side ** 2):]) # take the top nine images with largest mean response\n",
    "            # locate the top left corner of the current grid in the map\n",
    "            x = i * (img_size*3 + line_width) + line_width\n",
    "            y = j * (img_size*3 + line_width) + line_width\n",
    "            # fill the map's current grid with the selected nine images\n",
    "            for row in range(3):\n",
    "                for col in range(3):\n",
    "                    # load the image\n",
    "                    path = Imgset50K_path + str(int(image_label[row*3+col])) + \".bmp\" # the image name is 1-indexed\n",
    "                    img = np.array(Image.open(path))[20:80, 20:80, :] # obtain the non-blurred central part of the image\n",
    "                    img = resize(img, (img_size, img_size, 3), anti_aliasing=True) # resize the image\n",
    "                    # put the image onto the map\n",
    "                    map[x + row * img_size : x + (row + 1) * img_size, \n",
    "                        y + col * img_size : y + (col + 1) * img_size, \n",
    "                        :] = img\n",
    "        else: # white out the grid if no units are in this grid\n",
    "            x = i * (img_size*3 + line_width)\n",
    "            y = j * (img_size*3 + line_width)\n",
    "            map[x : x + img_size*3 + line_width*2, \n",
    "                y : y + img_size*3 + line_width*2, \n",
    "                :] = 1.0\n",
    "\n",
    "matplotlib.image.imsave(folder_path + 'Fig4/TDANN40_IT.bmp', map)\n",
    "del map, features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topoV4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
