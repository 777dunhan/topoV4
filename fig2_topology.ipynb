{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import scipy.io as sio\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.ndimage import label, sum as ndi_sum\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import binary_erosion\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "folder_path = \"/path/to/your/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V4 digital twin raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dispersity results\n",
    "dispersity = sio.loadmat(folder_path + \"V4DT/Dispersity_results/dispersity.mat\")['cmap'] # (128, 128, 3) <class 'numpy.ndarray'>\n",
    "dispersity_raw = sio.loadmat(folder_path + \"V4DT/Dispersity_results/FDraw.mat\")[\"FD\"] # (128, 128) <class 'numpy.ndarray'>\n",
    "# Load the roi for visual inspection with Transpose\n",
    "roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128) <class 'numpy.ndarray'>\n",
    "# Load the RF results\n",
    "polar_angle = sio.loadmat(folder_path + \"V4DT/RF_results/polar_angle.mat\")['map'] # (128, 128, 3) <class 'numpy.ndarray'>\n",
    "# theta_map_raw: [0.0 60.1088362]; mean=31.2365; std=13.07726\n",
    "theta = sio.loadmat(folder_path + \"V4DT/RF_results/theta_map_raw.mat\")['theta_map_raw'] # (128, 128); or theta_map.mat['theta_map'] after color coding\n",
    "eccentricity = sio.loadmat(folder_path + \"V4DT/RF_results/eccentricity.mat\")['map'] # (128, 128, 3) <class 'numpy.ndarray'>\n",
    "# r_map_raw: [12.7849 36.81318]; mean=24.77; std=5.7943; # r_map: [233, 767]; mean=501; std=129\n",
    "r = sio.loadmat(folder_path + \"V4DT/RF_results/r_map_raw.mat\")['r_map_raw'] # (128, 128); or r_map.mat['r_map'] after color coding\n",
    "r[roi!=0] -= np.min(r[roi!=0]) # adjusted r_map_raw: [0, 24.0283]; mean=11.985257; std=5.7943\n",
    "# Load the V4 digital twin responses to 50k images\n",
    "response = np.load(folder_path + \"V4DT/PRsp.npy\") # (50000, 128, 128) <class 'numpy.ndarray'>\n",
    "mean_rsp = np.zeros((128, 128))\n",
    "for i in range(128):\n",
    "    for j in range(128):\n",
    "        mean_rsp[i, j] = np.mean(response[:, i, j])\n",
    "# visualize the results\n",
    "fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
    "axes[0].imshow(np.flip(polar_angle, axis=(0, 1))[30:120, 35:110, :])\n",
    "axes[0].axis('off')  # Turn off the axis\n",
    "axes[0].set_title('Polar Angle')\n",
    "axes[1].imshow(np.flip(theta)[30:120, 35:110]) # polar angle range: 0.0 60.10883624961076\n",
    "axes[1].axis('off')  # Turn off the axis\n",
    "axes[1].set_title('Theta')\n",
    "axes[2].imshow(np.flip(eccentricity, axis=(0, 1))[30:120, 35:110, :])\n",
    "axes[2].axis('off')  # Turn off the axis\n",
    "axes[2].set_title('Eccentricity')\n",
    "axes[3].imshow(np.flip(r)[30:120, 35:110]) # eccentricity range: 0.0 24.028273811974355\n",
    "axes[3].axis('off')  # Turn off the axis\n",
    "axes[3].set_title('R')\n",
    "axes[4].imshow(np.flip(dispersity, axis=(0, 1))[30:120, 35:110, :])\n",
    "axes[4].axis('off')  # Turn off the axis\n",
    "axes[4].set_title('Dispersity')\n",
    "axes[5].imshow(np.flip(dispersity_raw)[30:120, 35:110])\n",
    "axes[5].axis('off')  # Turn off the axis\n",
    "axes[5].set_title('dispersity_raw')\n",
    "axes[6].imshow(np.flip(roi)[30:120, 35:110])\n",
    "axes[6].axis('off')  # Turn off the axis\n",
    "axes[6].set_title('ROI')\n",
    "axes[7].imshow(np.flip(mean_rsp)[30:120, 35:110])\n",
    "axes[7].axis('off')  # Turn off the axis\n",
    "axes[7].set_title('V4 Mean Rsp')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign V4 neuronal column onto simulated map grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between V4 columnar responses and the learned SOM weights\n",
    "def V4_assignment(folder_path, name):\n",
    "    # load essential data\n",
    "    som = np.load(folder_path + name + \"/weights.npy\") # (60, 60, 50000(+))\n",
    "    som = som[:, :, :50000] # (60, 60, 50250) to (60, 60, 50000)\n",
    "    v4_rsp = np.load(folder_path + \"V4DT/PRsp.npy\") # (50000, 128, 128)\n",
    "    roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128) <class 'numpy.ndarray'>\n",
    "    rsp = np.zeros((int(np.sum(roi)), 50000)) # (3048, 50000)\n",
    "    index = 0\n",
    "    for i in range(roi.shape[0]):\n",
    "        for j in range(roi.shape[1]):\n",
    "            if roi[i, j] == 1:\n",
    "                rsp[index, :] = v4_rsp[:, i, j]\n",
    "                index += 1\n",
    "    assert rsp.shape[0] == 3048\n",
    "    del v4_rsp, roi\n",
    "\n",
    "    # compute correlation\n",
    "    assign = np.zeros((60, 60, 3048))\n",
    "    for i in tqdm(range(60), desc=\"cor...\"):\n",
    "        for j in range(60):\n",
    "            for k in range(3048):\n",
    "                assign[i, j, k] = np.corrcoef(som[i, j, :], rsp[k, :])[0, 1]\n",
    "    if name == \"RSOM\":\n",
    "        np.save(folder_path + name +  \"/assign_rsom.npy\", assign)\n",
    "    else:\n",
    "        np.save(folder_path + name +  \"/assign_som.npy\", assign)\n",
    "        assert name == \"SOM\"\n",
    "\n",
    "V4_assignment(folder_path, name=\"RSOM\")\n",
    "V4_assignment(folder_path, name=\"SOM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the most correlated V4 columnar responses, assign the V4 columnar properties to the (R)SOM units\n",
    "v4idx = np.load(folder_path + \"V4DT/idx.npy\") # (128, 128), perserved only with 16 domains of 2875 neuronal columns, not the full 3048 neuronal columns\n",
    "roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128), of which 3048 neuronal columns are rois\n",
    "selected = np.zeros((np.sum(v4idx != 0))) # (2875,) the 0-based index of the selected 2875 columns in the 3048 columns\n",
    "selected_ij = np.zeros((int(np.sum(v4idx != 0)), 2)) # (2875, 2) the 0-based i, j positional index of the selected 2875 columns\n",
    "index_selected = 0\n",
    "index_roi = 0\n",
    "for i in range(128):\n",
    "    for j in range(128):\n",
    "        if roi[i, j] != 0:\n",
    "            if v4idx[i, j] != 0:\n",
    "                selected[index_selected] = index_roi\n",
    "                selected_ij[index_selected, 0] = i\n",
    "                selected_ij[index_selected, 1] = j\n",
    "                index_selected += 1\n",
    "            index_roi += 1\n",
    "assert index_selected == 2875\n",
    "assert index_roi == 3048\n",
    "\n",
    "name = \"RSOM\"\n",
    "correlation = np.load(folder_path + name + \"/assign_rsom.npy\") # (60, 60, 3048)\n",
    "correlation = correlation[:, :, selected.astype(int)] # (60, 60, 2875)\n",
    "matched_idx_rsom = np.zeros((60, 60))\n",
    "polar_angle_rsom = np.zeros((60, 60, 3))\n",
    "eccentricity_rsom = np.zeros((60, 60, 3))\n",
    "dispersity_rsom = np.zeros((60, 60, 3))\n",
    "dispersity_raw_rsom = np.zeros((60, 60))\n",
    "for i in range(60):\n",
    "    for j in range(60):\n",
    "        k = np.argmax(correlation[i, j, :]) # index of the entry with the maximum correlation\n",
    "        x = int(selected_ij[k, 0])\n",
    "        y = int(selected_ij[k, 1])\n",
    "        matched_idx_rsom[i, j] = v4idx[x, y]\n",
    "        polar_angle_rsom[i, j, :] = polar_angle[x, y, :]\n",
    "        eccentricity_rsom[i, j, :] = eccentricity[x, y, :]\n",
    "        dispersity_rsom[i, j, :] = dispersity[x, y, :]\n",
    "        dispersity_raw_rsom[i, j] = dispersity_raw[x, y]\n",
    "np.savez(folder_path + name + \"/assigned.npz\", \n",
    "         matched_idx_rsom=matched_idx_rsom, \n",
    "         polar_angle_rsom=polar_angle_rsom, \n",
    "         eccentricity_rsom=eccentricity_rsom, \n",
    "         dispersity_rsom=dispersity_rsom,\n",
    "         dispersity_raw_rsom=dispersity_raw_rsom)\n",
    "\n",
    "name = \"SOM\"\n",
    "correlation = np.load(folder_path + name + \"/assign_som.npy\") # (60, 60, 3048)\n",
    "correlation = correlation[:, :, selected.astype(int)] # (60, 60, 2875)\n",
    "matched_idx_som = np.zeros((60, 60))\n",
    "polar_angle_som = np.zeros((60, 60, 3))\n",
    "eccentricity_som = np.zeros((60, 60, 3))\n",
    "dispersity_som = np.zeros((60, 60, 3))\n",
    "dispersity_raw_som = np.zeros((60, 60))\n",
    "for i in range(60):\n",
    "    for j in range(60):\n",
    "        k = np.argmax(correlation[i, j, :])\n",
    "        x = int(selected_ij[k, 0])\n",
    "        y = int(selected_ij[k, 1])\n",
    "        matched_idx_som[i, j] = v4idx[x, y]\n",
    "        polar_angle_som[i, j, :] = polar_angle[x, y, :]\n",
    "        eccentricity_som[i, j, :] = eccentricity[x, y, :]\n",
    "        dispersity_som[i, j, :] = dispersity[x, y, :]\n",
    "        dispersity_raw_som[i, j] = dispersity_raw[x, y]\n",
    "np.savez(folder_path + name + \"/assigned.npz\", \n",
    "         matched_idx_som=matched_idx_som,\n",
    "         polar_angle_som=polar_angle_som,\n",
    "         eccentricity_som=eccentricity_som,\n",
    "         dispersity_som=dispersity_som,\n",
    "         dispersity_raw_som=dispersity_raw_som)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a matched som grid corresponding domain number, remove the unconnected components smaller than a threshold\n",
    "def connected_components(matched, threshold, return_largest=False):\n",
    "    # Define the structure (connectivity) for connected components\n",
    "    structure = np.ones((3, 3), dtype=int)  # 8-connectivity, meaning that both adjacent and diagonal pixels are considered as neighbors\n",
    "    connected = np.copy(matched) # Initialize an array to store the output\n",
    "    if return_largest: largest = np.copy(matched) # Initialize an array to store the largest connected component\n",
    "    for class_num in np.unique(matched): # Process each class separately\n",
    "        if class_num == 0: continue # Skip the background\n",
    "        class_mask = (matched == class_num) # Create a binary mask for the current class\n",
    "        labeled_array, num_features = label(class_mask, structure=structure) # Find connected components in the binary mask\n",
    "        component_sizes = ndi_sum(class_mask, labeled_array, index=range(1, num_features + 1)) # Compute the size of each component\n",
    "        for i, size in enumerate(component_sizes): # Zero out components smaller than the threshold\n",
    "            if size < threshold: connected[labeled_array == (i + 1)] = 0\n",
    "        if return_largest:\n",
    "            for i, size in enumerate(component_sizes):\n",
    "                if size != np.max(component_sizes): largest[labeled_array == (i + 1)] = 0\n",
    "    if return_largest: return connected, largest\n",
    "    else: return connected\n",
    "\n",
    "# domain color map\n",
    "v4idx = np.load(folder_path + \"V4DT/idx.npy\")\n",
    "roi = np.load(folder_path + \"V4DT/ROI.npy\").T # transpose to match V4idx\n",
    "v4idx = connected_components(v4idx, threshold=10, return_largest=False) + 1\n",
    "for i in range(roi.shape[0]):\n",
    "    for j in range(roi.shape[1]):\n",
    "        if roi[i, j] != 1: v4idx[i, j] = 0 # non roi area\n",
    "cc_color_v4 = np.flipud(np.fliplr(v4idx[14:90, 23:90])) # zoom in to the roi area and then do bottom-top rotate, left-right flip\n",
    "cc_color_som = connected_components(matched_idx_som.T, 10)\n",
    "cc_color_rsom = connected_components(matched_idx_rsom.T, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain size & adjacency matrix evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a matched som grid map detailing grid domain label, visualize all connected components' domain label and size\n",
    "def connected_components_visual(matched):\n",
    "    # Define the structure (connectivity) for connected components\n",
    "    structure = np.ones((3, 3), dtype=int)  # 8-connectivity, meaning that both adjacent and diagonal pixels are considered as neighbors\n",
    "    sizes = [] # all component sizes\n",
    "    labels = [] # all component labels\n",
    "    adjacency = [] # a list of all components, each entry detailing adjacent domains of that component\n",
    "    for class_num in np.unique(matched): # Process each class separately\n",
    "        adjacency_domain = []\n",
    "        if class_num == 0: continue # Skip the background\n",
    "        class_mask = (matched == class_num) # Create a binary mask for the current class\n",
    "        labeled_array, num_features = label(class_mask, structure=structure) # Find connected components in the binary mask\n",
    "        component_sizes = ndi_sum(class_mask, labeled_array, index=range(1, num_features + 1)) # Compute the size of each component\n",
    "        # consider every connected component's adjacency pattern\n",
    "        for k, size in enumerate(component_sizes):\n",
    "            adjacency_single = []\n",
    "            mask_k = np.zeros((np.shape(matched)[0], np.shape(matched)[1]))\n",
    "            mask_k[labeled_array == (k + 1)] = 1\n",
    "            idx = np.pad(matched, ((1, 1), (1, 1)), 'constant', constant_values=0) # zero padding for the boundary: top, bottom, left, right\n",
    "            for i in range(np.shape(mask_k)[0]):\n",
    "                for j in range(np.shape(mask_k)[1]):\n",
    "                    if mask_k[i, j] == 1: # considering one targted within-domain voxel at once\n",
    "                        m = i + 1 # the same i, j index in the padded idx matrix\n",
    "                        n = j + 1\n",
    "                        if int(idx[m + 1, n]) != 0 and int(idx[m + 1, n]) != int(idx[m, n]): # down\n",
    "                            adjacency_single.append(int(idx[m + 1, n]))\n",
    "                        if int(idx[m - 1, n]) != 0 and int(idx[m - 1, n]) != int(idx[m, n]): # up\n",
    "                            adjacency_single.append(int(idx[m - 1, n]))\n",
    "                        if int(idx[m, n + 1]) != 0 and int(idx[m, n + 1]) != int(idx[m, n]): # right\n",
    "                            adjacency_single.append(int(idx[m, n + 1]))\n",
    "                        if int(idx[m, n - 1]) != 0 and int(idx[m, n - 1]) != int(idx[m, n]): # left\n",
    "                            adjacency_single.append(int(idx[m, n - 1]))\n",
    "                        if int(idx[m + 1, n + 1]) != 0 and int(idx[m + 1, n + 1]) != int(idx[m, n]): # down right\n",
    "                            adjacency_single.append(int(idx[m + 1, n + 1]))\n",
    "                        if int(idx[m + 1, n - 1]) != 0 and int(idx[m + 1, n - 1]) != int(idx[m, n]): # down left\n",
    "                            adjacency_single.append(int(idx[m + 1, n - 1]))\n",
    "                        if int(idx[m - 1, n + 1]) != 0 and int(idx[m - 1, n + 1]) != int(idx[m, n]): # up right\n",
    "                            adjacency_single.append(int(idx[m - 1, n + 1]))\n",
    "                        if int(idx[m - 1, n - 1]) != 0 and int(idx[m - 1, n - 1]) != int(idx[m, n]): # up left\n",
    "                            adjacency_single.append(int(idx[m - 1, n - 1]))\n",
    "            adjacency_domain.append(np.unique(adjacency_single))\n",
    "        # sort adjacency_domain, component_sizes based on component_sizes order\n",
    "        sorted_indices = np.argsort(component_sizes)[::-1]\n",
    "        component_sizes = component_sizes[sorted_indices]\n",
    "        adjacency_domain = [adjacency_domain[i] for i in sorted_indices]\n",
    "        # component statistics\n",
    "        sizes.extend(component_sizes)\n",
    "        labels.extend([class_num] * num_features)\n",
    "        adjacency.extend(adjacency_domain)\n",
    "\n",
    "    sizes = np.array(sizes).astype(int)\n",
    "    labels = np.array(labels).astype(int)\n",
    "    # sizes for each domain\n",
    "    domain_sizes = np.zeros((16))\n",
    "    total_sizes = np.sum(sizes)\n",
    "    for i in range(16): \n",
    "        domain_sizes[i] = np.mean(sizes[labels == i + 1]) / total_sizes\n",
    "    # sorted_indices = np.argsort(domain_sizes)[::-1]\n",
    "    return sizes, labels, adjacency, domain_sizes\n",
    "\n",
    "# domain adjacency matrix correlation against V4 benchmark\n",
    "def adjacency_matrix(idx, sizes):\n",
    "    # find the domain adjacency matrix\n",
    "    num_domains = 16\n",
    "    domains = np.unique(idx).astype(int) # number of domains, may include 0, may not include all domains from 1 to 16\n",
    "    if domains[0] == 0: domains = domains[1:] # exclude 0\n",
    "    adjacency_matrix = np.zeros((num_domains, num_domains))\n",
    "    size = idx.shape[0]\n",
    "    assert size == idx.shape[1]\n",
    "    assert len(sizes) == num_domains\n",
    "    idx = np.pad(idx, ((1, 1), (1, 1)), 'constant', constant_values=0) # zero padding for the boundary: top, bottom, left, right\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            m = i + 1 # the same i, j index in the padded idx matrix\n",
    "            n = j + 1\n",
    "            if idx[m, n] != 0: # considering one targted within-domain neuron at once\n",
    "                if int(idx[m + 1, n]) != 0 and int(idx[m + 1, n]) != int(idx[m, n]): # down\n",
    "                    adjacency_matrix[int(idx[m, n]) - 1, int(idx[m + 1, n]) - 1] += 1\n",
    "                    adjacency_matrix[int(idx[m + 1, n]) - 1, int(idx[m, n]) - 1] += 1\n",
    "                if int(idx[m - 1, n]) != 0 and int(idx[m - 1, n]) != int(idx[m, n]): # up\n",
    "                    adjacency_matrix[int(idx[m, n]) - 1, int(idx[m - 1, n]) - 1] += 1\n",
    "                    adjacency_matrix[int(idx[m - 1, n]) - 1, int(idx[m, n]) - 1] += 1\n",
    "                if int(idx[m, n + 1]) != 0 and int(idx[m, n + 1]) != int(idx[m, n]): # right\n",
    "                    adjacency_matrix[int(idx[m, n]) - 1, int(idx[m, n + 1]) - 1] += 1\n",
    "                    adjacency_matrix[int(idx[m, n + 1]) - 1, int(idx[m, n]) - 1] += 1\n",
    "                if int(idx[m, n - 1]) != 0 and int(idx[m, n - 1]) != int(idx[m, n]): # left\n",
    "                    adjacency_matrix[int(idx[m, n]) - 1, int(idx[m, n - 1]) - 1] += 1\n",
    "                    adjacency_matrix[int(idx[m, n - 1]) - 1, int(idx[m, n]) - 1] += 1\n",
    "                if int(idx[m + 1, n + 1]) != 0 and int(idx[m + 1, n + 1]) != int(idx[m, n]): # down right\n",
    "                    adjacency_matrix[int(idx[m, n]) - 1, int(idx[m + 1, n + 1]) - 1] += 1\n",
    "                    adjacency_matrix[int(idx[m + 1, n + 1]) - 1, int(idx[m, n]) - 1] += 1\n",
    "                if int(idx[m + 1, n - 1]) != 0 and int(idx[m + 1, n - 1]) != int(idx[m, n]): # down left\n",
    "                    adjacency_matrix[int(idx[m, n]) - 1, int(idx[m + 1, n - 1]) - 1] += 1\n",
    "                    adjacency_matrix[int(idx[m + 1, n - 1]) - 1, int(idx[m, n]) - 1] += 1\n",
    "                if int(idx[m - 1, n + 1]) != 0 and int(idx[m - 1, n + 1]) != int(idx[m, n]): # up right\n",
    "                    adjacency_matrix[int(idx[m, n]) - 1, int(idx[m - 1, n + 1]) - 1] += 1\n",
    "                    adjacency_matrix[int(idx[m - 1, n + 1]) - 1, int(idx[m, n]) - 1] += 1\n",
    "                if int(idx[m - 1, n - 1]) != 0 and int(idx[m - 1, n - 1]) != int(idx[m, n]): # up left\n",
    "                    adjacency_matrix[int(idx[m, n]) - 1, int(idx[m - 1, n - 1]) - 1] += 1\n",
    "                    adjacency_matrix[int(idx[m - 1, n - 1]) - 1, int(idx[m, n]) - 1] += 1\n",
    "    # for d in range(num_domains): adjacency_matrix[d, :] /= np.sum(adjacency_matrix[d, :]) # adjacency_matrix[d, :] *= sizes[d]\n",
    "    adjacency_matrix /= np.max(adjacency_matrix)\n",
    "    return adjacency_matrix\n",
    "\n",
    "# V4 benchmark\n",
    "v4idx = np.load(folder_path + \"V4DT/idx.npy\") # (128, 128), perserved only with 16 domains, not the full 3048 neurons\n",
    "v4idx = connected_components(v4idx, 10, return_largest=False)\n",
    "sizes_v4, labels_v4, adjacency_v4, domain_sizes_v4 = connected_components_visual(v4idx)\n",
    "adj_matrix_v4 = adjacency_matrix(v4idx, domain_sizes_v4)\n",
    "print(\"V4: {} of components\".format(len(sizes_v4)))\n",
    "\n",
    "# SOM\n",
    "sizes_som, labels_som, adjacency_som, domain_sizes_som = connected_components_visual(cc_color_som)\n",
    "# domain adjacency score\n",
    "adj_matrix_som = adjacency_matrix(cc_color_som, domain_sizes_som)\n",
    "adj_cor = stats.pearsonr(adj_matrix_som.flatten(), adj_matrix_v4.flatten())[0]\n",
    "# visualizing the connected components' domain label and size\n",
    "print(\"SOM: {} of components, with a domain size cor: {:.3f}; adjacency matrix cor: {:.3f}\".format(len(sizes_som), stats.pearsonr(domain_sizes_v4, domain_sizes_som)[0], adj_cor))\n",
    "\n",
    "# RSOM\n",
    "sizes_rsom, labels_rsom, adjacency_rsom, domain_sizes_rsom = connected_components_visual(cc_color_rsom)\n",
    "# domain adjacency score\n",
    "adj_matrix_rsom = adjacency_matrix(cc_color_rsom, domain_sizes_rsom)\n",
    "adj_cor = stats.pearsonr(adj_matrix_rsom.flatten(), adj_matrix_v4.flatten())[0]\n",
    "# visualizing the connected components' domain label and size\n",
    "print(\"RSOM: {} of components, with a domain size cor: {:.3f}; adjacency matrix cor: {:.3f}\".format(len(sizes_rsom), stats.pearsonr(domain_sizes_v4, domain_sizes_rsom)[0], adj_cor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a matched som grid map detailing grid domain label, construct a relative positioning matrix\n",
    "# num_domains by num_domains matrix, each entry detailing the averaged distance between two domains\n",
    "def relative_positioning_matrix(matched):\n",
    "    num_domains = 16\n",
    "    size = np.shape(matched)[0]\n",
    "    assert size == np.shape(matched)[1] # squared map size\n",
    "    if size == 128: # V4 benchmark\n",
    "        roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128), of which 3048 neuronal columns are rois\n",
    "    else: # SOM or RSOM simulation\n",
    "        roi = np.ones((size, size))\n",
    "    position_matrix = np.zeros((num_domains, num_domains)) # initialize the relative positioning matrix\n",
    "    count_matrix = np.zeros((num_domains, num_domains)) # initialize the count matrix to record the number of pairs\n",
    "    unique_domains = np.unique(matched).astype(int) # number of domains, may include 0, may not include all domains from 1 to 16\n",
    "    for class_num in unique_domains: # Process each domain separately: 0 (erased unconnected), 1, 2, ..., 16\n",
    "        if class_num != 0: # Skip the erased components\n",
    "            class_mask = np.zeros((size, size))\n",
    "            class_mask[matched == class_num] = 1 # Create a binary mask for the current class\n",
    "            class_mask = class_mask.astype(int)\n",
    "            # iterate ROIs within the current domain\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    if class_mask[i, j] == 1:\n",
    "                        # iterate ROIs of all other domains\n",
    "                        for m in range(size):\n",
    "                            for n in range(size):\n",
    "                                if roi[m, n] == 1 and class_mask[m, n] == 0 and int(matched[m, n]) != 0: # not the same / erased domain\n",
    "                                    assert 0 <= class_num - 1 < num_domains\n",
    "                                    assert 0 <= matched[m, n] - 1 < num_domains\n",
    "                                    dis = np.sqrt((i - m) ** 2 + (j - n) ** 2)\n",
    "                                    position_matrix[class_num - 1, matched[m, n] - 1] += dis\n",
    "                                    position_matrix[matched[m, n] - 1, class_num - 1] += dis\n",
    "                                    count_matrix[class_num - 1, matched[m, n] - 1] += 1\n",
    "                                    count_matrix[matched[m, n] - 1, class_num - 1] += 1\n",
    "    # average the position matrix\n",
    "    for i in range(num_domains):\n",
    "        for j in range(num_domains):\n",
    "            if count_matrix[i, j] != 0:\n",
    "                position_matrix[i, j] /= count_matrix[i, j]\n",
    "    # normalize the position matrix\n",
    "    position_matrix /= np.max(position_matrix)\n",
    "    # for i in range(num_domains): position_matrix[i, :] /= np.max(position_matrix[i, :])\n",
    "    return position_matrix\n",
    "\n",
    "position_som = relative_positioning_matrix(cc_color_som.astype(int))\n",
    "position_rsom = relative_positioning_matrix(cc_color_rsom.astype(int))\n",
    "position_v4 = relative_positioning_matrix(v4idx.astype(int))\n",
    "# relative positioning score\n",
    "print(\"relative positioning matrix correlation between SOM and V4\", stats.pearsonr(position_som.flatten(), position_v4.flatten())[0])\n",
    "print(\"relative positioning matrix correlation between RSOM and V4\", stats.pearsonr(position_rsom.flatten(), position_v4.flatten())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature dispersity measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"V4\", \"SOM\", \"RSOM\"]\n",
    "fd_raw = False\n",
    "k = 20 # patch size\n",
    "for name in names:\n",
    "    if not fd_raw: # compute high / low dispersity clustering\n",
    "        if name == \"V4\":\n",
    "            FD_map = sio.loadmat(folder_path + \"V4DT/Dispersity_results/FDraw.mat\")[\"FD\"] # (128, 128) <class 'numpy.ndarray'>\n",
    "            roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128) <class 'numpy.ndarray'>\n",
    "            roi_indices = np.where(roi == 1)\n",
    "            roi_values = FD_map[roi_indices]\n",
    "        elif name == \"SOM\":\n",
    "            FD_map = np.load(folder_path + name + \"/assigned.npz\")[\"dispersity_raw_som\"] # (60, 60)\n",
    "            roi = np.ones((60, 60)).astype(int)\n",
    "            roi_indices = np.where(roi == 1)\n",
    "            roi_values = FD_map[roi_indices]\n",
    "        elif name == \"RSOM\":\n",
    "            FD_map = np.load(folder_path + name + \"/assigned.npz\")[\"dispersity_raw_rsom\"] # (60, 60)\n",
    "            roi = np.ones((60, 60)).astype(int)\n",
    "            roi_indices = np.where(roi == 1)\n",
    "            roi_values = FD_map[roi_indices]\n",
    "        # Perform hierarchical clustering\n",
    "        Z = linkage(roi_values.reshape(-1, 1), method='ward')  # Ward's method for clustering\n",
    "        cluster_labels = fcluster(Z, t=2, criterion='maxclust')  # Get 2 clusters\n",
    "\n",
    "        # Identify the cluster with higher dispersity\n",
    "        cluster_1_mean = roi_values[cluster_labels == 1].mean()\n",
    "        cluster_2_mean = roi_values[cluster_labels == 2].mean()\n",
    "\n",
    "        high_dispersity_cluster = 1 if cluster_1_mean > cluster_2_mean else 2\n",
    "\n",
    "        # Initialize clusters array\n",
    "        clusters = np.zeros_like(FD_map, dtype=int)  # Start with all zeros (out-of-ROI and low-dispersity)\n",
    "\n",
    "        # Assign 1 to grids in the high-dispersity cluster\n",
    "        clusters[roi_indices] = (cluster_labels == high_dispersity_cluster).astype(int)\n",
    "    if name == \"V4\":\n",
    "        fd = [] # patch mean fd values\n",
    "        num_patches = 0\n",
    "        roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128) <class 'numpy.ndarray'>\n",
    "        while num_patches < 1000000:\n",
    "            # generate random coordinate as the top-left corner of the current patch\n",
    "            x = np.random.randint(low=14, high=90-k)\n",
    "            y = np.random.randint(low=23, high=90-k)\n",
    "            roi_in = np.sum(roi[x:x+k, y:y+k])\n",
    "            if roi_in >= (0.5 * k * k): \n",
    "                fd.append(np.mean(clusters[x:x+k, y:y+k]))\n",
    "                num_patches += 1\n",
    "        fd = np.array(fd)\n",
    "        dispersion = np.var(fd) / np.mean(fd)\n",
    "        print(\"dispersion index of V4 benchmark : \", dispersion)\n",
    "    else:\n",
    "        fd = [] # patch mean fd values\n",
    "        num_patches = 0\n",
    "        while num_patches < 1000000:\n",
    "            # generate random coordinate as the top-left corner of the current patch\n",
    "            x = np.random.randint(low=0, high=60-k)\n",
    "            y = np.random.randint(low=0, high=60-k)\n",
    "            fd.append(np.mean(clusters[x:x+k, y:y+k]))\n",
    "            num_patches += 1\n",
    "        fd = np.array(fd)\n",
    "        dispersion = np.var(fd) / np.mean(fd)\n",
    "        print(\"dispersion index of\", name, \": \", dispersion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise columns / grids tuning correlation as a function of map distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise grid tuning curve coorelation as a function of exactly the map physical distance between them\n",
    "def cordis(rsp):\n",
    "    # check input response\n",
    "    if rsp.shape[1] == rsp.shape[2] == 128: # (50000, 128, 128) V4 benchmark\n",
    "        roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128) <class 'numpy.ndarray'>\n",
    "    else: \n",
    "        # roi = np.load(\"/TDANN41_roi.npy\")\n",
    "        roi = np.ones((60, 60)).astype(int) # artificial som map weight response\n",
    "    size = roi.shape[0]\n",
    "    assert size == roi.shape[1]\n",
    "    num_roi = int(np.sum(roi))\n",
    "    # response, roi data preparation\n",
    "    response = np.zeros((num_roi, 50000))\n",
    "    position = np.zeros((num_roi, 2))\n",
    "    voxel_index = 0\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if roi[i, j] == 1:\n",
    "                if roi.shape[0] == roi.shape[1] == 128: response[voxel_index, :] = rsp[:, i, j] # V4 benchmark\n",
    "                elif roi.shape[0] == roi.shape[1] == 60: response[voxel_index, :] = rsp[i, j, :] # artificial som map weight response\n",
    "                position[voxel_index, 0] = i\n",
    "                position[voxel_index, 1] = j\n",
    "                voxel_index += 1\n",
    "    assert voxel_index == num_roi\n",
    "    # calculate the correlation matrix\n",
    "    cordis_matrix = np.zeros((num_roi, num_roi, 2)) # 0th entry for correlation, 1st entry for pairwise map distance\n",
    "    for i in tqdm(range(num_roi), desc=\"calculating correlation matrix...\", disable=False):\n",
    "        for j in range(i, num_roi):\n",
    "            # correlation calculation\n",
    "            if i != j:\n",
    "                cordis_matrix[i, j, 0] = pearsonr(response[i, :], response[j, :])[0]\n",
    "                cordis_matrix[j, i, 0] = cordis_matrix[i, j, 0]\n",
    "            else: cordis_matrix[i, j, 0] = 1.0\n",
    "            # distance calculation\n",
    "            cordis_matrix[i, j, 1] = np.sqrt((position[i, 0] - position[j, 0]) ** 2 + (position[i, 1] - position[j, 1]) ** 2)\n",
    "            cordis_matrix[j, i, 1] = cordis_matrix[i, j, 1]\n",
    "    return cordis_matrix\n",
    "\n",
    "# SOM\n",
    "name = \"SOM\"\n",
    "rsp = np.load(folder_path + name + \"/weights.npy\") # (60, 60, 50000(+))\n",
    "assert rsp.shape[2] == 50000\n",
    "cordis_matrix = cordis(rsp)\n",
    "np.save(folder_path + name + \"/cordis_V4_som.npy\", cordis_matrix)\n",
    "\n",
    "# V4 benchmark\n",
    "rsp = np.load(folder_path + \"V4DT/PRsp.npy\") # (50000, 128, 128) <class 'numpy.ndarray'>\n",
    "cordis_matrix = cordis(rsp)\n",
    "np.save(folder_path + \"V4DT/cordis_v4_benchmark.npy\", cordis_matrix)\n",
    "\n",
    "# RSOM\n",
    "name = \"RSOM\"\n",
    "rsp = np.load(folder_path + name + \"/weights.npy\") # (60, 60, 50000(+))\n",
    "rsp = rsp[:, :, :50000] # (60, 60, 50000)\n",
    "assert rsp.shape[2] == 50000\n",
    "cordis_matrix = cordis(rsp)\n",
    "np.save(folder_path + name + \"/cordis_V4_rsom.npy\", cordis_matrix)\n",
    "\n",
    "del rsp, cordis_matrix # clear the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the pairwise distance into 100 segments and calculate the average correlation and standard deviation of all correlation estimates within each segment\n",
    "def cordis_avg(cordis_matrix, segment_num=100):\n",
    "    num_roi = cordis_matrix.shape[0]\n",
    "    assert num_roi == cordis_matrix.shape[1]\n",
    "    pd = cordis_matrix[:, :, 1] # distance (3048, 3048)\n",
    "    pc = cordis_matrix[:, :, 0] # correlation (3048, 3048)\n",
    "    dismax = np.max(pd) # maximum distance\n",
    "    dismin = np.min(pd) # minimum distance\n",
    "    segment_len = (dismax - dismin) / segment_num # length of one single segment\n",
    "    segments = np.zeros((num_roi, segment_num, 2)) # store the mean correlation and standard deviation of all correlation estimates within all samples of each segment\n",
    "    for i in tqdm(range(num_roi), desc=\"sorting...\", disable=True):\n",
    "        sorted_indices = np.argsort(pd[i, :]) # sort the distance from smallest to largest, get its indices\n",
    "        pd[i, :] = pd[i, sorted_indices] # distance matrix row vector sorted, from smallest to largest\n",
    "        pc[i, :] = pc[i, sorted_indices] # correlation matrix row vector sorted, from smallest to largest\n",
    "        for s in range(segment_num):\n",
    "            # indices of all entries within the current pairwise distance segment\n",
    "            segment_indices = np.where((pd[i, :] >= dismin + s * segment_len) & (pd[i, :] < dismin + (s + 1) * segment_len))[0]\n",
    "            if len(segment_indices) > 0:\n",
    "                selected_cor = pc[i, segment_indices] # current segment samples' correlation estimates\n",
    "                segments[i, s, 0] = np.mean(selected_cor) # average\n",
    "                segments[i, s, 1] = np.std(selected_cor) # standard deviation\n",
    "            else:\n",
    "                segments[i, s, 0] = -1\n",
    "                segments[i, s, 1] = -1\n",
    "    # compute the average of all ROIs' correlation estimates (mean and std) within each segment\n",
    "    mean_std_rois = np.zeros((segment_num, 3))\n",
    "    for i in range(num_roi):\n",
    "        for s in range(segment_num):\n",
    "            if segments[i, s, 0] != -1 and segments[i, s, 1] != -1:\n",
    "                mean_std_rois[s, 0] += segments[i, s, 0] # mean\n",
    "                mean_std_rois[s, 1] += segments[i, s, 1] # std\n",
    "                mean_std_rois[s, 2] += 1 # count\n",
    "    mean_std_rois[:, 0] /= mean_std_rois[:, 2] # average mean\n",
    "    mean_std_rois[:, 1] /= mean_std_rois[:, 2] # average std\n",
    "    mean_std_rois = mean_std_rois[:, :2] # discard the count\n",
    "    return mean_std_rois\n",
    "\n",
    "cordis_som = cordis_avg(np.load(folder_path + \"SOM/cordis_V4_som.npy\"), segment_num=110) # SOM\n",
    "cordis_v4 = cordis_avg(np.load(folder_path + \"V4DT/cordis_v4_benchmark.npy\"), segment_num=100) # V4 benchmark\n",
    "cordis_rsom = cordis_avg(np.load(folder_path + \"RSOM/cordis_V4_rsom.npy\"), segment_num=110) # RSOM\n",
    "print(pearsonr(cordis_som[:100, 0], cordis_v4[:, 0])[0]) # SOM vs V4\n",
    "print(pearsonr(cordis_rsom[:100, 0], cordis_v4[:, 0])[0]) # RSOM vs V4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOM: polar_angle_som, eccentricity_som, dispersity_som\n",
    "som_matched = np.load(folder_path + \"SOM/assigned.npz\")\n",
    "polar_angle_som = som_matched['polar_angle_som']\n",
    "eccentricity_som = som_matched['eccentricity_som']\n",
    "dispersity_som = som_matched['dispersity_som']\n",
    "polar_angle_som = np.swapaxes(polar_angle_som, 0, 1) # (60, 60, 3)\n",
    "polar_angle_grey = 0.2989 * polar_angle_som[:, :, 0] + 0.5870 * polar_angle_som[:, :, 1] + 0.1140 * polar_angle_som[:, :, 2]\n",
    "polar_angle_grey = gaussian_filter(polar_angle_grey, sigma=4)  # Adjust sigma for more or less smoothing, shape (60, 60)\n",
    "eccentricity_som = np.swapaxes(eccentricity_som, 0, 1) # (60, 60, 3)\n",
    "eccentricity_grey = 0.2989 * eccentricity_som[:, :, 0] + 0.5870 * eccentricity_som[:, :, 1] + 0.1140 * eccentricity_som[:, :, 2]\n",
    "eccentricity_grey = gaussian_filter(eccentricity_grey, sigma=4)  # (60, 60)\n",
    "dispersity_som = np.swapaxes(dispersity_som, 0, 1) # (60, 60, 3)\n",
    "cmap_black = ListedColormap(['black', 'red', 'green', 'blue', 'yellow', 'magenta', 'cyan', 'orange', 'purple', \n",
    "                             'brown', 'pink', 'lime', 'teal', 'navy', 'gold', 'silver', 'coral']) # black for unassigned 0\n",
    "domain_colors = ['red', 'green', 'blue', 'yellow', 'magenta', 'cyan', 'orange', 'purple', 'brown', 'pink', 'lime', 'teal', 'navy', 'gold', 'silver', 'coral']\n",
    "\n",
    "fig, axes = plt.subplots(1, 7, figsize=(21, 3))\n",
    "plt.subplots_adjust(wspace=1) # Adjust the horizontal distance between subplots\n",
    "fs_title = 16 # set font size for the title\n",
    "\"\"\"\n",
    "axes[0].bar(np.arange(len(labels_som)), sizes_som, color=cmap_black(labels_som), width=1)\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "axes[0].set_title(\"Component Size\", fontsize=fs_title)\n",
    "\"\"\"\n",
    "axes[0].plot(cordis_som[:100, 0], color='black')\n",
    "axes[0].fill_between(np.arange((len(cordis_som[:100, 0]))), cordis_som[:100, 0]-cordis_som[:100, 1], cordis_som[:100, 0]+cordis_som[:100, 1], color='black', alpha=0.3)\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "axes[0].set_xlabel(\"Distance\", fontsize=fs_title)\n",
    "axes[0].set_ylabel('Tuning Correlation', fontsize=fs_title)\n",
    "axes[0].spines['top'].set_visible(False)\n",
    "axes[0].spines['right'].set_visible(False)\n",
    "\n",
    "axes[1].imshow(cc_color_som, cmap=cmap_black)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('SOM', fontsize=fs_title, pad=8)\n",
    "\n",
    "heatmap_distance = axes[2].imshow(position_som, cmap='viridis')  # Use a perceptually uniform colormap for values\n",
    "# Add a colorbar for the heatmap values\n",
    "cbar_distance = plt.colorbar(heatmap_distance, ax=axes[2], shrink=0.9, aspect=20)\n",
    "cbar_distance.set_ticks([0, 1])  # Set ticks to only 0 and 1\n",
    "cbar_distance.set_ticklabels(['0', '1'])  # Set tick labels to display as '0' and '1'\n",
    "# Add rectangular boxes for the x-ticks (bottom) and y-ticks (left)\n",
    "offset = 0.3  # Add white space between rectangles and heatmap\n",
    "for i, color in enumerate(domain_colors):\n",
    "    # Add a rectangle at the bottom for the x-axis (adjust the y-position)\n",
    "    axes[2].add_patch(Rectangle((i - 0.5, -1.5 - offset), 1, 1, color=color, transform=axes[2].transData, clip_on=False))\n",
    "    # Add a rectangle to the left for the y-axis (adjust the x-position)\n",
    "    axes[2].add_patch(Rectangle((-1.5 - offset, i - 0.5), 1, 1, color=color, transform=axes[2].transData, clip_on=False))\n",
    "axes[2].set_xticks([])\n",
    "axes[2].set_yticks([])\n",
    "axes[2].set_title(\"Distance Matrix\", fontsize=fs_title, pad=20) # \"pad\" adjusts the distance between the title and the heatmap\n",
    "\n",
    "heatmap_adjacency = axes[3].imshow(adj_matrix_som, cmap='viridis')  # Use a perceptually uniform colormap for values\n",
    "# Add a colorbar for the heatmap values\n",
    "cbar_adjacency = plt.colorbar(heatmap_distance, ax=axes[3], shrink=0.9, aspect=20)\n",
    "cbar_adjacency.set_ticks([0, 1])  # Set ticks to only 0 and 1\n",
    "cbar_adjacency.set_ticklabels(['0', '1'])  # Set tick labels to display as '0' and '1'\n",
    "# Add rectangular boxes for the x-ticks (bottom) and y-ticks (left)\n",
    "offset = 0.3  # Add white space between rectangles and heatmap\n",
    "for i, color in enumerate(domain_colors):\n",
    "    # Add a rectangle at the bottom for the x-axis (adjust the y-position)\n",
    "    axes[3].add_patch(Rectangle((i - 0.5, -1.5 - offset), 1, 1, color=color, transform=axes[3].transData, clip_on=False))\n",
    "    # Add a rectangle to the left for the y-axis (adjust the x-position)\n",
    "    axes[3].add_patch(Rectangle((-1.5 - offset, i - 0.5), 1, 1, color=color, transform=axes[3].transData, clip_on=False))\n",
    "axes[3].set_xticks([])\n",
    "axes[3].set_yticks([])\n",
    "axes[3].set_title(\"Adjacency Matrix\", fontsize=fs_title, pad=20) # \"pad\" adjusts the distance between the title and the heatmap\n",
    "\n",
    "axes[4].imshow(polar_angle_som)\n",
    "axes[4].set_title(\"Polar Angle\", fontsize=fs_title, pad=8)\n",
    "polar_contours = axes[4].contour(polar_angle_grey, levels=1, colors='white', linewidths=1.5)\n",
    "axes[4].axis('off')\n",
    "\n",
    "axes[5].imshow(eccentricity_som)\n",
    "axes[5].set_title(\"Eccentricity\", fontsize=fs_title, pad=8)\n",
    "eccentricity_contours = axes[5].contour(eccentricity_grey, levels=2, colors='white', linewidths=1.5)\n",
    "axes[5].axis('off')\n",
    "\n",
    "axes[6].imshow(dispersity_som)\n",
    "axes[6].set_title(\"Dispersity\", fontsize=fs_title, pad=8)\n",
    "axes[6].axis('off')\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "plt.savefig(folder_path + \"Fig2/SOM.png\", dpi=1000)\n",
    "del fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128) <class 'numpy.ndarray'>\n",
    "polar_angle = sio.loadmat(folder_path + \"V4DT/RF_results/polar_angle.mat\")['map'] # (128, 128, 3) <class 'numpy.ndarray'>\n",
    "eccentricity = sio.loadmat(folder_path + \"V4DT/RF_results/eccentricity.mat\")['map'] # (128, 128, 3) <class 'numpy.ndarray'>\n",
    "white = np.ones((3))\n",
    "for i in range(roi.shape[0]): # white out all non-roi voxels\n",
    "    for j in range(roi.shape[1]):\n",
    "        if roi[i, j] != 1:\n",
    "            polar_angle[i, j, :] = white\n",
    "            eccentricity[i, j, :] = white\n",
    "# polar angle\n",
    "polar_angle = np.flip(polar_angle[14:90, 23:90, :], axis=(0, 1))\n",
    "polar_angle_grey = 0.2989 * polar_angle[:, :, 0] + 0.5870 * polar_angle[:, :, 1] + 0.1140 * polar_angle[:, :, 2]\n",
    "polar_angle_grey = gaussian_filter(polar_angle_grey, sigma=2.95) # Apply Gaussian smoothing\n",
    "roi_eroded = binary_erosion(roi, structure=np.ones((8,8))) # Erode the ROI mask slightly to remove edge effects\n",
    "roi_eroded = np.flip(roi_eroded[14:90, 23:90]) # flip the mask top-bottom\n",
    "polar_angle_grey = np.where(roi_eroded, polar_angle_grey, np.nan) # Mask non-ROI regions with NaN\n",
    "# eccentricity\n",
    "eccentricity = np.flip(eccentricity[14:90, 23:90, :], axis=(0, 1))\n",
    "eccentricity_grey = 0.2989 * eccentricity[:, :, 0] + 0.5870 * eccentricity[:, :, 1] + 0.1140 * eccentricity[:, :, 2]\n",
    "eccentricity_grey = gaussian_filter(eccentricity_grey, sigma=2) # Apply Gaussian smoothing\n",
    "roi_eroded = binary_erosion(roi, structure=np.ones((6,6))) # Erode the ROI mask slightly to remove edge effects\n",
    "roi_eroded = np.flip(roi_eroded[14:90, 23:90]) # flip the mask top-bottom\n",
    "eccentricity_grey = np.where(roi_eroded, eccentricity_grey, np.nan) # Mask non-ROI regions with NaN\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "axes[0].imshow(polar_angle)\n",
    "polar_contours = axes[0].contour(polar_angle_grey, levels=1, colors='white', linewidths=1.5)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(eccentricity)\n",
    "eccentricity_contours = axes[1].contour(eccentricity_grey, levels=2, colors='white', linewidths=1.5)\n",
    "axes[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the retinotopy and dispersity for the V4 benchmark\n",
    "# dispersity\n",
    "dispersity = sio.loadmat(folder_path + \"V4DT/Dispersity_results/dispersity.mat\")['cmap'] # (128, 128, 3) <class 'numpy.ndarray'>\n",
    "# load roi, mask non-roi voxels to white\n",
    "roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128) <class 'numpy.ndarray'>\n",
    "polar_angle = sio.loadmat(folder_path + \"V4DT/RF_results/polar_angle.mat\")['map'] # (128, 128, 3) <class 'numpy.ndarray'>\n",
    "eccentricity = sio.loadmat(folder_path + \"V4DT/RF_results/eccentricity.mat\")['map'] # (128, 128, 3) <class 'numpy.ndarray'>\n",
    "white = np.ones((3))\n",
    "for i in range(roi.shape[0]): # white out all non-roi voxels\n",
    "    for j in range(roi.shape[1]):\n",
    "        if roi[i, j] != 1:\n",
    "            polar_angle[i, j, :] = white\n",
    "            eccentricity[i, j, :] = white\n",
    "            dispersity[i, j, :] = white\n",
    "# polar angle\n",
    "polar_angle = np.flip(polar_angle[14:90, 23:90, :], axis=(0, 1))\n",
    "polar_angle_grey = 0.2989 * polar_angle[:, :, 0] + 0.5870 * polar_angle[:, :, 1] + 0.1140 * polar_angle[:, :, 2]\n",
    "polar_angle_grey = gaussian_filter(polar_angle_grey, sigma=2.95) # Apply Gaussian smoothing\n",
    "roi_eroded = binary_erosion(roi, structure=np.ones((8,8))) # Erode the ROI mask slightly to remove edge effects\n",
    "roi_eroded = np.flip(roi_eroded[14:90, 23:90]) # flip the mask top-bottom\n",
    "polar_angle_grey = np.where(roi_eroded, polar_angle_grey, np.nan) # Mask non-ROI regions with NaN\n",
    "# eccentricity\n",
    "eccentricity = np.flip(eccentricity[14:90, 23:90, :], axis=(0, 1))\n",
    "eccentricity_grey = 0.2989 * eccentricity[:, :, 0] + 0.5870 * eccentricity[:, :, 1] + 0.1140 * eccentricity[:, :, 2]\n",
    "eccentricity_grey = gaussian_filter(eccentricity_grey, sigma=2) # Apply Gaussian smoothing\n",
    "roi_eroded = binary_erosion(roi, structure=np.ones((6,6))) # Erode the ROI mask slightly to remove edge effects\n",
    "roi_eroded = np.flip(roi_eroded[14:90, 23:90]) # flip the mask top-bottom\n",
    "eccentricity_grey = np.where(roi_eroded, eccentricity_grey, np.nan) # Mask non-ROI regions with NaN\n",
    "\n",
    "cmap_wb = ListedColormap(['white', 'black', 'red', 'green', 'blue', 'yellow', 'magenta', 'cyan', 'orange', 'purple', \n",
    "                             'brown', 'pink', 'lime', 'teal', 'navy', 'gold', 'silver', 'coral']) # white for non-ROIs, black for erased\n",
    "\n",
    "fig, axes = plt.subplots(1, 7, figsize=(21, 3))\n",
    "fs_title = 16 # set font size for the title\n",
    "\n",
    "\"\"\"\n",
    "axes[0].bar(np.arange(len(labels_v4)), sizes_v4, color=cmap_black(labels_v4), width=1)\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "\"\"\"\n",
    "axes[0].plot(cordis_v4[:100, 0], color='black')\n",
    "axes[0].fill_between(np.arange((len(cordis_v4[:100, 0]))), cordis_v4[:100, 0]-cordis_v4[:100, 1], cordis_v4[:100, 0]+cordis_v4[:100, 1], color='black', alpha=0.3)\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "axes[0].set_xlabel(\"Distance\", fontsize=fs_title)\n",
    "axes[0].set_ylabel('Tuning Correlation', fontsize=fs_title)\n",
    "axes[0].spines['top'].set_visible(False)\n",
    "axes[0].spines['right'].set_visible(False)\n",
    "\n",
    "axes[1].imshow(cc_color_v4, cmap=cmap_wb)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('V4', fontsize=fs_title)\n",
    "\n",
    "heatmap_distance = axes[2].imshow(position_v4, cmap='viridis')  # Use a perceptually uniform colormap for values\n",
    "# Add a colorbar for the heatmap values\n",
    "cbar_distance = plt.colorbar(heatmap_distance, ax=axes[2], shrink=0.9, aspect=20)\n",
    "cbar_distance.set_ticks([0, 1])  # Set ticks to only 0 and 1\n",
    "cbar_distance.set_ticklabels(['0', '1'])  # Set tick labels to display as '0' and '1'\n",
    "# Add rectangular boxes for the x-ticks (bottom) and y-ticks (left)\n",
    "offset = 0.3  # Add white space between rectangles and heatmap\n",
    "for i, color in enumerate(domain_colors):\n",
    "    # Add a rectangle at the bottom for the x-axis (adjust the y-position)\n",
    "    axes[2].add_patch(Rectangle((i - 0.5, -1.5 - offset), 1, 1, color=color, transform=axes[2].transData, clip_on=False))\n",
    "    # Add a rectangle to the left for the y-axis (adjust the x-position)\n",
    "    axes[2].add_patch(Rectangle((-1.5 - offset, i - 0.5), 1, 1, color=color, transform=axes[2].transData, clip_on=False))\n",
    "axes[2].set_xticks([])\n",
    "axes[2].set_yticks([])\n",
    "\n",
    "heatmap_adjacency = axes[3].imshow(adj_matrix_v4, cmap='viridis')  # Use a perceptually uniform colormap for values\n",
    "# Add a colorbar for the heatmap values\n",
    "cbar_adjacency = plt.colorbar(heatmap_distance, ax=axes[3], shrink=0.9, aspect=20)\n",
    "cbar_adjacency.set_ticks([0, 1])  # Set ticks to only 0 and 1\n",
    "cbar_adjacency.set_ticklabels(['0', '1'])  # Set tick labels to display as '0' and '1'\n",
    "# Add rectangular boxes for the x-ticks (bottom) and y-ticks (left)\n",
    "offset = 0.3  # Add white space between rectangles and heatmap\n",
    "for i, color in enumerate(domain_colors):\n",
    "    # Add a rectangle at the bottom for the x-axis (adjust the y-position)\n",
    "    axes[3].add_patch(Rectangle((i - 0.5, -1.5 - offset), 1, 1, color=color, transform=axes[3].transData, clip_on=False))\n",
    "    # Add a rectangle to the left for the y-axis (adjust the x-position)\n",
    "    axes[3].add_patch(Rectangle((-1.5 - offset, i - 0.5), 1, 1, color=color, transform=axes[3].transData, clip_on=False))\n",
    "axes[3].set_xticks([])\n",
    "axes[3].set_yticks([])\n",
    "\"\"\"\n",
    "axes[4].imshow(np.flip(polar_angle[14:90, 23:90, :], axis=(0, 1)))\n",
    "axes[4].axis('off')\n",
    "axes[5].imshow(np.flip(eccentricity[14:90, 23:90, :], axis=(0, 1)))\n",
    "axes[5].axis('off')\n",
    "\"\"\"\n",
    "axes[4].imshow(polar_angle)\n",
    "polar_contours = axes[4].contour(polar_angle_grey, levels=1, colors='white', linewidths=1.5)\n",
    "axes[4].axis('off')\n",
    "\n",
    "axes[5].imshow(eccentricity)\n",
    "eccentricity_contours = axes[5].contour(eccentricity_grey, levels=2, colors='white', linewidths=1.5)\n",
    "axes[5].axis('off')\n",
    "\n",
    "axes[6].imshow(np.flip(dispersity[14:90, 23:90, :], axis=(0, 1)))\n",
    "axes[6].axis('off')\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "plt.savefig(folder_path + \"Fig2/V4.png\", dpi=1000)\n",
    "del fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSOM: polar_angle_rsom, eccentricity_rsom, dispersity_rsom\n",
    "rsom_matched = np.load(folder_path + \"RSOM/assigned.npz\")\n",
    "polar_angle_rsom = rsom_matched['polar_angle_rsom']\n",
    "eccentricity_rsom = rsom_matched['eccentricity_rsom']\n",
    "dispersity_rsom = rsom_matched['dispersity_rsom']\n",
    "polar_angle_rsom = np.swapaxes(polar_angle_rsom, 0, 1) # (60, 60, 3)\n",
    "polar_angle_grey = 0.2989 * polar_angle_rsom[:, :, 0] + 0.5870 * polar_angle_rsom[:, :, 1] + 0.1140 * polar_angle_rsom[:, :, 2]\n",
    "polar_angle_grey = gaussian_filter(polar_angle_grey, sigma=4)  # Adjust sigma for more or less smoothing, shape (60, 60)\n",
    "eccentricity_rsom = np.swapaxes(eccentricity_rsom, 0, 1) # (60, 60, 3)\n",
    "eccentricity_grey = 0.2989 * eccentricity_rsom[:, :, 0] + 0.5870 * eccentricity_rsom[:, :, 1] + 0.1140 * eccentricity_rsom[:, :, 2]\n",
    "eccentricity_grey = gaussian_filter(eccentricity_grey, sigma=4)  # (60, 60)\n",
    "dispersity_rsom = np.swapaxes(dispersity_rsom, 0, 1) # (60, 60, 3)\n",
    "cmap_black = ListedColormap(['black', 'red', 'green', 'blue', 'yellow', 'magenta', 'cyan', 'orange', 'purple', \n",
    "                             'brown', 'pink', 'lime', 'teal', 'navy', 'gold', 'silver', 'coral']) # black for unassigned 0\n",
    "domain_colors = ['red', 'green', 'blue', 'yellow', 'magenta', 'cyan', 'orange', 'purple', 'brown', 'pink', 'lime', 'teal', 'navy', 'gold', 'silver', 'coral']\n",
    "\n",
    "fig, axes = plt.subplots(1, 7, figsize=(21, 3))\n",
    "fs_title = 16 # set font size for the title\n",
    "\"\"\"\n",
    "axes[0].bar(np.arange(len(labels_rsom)), sizes_rsom, color=cmap_black(labels_rsom), width=1)\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "\"\"\"\n",
    "axes[0].plot(cordis_rsom[:100, 0], color='black')\n",
    "axes[0].fill_between(np.arange((len(cordis_rsom[:100, 0]))), cordis_rsom[:100, 0]-cordis_rsom[:100, 1], cordis_rsom[:100, 0]+cordis_rsom[:100, 1], color='black', alpha=0.3)\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "axes[0].set_xlabel(\"Distance\", fontsize=fs_title)\n",
    "axes[0].set_ylabel('Tuning Correlation', fontsize=fs_title)\n",
    "axes[0].spines['top'].set_visible(False)\n",
    "axes[0].spines['right'].set_visible(False)\n",
    "\n",
    "axes[1].imshow(cc_color_rsom, cmap=cmap_black)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('RSOM', fontsize=fs_title)\n",
    "\n",
    "heatmap_distance = axes[2].imshow(position_rsom, cmap='viridis')  # Use a perceptually uniform colormap for values\n",
    "# Add a colorbar for the heatmap values\n",
    "cbar_distance = plt.colorbar(heatmap_distance, ax=axes[2], shrink=0.9, aspect=20)\n",
    "cbar_distance.set_ticks([0, 1])  # Set ticks to only 0 and 1\n",
    "cbar_distance.set_ticklabels(['0', '1'])  # Set tick labels to display as '0' and '1'\n",
    "# Add rectangular boxes for the x-ticks (bottom) and y-ticks (left)\n",
    "offset = 0.3  # Add white space between rectangles and heatmap\n",
    "for i, color in enumerate(domain_colors):\n",
    "    # Add a rectangle at the bottom for the x-axis (adjust the y-position)\n",
    "    axes[2].add_patch(Rectangle((i - 0.5, -1.5 - offset), 1, 1, color=color, transform=axes[2].transData, clip_on=False))\n",
    "    # Add a rectangle to the left for the y-axis (adjust the x-position)\n",
    "    axes[2].add_patch(Rectangle((-1.5 - offset, i - 0.5), 1, 1, color=color, transform=axes[2].transData, clip_on=False))\n",
    "axes[2].set_xticks([])\n",
    "axes[2].set_yticks([])\n",
    "\n",
    "heatmap_adjacency = axes[3].imshow(adj_matrix_rsom, cmap='viridis')  # Use a perceptually uniform colormap for values\n",
    "# Add a colorbar for the heatmap values\n",
    "cbar_adjacency = plt.colorbar(heatmap_distance, ax=axes[3], shrink=0.9, aspect=20)\n",
    "cbar_adjacency.set_ticks([0, 1])  # Set ticks to only 0 and 1\n",
    "cbar_adjacency.set_ticklabels(['0', '1'])  # Set tick labels to display as '0' and '1'\n",
    "# Add rectangular boxes for the x-ticks (bottom) and y-ticks (left)\n",
    "offset = 0.3  # Add white space between rectangles and heatmap\n",
    "for i, color in enumerate(domain_colors):\n",
    "    # Add a rectangle at the bottom for the x-axis (adjust the y-position)\n",
    "    axes[3].add_patch(Rectangle((i - 0.5, -1.5 - offset), 1, 1, color=color, transform=axes[3].transData, clip_on=False))\n",
    "    # Add a rectangle to the left for the y-axis (adjust the x-position)\n",
    "    axes[3].add_patch(Rectangle((-1.5 - offset, i - 0.5), 1, 1, color=color, transform=axes[3].transData, clip_on=False))\n",
    "axes[3].set_xticks([])\n",
    "axes[3].set_yticks([])\n",
    "\n",
    "axes[4].imshow(polar_angle_rsom)\n",
    "polar_contours = axes[4].contour(polar_angle_grey, levels=1, colors='white', linewidths=1.5)\n",
    "axes[4].axis('off')\n",
    "\n",
    "axes[5].imshow(eccentricity_rsom)\n",
    "eccentricity_contours = axes[5].contour(eccentricity_grey, levels=2, colors='white', linewidths=1.5)\n",
    "axes[5].axis('off')\n",
    "\n",
    "axes[6].imshow(dispersity_rsom)\n",
    "axes[6].axis('off')\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "plt.savefig(folder_path + \"Fig2/RSOM.png\", dpi=1000)\n",
    "del fig, axes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topoV4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
