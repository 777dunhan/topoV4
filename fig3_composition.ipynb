{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "from scipy.ndimage import label, sum as ndi_sum, binary_erosion\n",
    "import matplotlib.image\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap, to_rgba\n",
    "from PIL import Image\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "folder_path = \"/path/to/your/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example grid tuning curve composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a matched som grid corresponding domain number, remove the unconnected components smaller than a threshold\n",
    "def connected_components(matched, threshold, return_largest=False):\n",
    "    # Define the structure (connectivity) for connected components\n",
    "    structure = np.ones((3, 3), dtype=int)  # 8-connectivity, meaning that both adjacent and diagonal pixels are considered as neighbors\n",
    "    connected = np.copy(matched) # Initialize an array to store the output\n",
    "    if return_largest: largest = np.copy(matched) # Initialize an array to store the largest connected component\n",
    "    for class_num in np.unique(matched): # Process each class separately\n",
    "        if class_num == 0: continue # Skip the background\n",
    "        class_mask = (matched == class_num) # Create a binary mask for the current class\n",
    "        labeled_array, num_features = label(class_mask, structure=structure) # Find connected components in the binary mask\n",
    "        component_sizes = ndi_sum(class_mask, labeled_array, index=range(1, num_features + 1)) # Compute the size of each component\n",
    "        for i, size in enumerate(component_sizes): # Zero out components smaller than the threshold\n",
    "            if size < threshold: connected[labeled_array == (i + 1)] = 0\n",
    "        if return_largest:\n",
    "            for i, size in enumerate(component_sizes):\n",
    "                if size != np.max(component_sizes): largest[labeled_array == (i + 1)] = 0\n",
    "    if return_largest: return connected, largest\n",
    "    else: return connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSOM domain centrality relative to boundaries\n",
    "rng = np.random.default_rng(0)\n",
    "name = \"RSOM\"\n",
    "# (60, 60) of 16 V4 voxel types, only connected components perserved\n",
    "idx_rsom = np.load(folder_path + name + \"/idx_newassign.npy\").astype(int)\n",
    "# simulation weight\n",
    "weight_rsom = np.load(folder_path + name + \"/weights.npy\")[:, :, :50000] # (60, 60, 50000)\n",
    "# V4 digital twin data\n",
    "# load V4 digital twin benchmark\n",
    "V4rsp = np.load(folder_path + \"V4DT/PRsp.npy\") # (50000, 128, 128)\n",
    "roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128); (x, y switch) and transpose back to match the others\n",
    "V4idx = np.load(folder_path + \"V4DT/idx.npy\") # (128, 128), v4 connected components\n",
    "V4idx, _ = connected_components(V4idx, threshold=10, return_largest=True) # remove the unconnected components smaller than a threshold of 10, also return the largest connected component\n",
    "# prepare the full training data, rsp to 50k images\n",
    "train_all = np.zeros((np.sum(roi), V4rsp.shape[0]))\n",
    "entry = 0\n",
    "for i in range(roi.shape[0]):\n",
    "    for j in range(roi.shape[1]):\n",
    "        if roi[i, j] == 1:\n",
    "            train_all[entry] = V4rsp[:, i, j]\n",
    "            entry += 1\n",
    "# create a 1d vector of all 3048 V4 columns' domain assignment\n",
    "columns_domain = np.zeros((int(np.sum(roi)))) # the domain assignment for all 3048 V4 voxels\n",
    "count = 0\n",
    "for i in range(128):\n",
    "    for j in range(128):\n",
    "        if roi[i, j] == 1:\n",
    "            columns_domain[count] = V4idx[i, j]\n",
    "            count += 1\n",
    "assert count == columns_domain.shape[0]\n",
    "# args\n",
    "device = \"cpu\"\n",
    "top_img_num = 1000\n",
    "epoch_num = 7500\n",
    "learning_rate = 0.05\n",
    "alpha = 0.01 # L1 regularization strength\n",
    "top_column_num = 15\n",
    "num_units_sample = 5\n",
    "# define the linear regression model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, num_rows):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(num_rows, device=device)) # train from scratch\n",
    "    def forward(self, rsp): return torch.sum(self.weights.view(-1, 1) * rsp, dim=0)\n",
    "# identify connectivity and boundary\n",
    "structure_conn = np.ones((3, 3), dtype=int)\n",
    "structure_boundary = np.ones((3, 3), dtype=bool)\n",
    "# statistics\n",
    "central_homogeneous_all = []\n",
    "boundary_homogeneous_all = []\n",
    "# iterate through all domains\n",
    "for domain_id in range(1, 17):\n",
    "    mask = idx_rsom == domain_id\n",
    "    if not np.any(mask):\n",
    "        print(f\"Domain {domain_id}: no units\")\n",
    "        continue\n",
    "    labeled, num = label(mask, structure=structure_conn)\n",
    "    if num == 0:\n",
    "        print(f\"Domain {domain_id}: no connected components\")\n",
    "        continue\n",
    "    component_sizes = ndi_sum(mask, labeled, index=range(1, num + 1))\n",
    "    largest_label = int(np.argmax(component_sizes) + 1)\n",
    "    largest_mask = labeled == largest_label\n",
    "    boundary_mask = largest_mask & (~binary_erosion(largest_mask, structure=structure_boundary, border_value=0))\n",
    "    boundary_coords = np.argwhere(boundary_mask)\n",
    "    interior_coords = np.argwhere(largest_mask & (~boundary_mask))\n",
    "    print(\n",
    "        f\"Domain {domain_id}: largest component {largest_mask.sum()} units, {len(boundary_coords)} boundary, {len(interior_coords)} interior\"\n",
    "    )\n",
    "    if len(boundary_coords) == 0 or len(interior_coords) == 0:\n",
    "        print(\"  Skipping distance ranking (missing boundary or interior)\")\n",
    "        continue\n",
    "    # distance between interior and boundary unit, the smallest variance yields central units\n",
    "    distances = np.linalg.norm(interior_coords[:, None, :] - boundary_coords[None, :, :], axis=2)\n",
    "    mean_dist = distances.mean(axis=1)\n",
    "    var_dist = distances.var(axis=1)\n",
    "    top_indices = np.argsort(var_dist)[:num_units_sample] # indices of central units with smallest variance\n",
    "    # iterate through current domain identified central units\n",
    "    central_homogeneous = []\n",
    "    for i in top_indices:\n",
    "        interior_x = int(interior_coords[i][0]) # row index of central units\n",
    "        interior_y = int(interior_coords[i][1]) # col index of central units\n",
    "        weight_target = weight_rsom[interior_x, interior_y, :]\n",
    "        # if restricted to the top-1k responsive images out of 50k, prepare data\n",
    "        if top_img_num != 50000:\n",
    "            image_label = np.arange(50000) # 0indexed image names\n",
    "            _, image_label = zip(*sorted(zip(weight_target, image_label))) # sort the mean responses (from small to large) and the image_label according to the order of mean responses\n",
    "            image_label = np.flip(image_label[-top_img_num:]) # take selected most responsive images' 0index (from large to small)\n",
    "            train = train_all[:, image_label] # select the most responsive images\n",
    "            weight_target = weight_target[image_label] # select the most responsive images' mean responses\n",
    "        weight_target = torch.tensor(weight_target, device=device, dtype=torch.float32) # target data\n",
    "        train = torch.tensor(train.astype(np.float32), device=device) # training data of shape (3048, ...)\n",
    "        # initialize the model, loss function, and optimizer\n",
    "        model = LinearRegressionModel(num_rows=np.sum(roi)).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epoch_num, gamma=1) # no learning rate decay\n",
    "        pbar = tqdm(range(epoch_num), desc=\"train...\", disable=True)\n",
    "        # train the model\n",
    "        for epoch in pbar: # Forward pass: Compute predicted y by passing xs to the model\n",
    "            y_pred = model(train)\n",
    "            l1_regularization = alpha * torch.sum(torch.abs(model.weights)) # L1 LASSO penalty to encourage sparsity\n",
    "            loss = criterion(y_pred, weight_target) + l1_regularization # the total loss\n",
    "            optimizer.zero_grad() # Backward pass: Compute gradients\n",
    "            loss.backward()\n",
    "            optimizer.step() # Update weights\n",
    "            scheduler.step() # Update learning rate\n",
    "            pbar.set_postfix({\"Loss\": loss.item()})\n",
    "        regression_w = model.weights.detach().cpu().numpy() # The trained weights (coefficients) of the linear regression model (3048,)\n",
    "        final_loss = loss.item() # Final error measurement\n",
    "        # homogeneous connection statistics\n",
    "        regression_w, columns_domain_sorted = zip(*sorted(zip(regression_w, columns_domain), reverse=True))\n",
    "        regression_w = np.array(regression_w)[:top_column_num]\n",
    "        columns_domain_sorted = np.array(columns_domain_sorted).astype(int)[:top_column_num]\n",
    "        homogeneous_w = 0\n",
    "        for i in range(top_column_num):\n",
    "            if columns_domain_sorted[i] == domain_id: homogeneous_w += regression_w[i]\n",
    "        percentage = homogeneous_w / np.sum(regression_w)\n",
    "        central_homogeneous.append(percentage)\n",
    "        central_homogeneous_all.append(percentage)\n",
    "\n",
    "    # iterate through current domain identified boundary units\n",
    "    sampled_idx = rng.choice(len(boundary_coords), size=min(num_units_sample, len(boundary_coords)), replace=False)\n",
    "    boundary_homogeneous = []\n",
    "    for i in sampled_idx:\n",
    "        boundary_x = int(boundary_coords[i][0]) # row index of boundary units\n",
    "        boundary_y = int(boundary_coords[i][1]) # col index of boundary units\n",
    "        weight_target = weight_rsom[boundary_x, boundary_y, :]\n",
    "        # if restricted to the top-1k responsive images out of 50k, prepare data\n",
    "        if top_img_num != 50000:\n",
    "            image_label = np.arange(50000) # 0indexed image names\n",
    "            _, image_label = zip(*sorted(zip(weight_target, image_label))) # sort the mean responses (from small to large) and the image_label according to the order of mean responses\n",
    "            image_label = np.flip(image_label[-top_img_num:]) # take selected most responsive images' 0index (from large to small)\n",
    "            train = train_all[:, image_label] # select the most responsive images\n",
    "            weight_target = weight_target[image_label] # select the most responsive images' mean responses\n",
    "        weight_target = torch.tensor(weight_target, device=device, dtype=torch.float32) # target data\n",
    "        train = torch.tensor(train.astype(np.float32), device=device) # training data of shape (3048, ...)\n",
    "        # initialize the model, loss function, and optimizer\n",
    "        model = LinearRegressionModel(num_rows=np.sum(roi)).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epoch_num, gamma=1) # no learning rate decay\n",
    "        pbar = tqdm(range(epoch_num), desc=\"train...\", disable=True)\n",
    "        # train the model\n",
    "        for epoch in pbar: # Forward pass: Compute predicted y by passing xs to the model\n",
    "            y_pred = model(train)\n",
    "            l1_regularization = alpha * torch.sum(torch.abs(model.weights)) # L1 LASSO penalty to encourage sparsity\n",
    "            loss = criterion(y_pred, weight_target) + l1_regularization # the total loss\n",
    "            optimizer.zero_grad() # Backward pass: Compute gradients\n",
    "            loss.backward()\n",
    "            optimizer.step() # Update weights\n",
    "            scheduler.step() # Update learning rate\n",
    "            pbar.set_postfix({\"Loss\": loss.item()})\n",
    "        regression_w = model.weights.detach().cpu().numpy() # The trained weights (coefficients) of the linear regression model (3048,)\n",
    "        final_loss = loss.item() # Final error measurement\n",
    "        # homogeneous connection statistics\n",
    "        regression_w, columns_domain_sorted = zip(*sorted(zip(regression_w, columns_domain), reverse=True))\n",
    "        regression_w = np.array(regression_w)[:top_column_num]\n",
    "        columns_domain_sorted = np.array(columns_domain_sorted).astype(int)[:top_column_num]\n",
    "        homogeneous_w = 0\n",
    "        for i in range(top_column_num):\n",
    "            if columns_domain_sorted[i] == domain_id: homogeneous_w += regression_w[i]\n",
    "        percentage = homogeneous_w / np.sum(regression_w)\n",
    "        boundary_homogeneous.append(percentage)\n",
    "        boundary_homogeneous_all.append(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize statistics\n",
    "central_homogeneous_all = np.array(central_homogeneous_all)\n",
    "boundary_homogeneous_all = np.array(boundary_homogeneous_all)\n",
    "print(\"interior units homogeneous connection rate mean\", np.mean(central_homogeneous_all), \"and variance\", np.var(central_homogeneous_all))\n",
    "print(\"boundary units homogeneous connection rate mean\", np.mean(boundary_homogeneous_all), \"and variance\", np.var(boundary_homogeneous_all))\n",
    "\n",
    "bins = np.linspace(0, 1, 30)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(central_homogeneous_all, bins=bins, alpha=0.6, label=\"Interior Unit\", color=\"tab:blue\", density=True)\n",
    "plt.hist(boundary_homogeneous_all, bins=bins, alpha=0.6, label=\"Boundary Unit\", color=\"tab:orange\", density=True)\n",
    "plt.xlabel(\"Homogeneous connection rate\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Interior vs Boundary homogeneous connections\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO Linear Regression and Visualization\n",
    "# load V4 digital twin benchmark\n",
    "V4rsp = np.load(folder_path + \"V4DT/PRsp.npy\") # (50000, 128, 128)\n",
    "roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128); (x, y switch) and transpose back to match the others\n",
    "V4idx = np.load(folder_path + \"V4DT/idx.npy\") # (128, 128), v4 connected components\n",
    "V4idx, _ = connected_components(V4idx, threshold=10, return_largest=True) # remove the unconnected components smaller than a threshold of 10, also return the largest connected component\n",
    "V4domain_map = np.zeros_like(V4idx)\n",
    "V4domain_map[roi == 1] = V4idx[roi == 1]\n",
    "top9_0index_V4 = np.load(folder_path + \"V4DT/top9_0index.npy\") # (128, 128, 9)\n",
    "voxels_domain = np.zeros((int(np.sum(roi)))) # the domain assignment for all 3048 V4 voxels\n",
    "count = 0\n",
    "fs = 12 # set font size for the axes\n",
    "fs_title = 16 # set font size for the title\n",
    "for i in range(128): # create a 1d vector of all 3048 V4 voxels' domain assignment\n",
    "    for j in range(128):\n",
    "        if roi[i, j] == 1:\n",
    "            voxels_domain[count] = V4idx[i, j]\n",
    "            count += 1\n",
    "assert count == voxels_domain.shape[0]\n",
    "\n",
    "# low_contrast_palette = ['#f5f8fb', '#d6e1ee', '#b1c7e0', '#7fa6c9', '#4c79a7']\n",
    "low_contrast_palette = ['#d6e1ee', '#b1c7e0', '#7fa6c9', '#4c79a7']\n",
    "som_low_contrast_cmap = LinearSegmentedColormap.from_list('som_low_contrast', low_contrast_palette)\n",
    "som_low_contrast_cmap.set_bad('white')\n",
    "v4_low_contrast_cmap = LinearSegmentedColormap.from_list('v4_low_contrast', low_contrast_palette)\n",
    "v4_low_contrast_cmap.set_bad('white')\n",
    "\n",
    "def upscale_grid(array, scale=3):\n",
    "    if isinstance(array, np.ma.MaskedArray):\n",
    "        return np.ma.repeat(np.ma.repeat(array, scale, axis=0), scale, axis=1)\n",
    "    return np.repeat(np.repeat(array, scale, axis=0), scale, axis=1)\n",
    "\n",
    "def get_domain_mask(domain_map, domain_id, min_area=10):\n",
    "    mask = (domain_map == domain_id)\n",
    "    if not np.any(mask):\n",
    "        return np.zeros_like(domain_map, dtype=bool)\n",
    "    mask_with_id = np.where(mask, domain_id, 0)\n",
    "    cleaned = connected_components(mask_with_id, threshold=min_area, return_largest=False)\n",
    "    return cleaned == domain_id\n",
    "\n",
    "def create_boundary_overlay(domain_mask, color, scale=3):\n",
    "    mask = domain_mask.astype(bool)\n",
    "    if not np.any(mask):\n",
    "        return np.zeros((domain_mask.shape[0] * scale, domain_mask.shape[1] * scale, 4))\n",
    "    up_mask = upscale_grid(mask.astype(int), scale) > 0\n",
    "    structure = np.ones((3, 3), dtype=bool)\n",
    "    eroded = binary_erosion(up_mask, structure=structure, border_value=0)\n",
    "    boundary = up_mask & (~eroded)\n",
    "    overlay = np.zeros((boundary.shape[0], boundary.shape[1], 4))\n",
    "    overlay[boundary] = to_rgba(color)\n",
    "    return overlay\n",
    "\n",
    "# load SOM simulation\n",
    "top_img_num = 1000\n",
    "epoch_num = 7500\n",
    "learning_rate = 0.05\n",
    "name = \"RSOM\"\n",
    "regression_folder_name = \"LASSO_top1k\"\n",
    "num_of_voxels_displayed = 15\n",
    "weight = np.load(folder_path + name + \"/weights.npy\")[:, :, :50000] # (60, 60, 50000)\n",
    "top9_0index = np.load(folder_path + name + \"/rsptop_0index.npy\") # (60, 60, 9)\n",
    "idx = np.load(folder_path + name + \"/idx_newassign.npy\").astype(int) # (60, 60) of 16 V4 voxel types, only connected components perserved\n",
    "som_domain_map = np.zeros((idx.shape[1] + 2, idx.shape[0] + 2), dtype=int)\n",
    "som_domain_map[1:-1, 1:-1] = np.swapaxes(idx, 0, 1)\n",
    "\n",
    "# loop through all domains from 1 to 16\n",
    "idx_targets = [2] # domain numbers to be optimized, ###################### change this to your target domains\n",
    "for idx_target in idx_targets:\n",
    "    # SOM simulation response pattern to one largest connected component's preferred images\n",
    "    SOMmask = np.where(idx == idx_target) # x, y 0index of the largest connected component\n",
    "    preferred_imgs_som, imgs_count_som = np.unique(top9_0index[SOMmask[0], SOMmask[1], :].astype(int).flatten(), return_counts=True) # unique top-9 image 0index the largest connected domain prefers\n",
    "    rsp = np.zeros((60, 60))\n",
    "    for i in range(60):\n",
    "        for j in range(60):\n",
    "            rsp[i, j] = np.mean(weight[i, j, preferred_imgs_som] * imgs_count_som) # weighted sum of the preferred images' responses\n",
    "    max_rsp_idx = np.where((rsp == np.max(rsp[idx == idx_target]))) # find the maximum within-domain rsp value's index\n",
    "    # print(\"SOM current largest component's most preferred image:\", len(preferred_imgs), idx[max_rsp_idx])\n",
    "\n",
    "    # V4 digital twin response pattern to one largest connected component's preferred images\n",
    "    V4mask = np.where(V4idx == idx_target) # x, y 0index of the largest connected component\n",
    "    preferred_imgs_v4, imgs_count_v4 = np.unique(top9_0index_V4[V4mask[0], V4mask[1], :].astype(int).flatten(), return_counts=True) # unique top-9 image 0index the largest connected domain prefers\n",
    "    # print(\"V4 digital twin current largest component preferred images:\", len(preferred_imgs))\n",
    "    rsp_V4 = np.zeros((128, 128))\n",
    "    for i in range(128):\n",
    "        for j in range(128):\n",
    "            if roi[i, j] == 1: \n",
    "                rsp_V4[i, j] = np.mean(V4rsp[preferred_imgs_v4, i, j] * imgs_count_v4) # weighted sum of the preferred images' responses\n",
    "            else: rsp_V4[i, j] = np.nan # unassigned voxels stay white in plots\n",
    "\n",
    "    # whether to train the linear regression model further or not\n",
    "    device = \"cpu\"\n",
    "    # prepare the training data\n",
    "    train = np.zeros((np.sum(roi), V4rsp.shape[0]))\n",
    "    entry = 0\n",
    "    for i in range(roi.shape[0]):\n",
    "        for j in range(roi.shape[1]):\n",
    "            if roi[i, j] == 1:\n",
    "                train[entry] = V4rsp[:, i, j]\n",
    "                entry += 1\n",
    "    weight_target = weight[max_rsp_idx[0][0], max_rsp_idx[1][0], :50000]\n",
    "    # if restricted to the top-1k responsive images out of 50k\n",
    "    if top_img_num != 50000:\n",
    "        image_label = np.arange(50000) # 0indexed image names\n",
    "        _, image_label = zip(*sorted(zip(weight_target, image_label))) # sort the mean responses (from small to large) and the image_label according to the order of mean responses\n",
    "        image_label = np.flip(image_label[-top_img_num:]) # take selected most responsive images' 0index (from large to small)\n",
    "        train = train[:, image_label] # select the most responsive images\n",
    "        weight_target = weight_target[image_label] # select the most responsive images' mean responses\n",
    "    weight_target = torch.tensor(weight_target, device=device, dtype=torch.float32) # target data\n",
    "    train = torch.tensor(train.astype(np.float32), device=device) # training data of shape (3048, ...)\n",
    "\n",
    "    # Define the linear regression model\n",
    "    class LinearRegressionModel(nn.Module):\n",
    "        def __init__(self, num_rows):\n",
    "            super(LinearRegressionModel, self).__init__()\n",
    "            self.weights = nn.Parameter(torch.randn(num_rows, device=device)) # train from scratch\n",
    "        def forward(self, rsp): return torch.sum(self.weights.view(-1, 1) * rsp, dim=0)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = LinearRegressionModel(num_rows=3048).to(device)\n",
    "    alpha = 0.01 # L1 regularization strength\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epoch_num, gamma=1) # no learning rate decay\n",
    "    num_epochs = epoch_num # Training loop\n",
    "    pbar = tqdm(range(num_epochs), desc=\"train...\")\n",
    "    for epoch in pbar: # Forward pass: Compute predicted y by passing xs to the model\n",
    "        y_pred = model(train)\n",
    "        l1_regularization = alpha * torch.sum(torch.abs(model.weights)) # L1 LASSO penalty to encourage sparsity\n",
    "        loss = criterion(y_pred, weight_target) + l1_regularization # the total loss\n",
    "        optimizer.zero_grad() # Backward pass: Compute gradients\n",
    "        loss.backward()\n",
    "        optimizer.step() # Update weights\n",
    "        # with torch.no_grad(): model.weights.clamp_(min=0) # the non-negative weights constraint\n",
    "        scheduler.step() # Update learning rate\n",
    "        pbar.set_postfix({\"Loss\": loss.item()})\n",
    "    weights_finetuned = model.weights.detach().cpu().numpy() # The trained weights (coefficients) of the linear regression model (3048,)\n",
    "    final_loss = loss.item() # Final error measurement\n",
    "        \n",
    "\n",
    "    scale = 3\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(9, 3))\n",
    "    # Subplot2: RSOM example grid weight composition\n",
    "    regression_w, voxels_domain_sorted = zip(*sorted(zip(weights_finetuned, voxels_domain), reverse=True))\n",
    "    regression_w = np.array(regression_w)\n",
    "    voxels_domain_sorted = np.array(voxels_domain_sorted).astype(int)\n",
    "    colors = ['black', 'red', 'green', 'blue', 'yellow', 'magenta', 'cyan', 'orange', 'purple', 'brown', 'pink', 'lime', 'teal', 'navy', 'gold', 'silver', 'coral'] # black for unassigned 0\n",
    "    cmap_black = ListedColormap(colors)\n",
    "    ax[1].bar(range(num_of_voxels_displayed), regression_w[:num_of_voxels_displayed], color=cmap_black(voxels_domain_sorted[:num_of_voxels_displayed]), width=1)\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    ax[1].set_xlabel('V4 columns', fontsize=fs)\n",
    "    ax[1].set_ylabel('Weights', fontsize=fs)\n",
    "    ax[1].spines['top'].set_visible(False)\n",
    "    ax[1].spines['right'].set_visible(False)\n",
    "    # Subplot1: RSOM simulation\n",
    "    rsp_pad = np.full((62, 62), np.nan) # white outer rim\n",
    "    rsp_pad[1:61, 1:61] = rsp\n",
    "    rsp_pad = np.swapaxes(rsp_pad, 0, 1) # transposed\n",
    "    rsp_up = upscale_grid(rsp_pad, scale)\n",
    "    ax[0].imshow(rsp_up, cmap=som_low_contrast_cmap, interpolation='nearest')\n",
    "    som_mask_target = get_domain_mask(som_domain_map, idx_target)\n",
    "    overlay_target = create_boundary_overlay(som_mask_target, colors[idx_target], scale=scale)\n",
    "    ax[0].imshow(overlay_target, interpolation='nearest')\n",
    "    colors_to_add_switch = False\n",
    "    if idx_target == 16:\n",
    "        colors_to_add = [8]\n",
    "        colors_to_add_switch = True\n",
    "    if idx_target == 1:\n",
    "        colors_to_add = [11, 12]\n",
    "        colors_to_add_switch = True\n",
    "    if idx_target == 5:\n",
    "        colors_to_add = [7]\n",
    "        colors_to_add_switch = True\n",
    "    if idx_target == 15:\n",
    "        colors_to_add = [16]\n",
    "        colors_to_add_switch = True\n",
    "    if idx_target == 4:\n",
    "        colors_to_add = [10]\n",
    "        colors_to_add_switch = True\n",
    "    if colors_to_add_switch:\n",
    "        for current_color in colors_to_add:\n",
    "            som_mask_extra = get_domain_mask(som_domain_map, current_color)\n",
    "            if np.any(som_mask_extra):\n",
    "                overlay_extra = create_boundary_overlay(som_mask_extra, colors[current_color], scale=scale)\n",
    "                ax[0].imshow(overlay_extra, interpolation='nearest')\n",
    "    highlight_mask = np.zeros((62, 62))\n",
    "    highlight_mask[max_rsp_idx[1][0] + 1, max_rsp_idx[0][0] + 1] = 1 # transposed with padding\n",
    "    highlight_up = upscale_grid(highlight_mask, scale) > 0\n",
    "    highlight_overlay = np.zeros((highlight_up.shape[0], highlight_up.shape[1], 4))\n",
    "    highlight_overlay[highlight_up] = to_rgba('black')\n",
    "    highlight_overlay[..., 3] = highlight_overlay[..., 3] * 0.7\n",
    "    ax[0].imshow(highlight_overlay, interpolation='nearest')\n",
    "    ax[0].axis('off')\n",
    "    # Subplot3: V4 digital twin\n",
    "    rsp_V4 = np.flipud(np.fliplr(rsp_V4))\n",
    "    roi_display = np.flipud(np.fliplr(roi))\n",
    "    masked_rsp_V4 = np.ma.array(rsp_V4, mask=(roi_display == 0))\n",
    "    v4_slice = masked_rsp_V4[38:114, 38:105]\n",
    "    ax[2].imshow(upscale_grid(v4_slice, scale), cmap=v4_low_contrast_cmap, interpolation='nearest') # low-contrast palette\n",
    "    domain_mask_target = get_domain_mask(V4domain_map, idx_target)\n",
    "    domain_map_display = np.flipud(np.fliplr(domain_mask_target))\n",
    "    domain_slice = domain_map_display[38:114, 38:105]\n",
    "    overlay_v4 = create_boundary_overlay(domain_slice, colors[idx_target], scale=scale)\n",
    "    ax[2].imshow(overlay_v4, interpolation='nearest')\n",
    "    colors_to_add_switch = False\n",
    "    if idx_target == 16:\n",
    "        colors_to_add = [8]\n",
    "        colors_to_add_switch = True\n",
    "    if idx_target == 1:\n",
    "        colors_to_add = [11, 12]\n",
    "        colors_to_add_switch = True\n",
    "    if idx_target == 5:\n",
    "        colors_to_add = [7]\n",
    "        colors_to_add_switch = True\n",
    "    if idx_target == 15:\n",
    "        colors_to_add = [16]\n",
    "        colors_to_add_switch = True\n",
    "    if idx_target == 4:\n",
    "        colors_to_add = [10]\n",
    "        colors_to_add_switch = True\n",
    "    if colors_to_add_switch:\n",
    "        for current_color in colors_to_add:\n",
    "            domain_mask_extra = get_domain_mask(V4domain_map, current_color)\n",
    "            if np.any(domain_mask_extra):\n",
    "                domain_mask_extra_display = np.flipud(np.fliplr(domain_mask_extra))[38:114, 38:105]\n",
    "                overlay_extra_v4 = create_boundary_overlay(domain_mask_extra_display, colors[current_color], scale=scale)\n",
    "                ax[2].imshow(overlay_extra_v4, interpolation='nearest')\n",
    "    ax[2].axis('off')\n",
    "    plt.tight_layout()\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(folder_path + name +  \"/\" + regression_folder_name + \"/domain\" + str(int(idx_target)) + \"_\" + str(int(epoch_num)) + \"_T.png\", dpi=1000)\n",
    "    plt.close()\n",
    "\n",
    "    # SOM domain preferred images visualization\n",
    "    # sort imgs_count and preferred_imgs according to the descending order of imgs_count\n",
    "    imgs_count_som, preferred_imgs_som = zip(*sorted(zip(imgs_count_som, preferred_imgs_som), reverse=True))\n",
    "    imgs_num = 20\n",
    "    col_num = 2\n",
    "    img_size = 100\n",
    "    line_width = 30\n",
    "    imgs_visualized = np.zeros((int(imgs_num / col_num) * (img_size + line_width) - line_width, col_num * (img_size + line_width) - line_width, 3))\n",
    "    for i in range(col_num):\n",
    "        for j in range(int(imgs_num / col_num)):\n",
    "            path = folder_path + \"50K_Imgset/\" + str(int(1 + preferred_imgs_som[int(i * (imgs_num / col_num) + j)])) + \".bmp\" # the image name is 1-indexed\n",
    "            img = np.array(Image.open(path))[20:80, 20:80, :] # obtain the non-blurred central part of the image\n",
    "            img = resize(img, (img_size, img_size, 3), anti_aliasing=True) # resize the image\n",
    "            imgs_visualized[j*(img_size+line_width) : j*(img_size+line_width)+img_size, \n",
    "                            i*(img_size+line_width) : i*(img_size+line_width)+img_size, \n",
    "                            :] = img\n",
    "    for i in range(col_num): imgs_visualized[:, ((i+1)*img_size+i*line_width) : ((i+1)*(img_size+line_width)), :] = 1 # vertical intervals to be white\n",
    "    for i in range(int(imgs_num / col_num) - 1): imgs_visualized[((i+1)*img_size+i*line_width) : ((i+1)*(img_size+line_width)), :, :] = 1 # horizontal intervals to be white\n",
    "    plt.imshow(imgs_visualized)\n",
    "    plt.axis('off')\n",
    "    plt.close()\n",
    "    save_path = folder_path + name +  \"/\" + regression_folder_name + \"/domain\" + str(int(idx_target)) + \"_\" + \"SOMpreferred\" + \".bmp\"\n",
    "    matplotlib.image.imsave(save_path, imgs_visualized)\n",
    "\n",
    "    # V4 domain preferred images visualization\n",
    "    # sort imgs_count and preferred_imgs according to the descending order of imgs_count\n",
    "    imgs_count_v4, preferred_imgs_v4 = zip(*sorted(zip(imgs_count_v4, preferred_imgs_v4), reverse=True))\n",
    "    imgs_num = 20\n",
    "    col_num = 2\n",
    "    img_size = 100\n",
    "    line_width = 30\n",
    "    imgs_visualized = np.zeros((int(imgs_num / col_num) * (img_size + line_width) - line_width, col_num * (img_size + line_width) - line_width, 3))\n",
    "    for i in range(col_num):\n",
    "        for j in range(int(imgs_num / col_num)):\n",
    "            path = folder_path + \"50K_Imgset/\" + str(int(1 + preferred_imgs_v4[int(i * (imgs_num / col_num) + j)])) + \".bmp\" # the image name is 1-indexed\n",
    "            img = np.array(Image.open(path))[20:80, 20:80, :] # obtain the non-blurred central part of the image\n",
    "            img = resize(img, (img_size, img_size, 3), anti_aliasing=True) # resize the image\n",
    "            imgs_visualized[j*(img_size+line_width) : j*(img_size+line_width)+img_size, \n",
    "                            i*(img_size+line_width) : i*(img_size+line_width)+img_size, \n",
    "                            :] = img\n",
    "    for i in range(col_num): imgs_visualized[:, ((i+1)*img_size+i*line_width) : ((i+1)*(img_size+line_width)), :] = 1 # vertical intervals to be white\n",
    "    for i in range(int(imgs_num / col_num) - 1): imgs_visualized[((i+1)*img_size+i*line_width) : ((i+1)*(img_size+line_width)), :, :] = 1 # horizontal intervals to be white\n",
    "    plt.imshow(imgs_visualized)\n",
    "    plt.axis('off')\n",
    "    plt.close()\n",
    "    save_path = folder_path + name +  \"/\" + regression_folder_name + \"/domain\" + str(int(idx_target)) + \"_\" + \"V4preferred\" + \".bmp\"\n",
    "    matplotlib.image.imsave(save_path, imgs_visualized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain boundary columns / grids tuning curve correlation against other columns / grid across the boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"RSOM\" # V4, RSOM, SOM\n",
    "if name == \"RSOM\":\n",
    "    # idx = np.load(folder_path + name + \"/idx_newassign.npy\").astype(int) # (60, 60) of 16 V4 voxel types\n",
    "    idx = np.load(folder_path + name + \"/assigned.npz\")[\"matched_idx_rsom\"].astype(int) # (60, 60) of 16 V4 voxel types\n",
    "    size = idx.shape[0]\n",
    "    assert size == idx.shape[1]\n",
    "    roi = np.ones((size, size)).astype(int)\n",
    "    rsp = np.load(folder_path + name + \"/weights.npy\")[:, :, :50000] # (60, 60, 50000)\n",
    "elif name == \"SOM\":\n",
    "    idx = np.load(folder_path + name + \"/assigned.npz\")[\"matched_idx_som\"].astype(int) # (60, 60) of 16 V4 voxel types\n",
    "    size = idx.shape[0]\n",
    "    assert size == idx.shape[1]\n",
    "    roi = np.ones((size, size)).astype(int)\n",
    "    rsp = np.load(folder_path + name + \"/weights.npy\")[:, :, :50000] # (60, 60, 50000)\n",
    "elif name == \"V4\":\n",
    "    idx = np.load(folder_path + \"V4DT/idx.npy\").astype(int) # (128, 128) of 16 V4 voxel types\n",
    "    size = idx.shape[0]\n",
    "    assert size == idx.shape[1]\n",
    "    roi = np.load(folder_path + \"V4DT/ROI.npy\").T\n",
    "    rsp = np.load(folder_path + \"V4DT/PRsp.npy\") # (50000, 128, 128)\n",
    "idx, _ = connected_components(idx, threshold=10, return_largest=True) # remove the unconnected components smaller than a threshold of 10, also return the largest connected component\n",
    "\n",
    "for targeted_domain in range(1, 17): # loop through all 16 domains\n",
    "    domain_mask = np.where(idx == targeted_domain) # targeted domain mask\n",
    "    \n",
    "    map = np.zeros((size, size))\n",
    "    map[domain_mask[0], domain_mask[1]] = 1\n",
    "    I = [] # target domain boundary grids' x coordinate\n",
    "    J = [] # target domain boundary grids' y coordinate\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if map[i, j] == 1:\n",
    "                try:\n",
    "                    if map[i-1, j] == 0 or map[i+1, j] == 0 or map[i, j-1] == 0 or map[i, j+1] == 0 or map[i-1, j-1] == 0 or map[i-1, j+1] == 0 or map[i+1, j-1] == 0 or map[i+1, j+1] == 0:\n",
    "                        I.append(i)\n",
    "                        J.append(j)\n",
    "                except: pass # out of boundary case is not considered, no meaning at all\n",
    "\n",
    "    index_map = np.zeros((size, size)) # map the 2D grid to 1D index\n",
    "    index = 0\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if roi[i, j] == 1:\n",
    "                index_map[i, j] = index\n",
    "                index += 1\n",
    "    if name == \"RSOM\": assert index == 3600\n",
    "    elif name == \"SOM\": assert index == 3600\n",
    "    elif name == \"V4\": assert index == 3048\n",
    "\n",
    "    assert len(I) == len(J)\n",
    "    half_size = 5\n",
    "    cor_in = []\n",
    "    dis_in = []\n",
    "    cor_out = []\n",
    "    dis_out = []\n",
    "    failure = 0\n",
    "    for g in range(len(I)): # loop through all the boundary grids\n",
    "        target_idx = int(index_map[I[g], J[g]]) # 1D index of the current boundary grid\n",
    "        try:\n",
    "            for x in range(I[g] - half_size, I[g] + half_size + 1): # another pairwise grid\n",
    "                for y in range(J[g] - half_size, J[g] + half_size + 1):\n",
    "                    if roi[x, y] == 0: continue\n",
    "                    d = np.sqrt((I[g] - x) ** 2 + (J[g] - y) ** 2)\n",
    "                    if name == \"V4\": c = pearsonr(rsp[:, I[g], J[g]], rsp[:, x, y])[0] # correlation\n",
    "                    elif name == \"RSOM\": c = pearsonr(rsp[I[g], J[g], :], rsp[x, y, :])[0] # correlation\n",
    "                    elif name == \"SOM\": c = pearsonr(rsp[I[g], J[g], :], rsp[x, y, :])[0] # correlation\n",
    "                    if map[x, y] == 1: # both pairwise grids in the same domain\n",
    "                        cor_in.append(c)\n",
    "                        dis_in.append(d)\n",
    "                    else: # the other pairwise grid is out of the domain\n",
    "                        cor_out.append(c)\n",
    "                        dis_out.append(d) # distance\n",
    "        except: failure += 1 # out of map boundary case is not considered, no meaning at all\n",
    "\n",
    "    cor_in_mean = []\n",
    "    cor_in_std = []\n",
    "    for d in np.unique(dis_in):\n",
    "        i = np.where(np.array(dis_in) == d)[0]\n",
    "        cor_in_mean.append(np.mean(np.array(cor_in)[i]))\n",
    "        cor_in_std.append(np.std(np.array(cor_in)[i]) / np.sqrt(len(np.array(cor_in)[i]))) # standard error of the mean\n",
    "\n",
    "    cor_out_mean = [1.0] # the correlation of the boundary grid with itself is 1\n",
    "    cor_out_std = [0.0]\n",
    "    for d in np.unique(dis_out):\n",
    "        i = np.where(np.array(dis_out) == d)[0]\n",
    "        cor_out_mean.append(np.mean(np.array(cor_out)[i]))\n",
    "        cor_out_std.append(np.std(np.array(cor_out)[i]) / np.sqrt(len(np.array(cor_out)[i]))) # standard error of the mean\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    if name == \"RSOM\": \n",
    "        label_within = \"Unit pairs \\nwithin domain\"\n",
    "        label_across = \"across domain\"\n",
    "    elif name == \"SOM\":\n",
    "        label_within = \"Unit pairs \\nwithin domain\"\n",
    "        label_across = \"across domain\"\n",
    "    elif name == \"V4\":\n",
    "        label_within = \"Columns pairs \\nwithin domain\"\n",
    "        label_across = \"across domain\"\n",
    "    plt.errorbar(\n",
    "        x=np.unique(dis_in),                 # X-axis positions\n",
    "        y=cor_in_mean,                       # Mean values\n",
    "        yerr=cor_in_std,                     # Error bars\n",
    "        fmt='-o',                            # 'o' for dot markers\n",
    "        ecolor='gray',                       # Error bar color\n",
    "        capsize=3,                           # Add caps to error bars\n",
    "        label=label_within\n",
    "    )\n",
    "\n",
    "    plt.errorbar(\n",
    "        x=np.insert(np.unique(dis_out), 0, 0.0), # X-axis positions\n",
    "        y=cor_out_mean,                           # Mean values\n",
    "        yerr=cor_out_std,                         # Error bars\n",
    "        fmt='-*',                                # 'o' for dot markers\n",
    "        ecolor='gray',                           # Error bar color\n",
    "        capsize=3,                               # Add caps to error bars\n",
    "        label=label_across\n",
    "    )\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Distance\", fontsize=22)\n",
    "    plt.ylabel(\"Tuning Correlation\", fontsize=22)\n",
    "    plt.legend(loc='lower left', fontsize=22)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    if name == \"RSOM\": plt.savefig(folder_path + \"RSOM/LASSO_top1k/RSOM_boundary_\" + str(targeted_domain) + \".png\", dpi=1000)\n",
    "    elif name == \"SOM\": plt.savefig(folder_path + \"RSOM/LASSO_top1k/SOM_boundary_\" + str(targeted_domain) + \".png\", dpi=1000)\n",
    "    elif name == \"V4\": plt.savefig(folder_path + \"RSOM/LASSO_top1k/V4_boundary_\" + str(targeted_domain) + \".png\", dpi=1000)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\"]:    \n",
    "    # To syhthesize figure from several subplots\n",
    "    # Step 1: Load all subplots\n",
    "    # domain = \"5\" # {1, 2, 5 for example domains}\n",
    "    png_image = Image.open(folder_path + 'RSOM/LASSO_top1k/domain' + domain + '_7500_T.png')\n",
    "    bmp_image_left = Image.open(folder_path + 'RSOM/LASSO_top1k/domain' + domain + '_SOMpreferred.bmp')\n",
    "    bmp_image_right = Image.open(folder_path + 'RSOM/LASSO_top1k/domain' + domain + '_V4preferred.bmp')\n",
    "    boundary = Image.open(folder_path + 'RSOM/LASSO_top1k/boundary_' + domain + '.png')\n",
    "    boundary_V4 = Image.open(folder_path + 'RSOM/LASSO_top1k/V4_boundary_' + domain + '.png')\n",
    "    # Step 2: Resize the images (optional) to ensure they have the same height: finding the maximum height to resize all images to the same height\n",
    "    max_height = max(png_image.height, bmp_image_left.height, bmp_image_right.height)\n",
    "    def resize_image(image, max_height):\n",
    "        width, height = image.size\n",
    "        if height != max_height: # Calculate the new width while keeping the aspect ratio\n",
    "            new_width = int((max_height / height) * width)\n",
    "            return image.resize((new_width, max_height))\n",
    "        return image\n",
    "    png_image = resize_image(png_image, max_height)\n",
    "    bmp_image_left = resize_image(bmp_image_left, max_height)\n",
    "    bmp_image_right = resize_image(bmp_image_right, max_height)\n",
    "    boundary = resize_image(boundary, max_height)\n",
    "    boundary_V4 = resize_image(boundary_V4, max_height)\n",
    "    # Step 3: Create a new image with the width equal to the sum of the three images' widths\n",
    "    total_width = bmp_image_left.width + boundary.width + png_image.width + boundary_V4.width + bmp_image_right.width\n",
    "    concatenated_image = Image.new('RGB', (total_width, max_height))\n",
    "    # Step 4: Paste each image next to each other in the new wide image\n",
    "    x_offset = 0\n",
    "    for img in [bmp_image_left, boundary, png_image, boundary_V4, bmp_image_right]:\n",
    "        concatenated_image.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "    # Step 5: Save the concatenated image\n",
    "    concatenated_image.save(folder_path + 'RSOM/LASSO_top1k/composition_domain' + domain + '.png')\n",
    "\n",
    "# resize domain example grid tuning composition map to lower resolution\n",
    "domain = \"2\" # {1, 2, 5 for example domains}, change as needed\n",
    "image = Image.open(folder_path + \"RSOM/LASSO_top1k/composition_domain\" + domain + \".png\")\n",
    "width, height = image.size\n",
    "resized_image = image.resize((int(np.round(width/9)), int(np.round(height/9))), Image.Resampling.LANCZOS)\n",
    "resized_image.save(folder_path + \"Fig3/composition_domain\" + domain + \".png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topoV4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
