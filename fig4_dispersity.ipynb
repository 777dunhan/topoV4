{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import scipy.io\n",
    "import h5py\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "from scipy.ndimage import label, sum as ndi_sum\n",
    "import matplotlib.image\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from PIL import Image\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from skimage.measure import find_contours\n",
    "import math\n",
    "\n",
    "folder_path = \"/path/to/your/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retinotopy map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"layer40\"\n",
    "positions = np.load(folder_path + \"Fig4/TDANNfinal_positions/layer\" + str(layer[-2:-1]) + \".\" + str(layer[-1]) + \".npz\")[\"coordinates\"] # (num_units, 2)\n",
    "polar_angle = np.zeros((positions.shape[0])) # theta\n",
    "eccentricity = np.zeros((positions.shape[0])) # r\n",
    "for i in range(positions.shape[0]):\n",
    "    if layer == \"layer40\" or layer == \"layer41\": # 25088 = 512 * 7 * 7\n",
    "        size = 7\n",
    "        center = 3\n",
    "        s = 3\n",
    "    elif layer == \"layer30\" or layer == \"layer31\": # 50176 = 256 * 14 * 14\n",
    "        size = 14\n",
    "        center = 7\n",
    "        s = 1.5\n",
    "    depth_index = i // (size * size)        # Get index along the first dimension (512)\n",
    "    row_col_index = i % (size * size)       # Remaining index within the 7x7 matrix\n",
    "    row_index = row_col_index // size    # Get index along the second dimension (7)\n",
    "    col_index = row_col_index % size     # Get index along the third dimension (7)\n",
    "    polar_angle[i] = math.degrees(np.arctan2((center - (size-col_index)), (center - (row_index))))\n",
    "    eccentricity[i] = math.degrees(math.atan(np.sqrt((row_index-center) ** 2 + (col_index-center) ** 2) / 45)) # assuming 45 cm away from the fovea\n",
    "polar_angle = (polar_angle - np.min(polar_angle)) / (np.max(polar_angle) - np.min(polar_angle))\n",
    "eccentricity = (eccentricity - np.min(eccentricity)) / (np.max(eccentricity) - np.min(eccentricity))\n",
    "\n",
    "# visualization\n",
    "\"\"\"\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "fs = 16\n",
    "axes[0].scatter(positions[:, 1], positions[:, 0], c=polar_angle, s=s, marker='o', edgecolors='none')\n",
    "axes[0].invert_yaxis()  # Move the origin to the top-left corner\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "axes[0].set_title(\"Polar Angle\", fontsize=fs)\n",
    "for spine in axes[0].spines.values(): spine.set_visible(False)  # Remove outer black box\n",
    "axes[1].scatter(positions[:, 1], positions[:, 0], c=eccentricity, s=s, marker='o', edgecolors='none')\n",
    "axes[1].invert_yaxis()  # Move the origin to the top-left corner\n",
    "axes[1].set_xticks([])\n",
    "axes[1].set_yticks([])\n",
    "axes[1].set_title(\"Eccentricity\", fontsize=fs)\n",
    "for spine in axes[1].spines.values(): spine.set_visible(False)  # Remove outer black box\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "fig.savefig(folder_path + \"Fig4/\" + layer + \"/theta_r.png\", dpi=1000)\n",
    "plt.close()\n",
    "del fig\n",
    "\"\"\"\n",
    "fig = plt.subplots(figsize=(4, 4))\n",
    "plt.scatter(positions[:, 1], positions[:, 0], c=polar_angle, s=s, marker='o', edgecolors='none', cmap='hsv')\n",
    "plt.gca().invert_yaxis()  # Move the origin to the top-left corner\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "# plt.title(\"Polar Angle\", fontsize=22)\n",
    "plt.tight_layout()\n",
    "plt.savefig(folder_path + \"Fig4/\" + layer + \"/theta.png\", dpi=300)\n",
    "plt.close()\n",
    "del fig\n",
    "\n",
    "fig = plt.subplots(figsize=(4, 4))\n",
    "plt.scatter(positions[:, 1], positions[:, 0], c=eccentricity, s=s, marker='o', edgecolors='none', cmap='hsv')\n",
    "plt.gca().invert_yaxis()  # Move the origin to the top-left corner\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "# plt.title(\"Eccentricity\", fontsize=22)\n",
    "plt.tight_layout()\n",
    "plt.savefig(folder_path + \"Fig4/\" + layer + \"/r.png\", dpi=300)\n",
    "plt.close()\n",
    "del fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate TDANN unit heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path: str, device: str):\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "    # drop the FC layer\n",
    "    model.fc = nn.Identity()\n",
    "    # load weights\n",
    "    ckpt = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    state_dict = ckpt[\"classy_state_dict\"][\"base_model\"][\"model\"][\"trunk\"]\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"base_model\") and \"fc.\" not in k:\n",
    "            remainder = k.split(\"base_model.\")[-1]\n",
    "            new_state_dict[remainder] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    # freeze all weights\n",
    "    for param in model.parameters(): param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "def heatmap_estimation():\n",
    "    layer = \"layer41\" # targeted layer\n",
    "    imgs_0index = np.load(folder_path + \"Fig4/TDANNrsp_1kimg_0idx.npz\")[layer] # (num_units, 1000), 0-indexed top 100 images for each unit\n",
    "    # load TDANN model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    checkpoint_path = folder_path + \"Fig4/TDANNfinal.torch\"\n",
    "    model = load_model_from_checkpoint(checkpoint_path, device)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    # number of feature channels, essentially num_units to estimate a heatmap\n",
    "    if layer == \"layer10\" or layer == \"layer11\":\n",
    "        fc_num = 64\n",
    "    elif layer == \"layer20\" or layer == \"layer21\":\n",
    "        fc_num = 128\n",
    "    elif layer == \"layer30\" or layer == \"layer31\":\n",
    "        fc_num = 256\n",
    "    elif layer == \"layer40\" or layer == \"layer41\":\n",
    "        fc_num = 512\n",
    "    heatmaps1k_all = np.zeros((fc_num, 26, 224, 224)) # to store one averaged heatmaps1k + top25 heatmaps for all estimated units\n",
    "    for fc in range(fc_num): # Find the target unit's index in the targeted layer response tensor: 3rd layer [256, 14, 14]; 4th layer [512, 7, 7]\n",
    "        # central unit localization\n",
    "        if layer == \"layer10\" or layer == \"layer11\":\n",
    "            feature_map_size = 56\n",
    "            xy = 28\n",
    "        elif layer == \"layer20\" or layer == \"layer21\":\n",
    "            feature_map_size = 28\n",
    "            xy = 14\n",
    "        elif layer == \"layer30\" or layer == \"layer31\":\n",
    "            feature_map_size = 14\n",
    "            xy = 7\n",
    "        elif layer == \"layer40\" or layer == \"layer41\":\n",
    "            feature_map_size = 7\n",
    "            xy = 3\n",
    "        flat_index = fc*feature_map_size*feature_map_size + xy*feature_map_size + xy # 0-indexed unit index\n",
    "        depth_index = flat_index // (feature_map_size * feature_map_size)        # Get index along the first dimension\n",
    "        row_col_index = flat_index % (feature_map_size * feature_map_size)       # Remaining index within the 7x7 matrix\n",
    "        row_index = row_col_index // feature_map_size             # Get index along the second dimension (7)\n",
    "        col_index = row_col_index % feature_map_size             # Get index along the third dimension (7)\n",
    "        print(\"Currently targeting at unit: \", flat_index, (0, depth_index, row_index, col_index)) # unit index indicates its original position in the 3rd layer response tensor\n",
    "        # iterate through this unit most preferred 1k images\n",
    "        gradients1k = np.zeros((224, 224)) # store the gradient of the current unit's activation w.r.t. 1000 input image\n",
    "        for k in tqdm(range(1000), desc=\"top 1k imgs...\", disable=True):\n",
    "            # Define hook to capture activation\n",
    "            activation = [None]  # Use a mutable container to avoid using global variable\n",
    "            def hook_fn(module, input, output):\n",
    "                activation[0] = output  # Store output in the list\n",
    "            # targeting at different layer\n",
    "            if layer == \"layer10\":\n",
    "                target_layer = model.layer1[0]\n",
    "            elif layer == \"layer11\":\n",
    "                target_layer = model.layer1[1]\n",
    "            elif layer == \"layer20\":\n",
    "                target_layer = model.layer2[0]\n",
    "            elif layer == \"layer21\":\n",
    "                target_layer = model.layer2[1]\n",
    "            elif layer == \"layer30\":\n",
    "                target_layer = model.layer3[0]\n",
    "            elif layer == \"layer31\":\n",
    "                target_layer = model.layer3[1]\n",
    "            elif layer == \"layer40\":\n",
    "                target_layer = model.layer4[0]\n",
    "            elif layer == \"layer41\":\n",
    "                target_layer = model.layer4[1]\n",
    "            hook = target_layer.register_forward_hook(hook_fn)\n",
    "            preprocess = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "            # generate 20 image samples with noise from the same image\n",
    "            num_samples = 20\n",
    "            sigma = 0.20  # Noise standard deviation\n",
    "            index1 = (1 + imgs_0index[fc, :]).astype(int) # 0-indexed to 1-indexed to match image file name\n",
    "            batch_vector = []\n",
    "            image_path = folder_path + \"50K_Imgset/\" + str(index1[k]) + \".bmp\"\n",
    "            input_image = preprocess(Image.open(image_path)).unsqueeze(0)  # Add batch dimension\n",
    "            for _ in range(num_samples): batch_vector.append(input_image + sigma * torch.randn_like(input_image)) # Add Gaussian noise to input image\n",
    "            batch_vector = torch.cat(batch_vector, dim=0)  # Shape will be [20, 3, 224, 224]\n",
    "            batch_vector = batch_vector.to(device)\n",
    "            batch_vector.requires_grad_()\n",
    "            # Forward pass\n",
    "            model.fc = nn.Identity()\n",
    "            model(batch_vector)\n",
    "            # Ensure that activation is correctly captured\n",
    "            if activation[0] is None: raise RuntimeError(\"Activation was not captured. Check the hook function.\")\n",
    "            # Target the specific unit's activation\n",
    "            loss = activation[0][:, depth_index, row_index, col_index]\n",
    "            loss = loss.mean() # if for multiple samples, sum over all of them\n",
    "            loss.backward()\n",
    "            # Square and store gradient, torch.Size([1000, 3, 224, 224])\n",
    "            gradients = batch_vector.grad ** 2\n",
    "            # Average over samples, from torch.Size([batch_size, 3, 224, 224]) to torch.Size([3, 224, 224]) and to [224, 224] of np array\n",
    "            gradients = gradients.mean(dim=0).mean(dim=0).detach().cpu().numpy()\n",
    "            if k < 25: heatmaps1k_all[fc, int(k+1), :, :] = gradients # store top 25 images' heatmap as well\n",
    "            # normalize the gradient to have total sum of the current averaged loss value, np array of shape [224, 224]\n",
    "            g_sum = gradients.sum()\n",
    "            if g_sum != 0.0: gradients1k += gradients / g_sum * loss.item()\n",
    "            hook.remove()\n",
    "        # store the current feature channel / unit's estimated heatmap averaged from 1k imgs\n",
    "        heatmaps1k_all[fc, 0, :, :] = gradients1k\n",
    "    # save the final results\n",
    "    ###### change the next line for different layer\n",
    "    np.save(folder_path + \"heatmaps41.npy\", heatmaps1k_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert npy heatmaps to mat\n",
    "heatmaps = np.load(folder_path + \"Fig4/heatmaps.npy\")[:, 0, :, :]\n",
    "print(heatmaps.shape)\n",
    "scipy.io.savemat(folder_path + \"Fig4/layer30/heatmaps.mat\", {'heatmaps': heatmaps})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT, run Fit_2D_Gaussian.m (contributed by Tianye Wang) to fit 2D Gaussian to TDANN unit's heatmaps, then continue with Fig4_dispersity.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the estimated 2D Gaussian fit center and std for layer3.0, the V2 layer\n",
    "xs = scipy.io.loadmat(folder_path + \"Fig4/layer30/b1.mat\")[\"b1\"] # x-center, (256, 1)\n",
    "ys = scipy.io.loadmat(folder_path + \"Fig4/layer30/b2.mat\")[\"b2\"] # y-center, (256, 1)\n",
    "xs_std = scipy.io.loadmat(folder_path + \"Fig4/layer30/c1.mat\")[\"c1\"] # x-std, (256, 1)\n",
    "ys_std = scipy.io.loadmat(folder_path + \"Fig4/layer30/c2.mat\")[\"c2\"] # y-std, (256, 1)\n",
    "heatmaps = np.load(folder_path + \"Fig4/layer30/heatmaps.npy\")[:, 0, :, :] # Estimated averaged Heatmap (256, 224, 224)\n",
    "\n",
    "# visualize the heatmap, with the estimated 2D Gaussian fit center shown in a black dot\n",
    "fig, axes = plt.subplots(2, 16, figsize=(32, 4))\n",
    "l = 3\n",
    "for i in range(32):\n",
    "    ax = axes[i // 16, i % 16]\n",
    "    heatmap = heatmaps[i, :, :]\n",
    "    x, y = int(xs[i][0]), int(ys[i][0])\n",
    "    heatmap[x-l:x+l, y-l:y+l] = np.min(heatmap) # set the center to the minimum value\n",
    "    ax.imshow(heatmap, cmap=\"hot\")\n",
    "    ax.axis(\"off\")\n",
    "    ellipse = matplotlib.patches.Ellipse((y, x), width=2*ys_std[i][0], height=2*xs_std[i][0], edgecolor=\"cyan\", facecolor=\"none\", linewidth=2)\n",
    "    ax.add_patch(ellipse)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the estimated 2D Gaussian fit center and std for layer3.1, the V4 layer\n",
    "xs = scipy.io.loadmat(folder_path + \"Fig4/layer31/b1.mat\")[\"b1\"] # x-center, (256, 1)\n",
    "ys = scipy.io.loadmat(folder_path + \"Fig4/layer31/b2.mat\")[\"b2\"] # y-center, (256, 1)\n",
    "xs_std = scipy.io.loadmat(folder_path + \"Fig4/layer31/c1.mat\")[\"c1\"] # x-std, (256, 1)\n",
    "ys_std = scipy.io.loadmat(folder_path + \"Fig4/layer31/c2.mat\")[\"c2\"] # y-std, (256, 1)\n",
    "heatmaps = np.load(folder_path + \"Fig4/layer31/heatmaps.npy\")[:, 0, :, :] # Estimated averaged Heatmap (256, 224, 224)\n",
    "\n",
    "# visualize the heatmap, with the estimated 2D Gaussian fit center shown in a black dot\n",
    "fig, axes = plt.subplots(2, 16, figsize=(32, 4))\n",
    "l = 3\n",
    "for i in range(32):\n",
    "    ax = axes[i // 16, i % 16]\n",
    "    heatmap = heatmaps[i, :, :]\n",
    "    x, y = int(xs[i][0]), int(ys[i][0])\n",
    "    heatmap[x-l:x+l, y-l:y+l] = np.min(heatmap) # set the center to the minimum value\n",
    "    ax.imshow(heatmap, cmap=\"hot\")\n",
    "    ax.axis(\"off\")\n",
    "    ellipse = matplotlib.patches.Ellipse((y, x), width=2*ys_std[i][0], height=2*xs_std[i][0], edgecolor=\"cyan\", facecolor=\"none\", linewidth=2)\n",
    "    ax.add_patch(ellipse)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the top 25 images' heatmaps for an example unit in layer 3.1, the V4 layer\n",
    "heatmaps = np.load(folder_path + \"Fig4/layer31/heatmaps.npy\")[17, 1:, :, :] # (25, 224, 224)\n",
    "areas = []\n",
    "fig, axes = plt.subplots(4, 12, figsize=(24, 8))\n",
    "for i in range(24):\n",
    "    mask = np.where(heatmaps[i, :, :] >= np.mean(heatmaps[i, :, :]))\n",
    "    areas.append(len(mask[0]) / (224*224))\n",
    "    ax = axes[i // 12 + 2, i % 12]\n",
    "    ax.imshow(heatmaps[i, :, :], cmap=\"hot\")\n",
    "    ax.axis(\"off\")\n",
    "    ax = axes[i // 12, i % 12]\n",
    "    # adjust unmasked pixel value to +1, visualize the > average area\n",
    "    masked_array = heatmaps[i, :, :]\n",
    "    masked_array[mask[0], mask[1]] += 1\n",
    "    ax.imshow(masked_array, cmap=\"hot\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the distribution of the estimated heatmaps' area\n",
    "layer = \"layer31\"\n",
    "heatmaps = np.load(folder_path + \"Fig4/\" + layer + \"/heatmaps.npy\")[:, 1:, :, :] # Estimated averaged Heatmap (256, 25, 224, 224)\n",
    "areas = []\n",
    "for i in range(heatmaps.shape[0]):\n",
    "    for j in range(heatmaps.shape[1]):\n",
    "        mask = np.where(heatmaps[i, j, :, :] >= np.mean(heatmaps[i, j, :, :]))\n",
    "        areas.append(len(mask[0]) / (224*224))\n",
    "        assert len(mask[0]) == len(mask[1])\n",
    "areas = np.array(areas)\n",
    "areas = areas[areas <= 0.5]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 2))\n",
    "axes[0].hist(areas, bins=50)\n",
    "axes[0].set_xlabel(\"heat salient pixels percentage\")\n",
    "axes[0].set_ylabel(\"counts\")\n",
    "heatmaps = np.load(folder_path + \"Fig4/\" + layer + \"/heatmaps.npy\")[:, 0, :, :] # Estimated averaged Heatmap (256, 224, 224)\n",
    "areas = []\n",
    "for i in range(heatmaps.shape[0]):\n",
    "    mask = np.where(heatmaps[i, :, :] >= np.mean(heatmaps[i, :, :]))\n",
    "    areas.append(len(mask[0]) / (224*224))\n",
    "    assert len(mask[0]) == len(mask[1])\n",
    "areas = np.array(areas)\n",
    "areas = areas[areas <= 0.5]\n",
    "axes[1].hist(areas, bins=50)\n",
    "axes[1].set_xlabel(\"averaged heat salient pixels percentage\")\n",
    "axes[1].set_ylabel(\"counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the image aperture radius\n",
    "img = np.array(Image.open(folder_path + \"50K_Imgset/1.bmp\"))\n",
    "print(img.shape, img[0, 0, :], img[0, 99, :], img[99, 0, :], img[99, 99, :])\n",
    "dist = []\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        if img[i, j, 0] == 48 and img[i, j, 1] == 48 and img[i, j, 2] == 48:\n",
    "            dist.append(np.sqrt((i-50)**2 + (j-50)**2))\n",
    "print(len(dist), np.mean(dist), np.min(dist)) # aperture radius: 46.0 / 100.0 of the image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the estimated heatmap for layer4.0, the IT layer\n",
    "heatmaps = np.load(folder_path + \"Fig4/layer40/heatmaps.npy\")[:, 0, :, :] # Estimated Heatmap (512, 224, 224)\n",
    "for k in tqdm(range(heatmaps.shape[0]), desc=\"heatmap preprocessing...\", disable=False):\n",
    "    for i in range(224):\n",
    "        for j in range(224):\n",
    "            if heatmaps[k, i, j] != 0:\n",
    "                if np.sqrt((i-112)**2 + (j-112)**2) > (0.46 * 224): # out of the image aperture\n",
    "                    heatmaps[k, i, j] = 0\n",
    "scipy.io.savemat(folder_path + \"Fig4/layer40/heatmaps.mat\", {'heatmaps': heatmaps}) # save the heatmaps to mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the estimated 2D Gaussian fit center and std for layer4.0, the IT layer\n",
    "xs = scipy.io.loadmat(folder_path + \"Fig4/layer40/b1.mat\")[\"b1\"] # x-center, (512, 1)\n",
    "ys = scipy.io.loadmat(folder_path + \"Fig4/layer40/b2.mat\")[\"b2\"] # y-center, (512, 1)\n",
    "xs_std = scipy.io.loadmat(folder_path + \"Fig4/layer40/c1.mat\")[\"c1\"] # x-std, (512, 1)\n",
    "ys_std = scipy.io.loadmat(folder_path + \"Fig4/layer40/c2.mat\")[\"c2\"] # y-std, (512, 1)\n",
    "# visualize the heatmap, with the estimated 2D Gaussian fit center shown in a black dot\n",
    "fig, axes = plt.subplots(2, 16, figsize=(32, 4))\n",
    "l = 3\n",
    "for i in range(32):\n",
    "    ax = axes[i // 16, i % 16]\n",
    "    heatmap = heatmaps[i, :, :]\n",
    "    x, y = int(xs[i][0]), int(ys[i][0])\n",
    "    heatmap[x-l:x+l, y-l:y+l] = np.min(heatmap) # set the center to the minimum value\n",
    "    ax.imshow(heatmap, cmap=\"hot\")\n",
    "    ax.axis(\"off\")\n",
    "    ellipse = matplotlib.patches.Ellipse((y, x), width=2*ys_std[i][0], height=2*xs_std[i][0], edgecolor=\"cyan\", facecolor=\"none\", linewidth=2)\n",
    "    ax.add_patch(ellipse)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the estimated heatmap for layer4.1, the IT layer\n",
    "heatmaps = np.load(folder_path + \"Fig4/layer41/heatmaps.npy\")[:, 0, :, :] # Estimated Heatmap (512, 224, 224)\n",
    "for k in tqdm(range(heatmaps.shape[0]), desc=\"heatmap preprocessing...\", disable=False):\n",
    "    for i in range(224):\n",
    "        for j in range(224):\n",
    "            if heatmaps[k, i, j] != 0:\n",
    "                if np.sqrt((i-112)**2 + (j-112)**2) > (0.46 * 224): # out of the image aperture\n",
    "                    heatmaps[k, i, j] = 0\n",
    "scipy.io.savemat(folder_path + \"Fig4/layer41/heatmaps.mat\", {'heatmaps': heatmaps}) # save the heatmaps to mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the estimated 2D Gaussian fit center and std for layer4.1, the IT layer\n",
    "xs = scipy.io.loadmat(folder_path + \"Fig4/layer41/b1.mat\")[\"b1\"] # x-center, (512, 1)\n",
    "ys = scipy.io.loadmat(folder_path + \"Fig4/layer41/b2.mat\")[\"b2\"] # y-center, (512, 1)\n",
    "xs_std = scipy.io.loadmat(folder_path + \"Fig4/layer41/c1.mat\")[\"c1\"] # x-std, (512, 1)\n",
    "ys_std = scipy.io.loadmat(folder_path + \"Fig4/layer41/c2.mat\")[\"c2\"] # y-std, (512, 1)\n",
    "# visualize the heatmap, with the estimated 2D Gaussian fit center shown in a black dot\n",
    "fig, axes = plt.subplots(2, 16, figsize=(32, 4))\n",
    "l = 3\n",
    "for i in range(32):\n",
    "    ax = axes[i // 16, i % 16]\n",
    "    heatmap = heatmaps[i, :, :]\n",
    "    x, y = int(xs[i][0]), int(ys[i][0])\n",
    "    heatmap[x-l:x+l, y-l:y+l] = np.min(heatmap) # set the center to the minimum value\n",
    "    ax.imshow(heatmap, cmap=\"hot\")\n",
    "    ax.axis(\"off\")\n",
    "    ellipse = matplotlib.patches.Ellipse((y, x), width=2*ys_std[i][0], height=2*xs_std[i][0], edgecolor=\"cyan\", facecolor=\"none\", linewidth=2)\n",
    "    ax.add_patch(ellipse)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDANN unit response to partially Occlued/Preserved images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path: str, device: str):\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "    # drop the FC layer\n",
    "    model.fc = nn.Identity()\n",
    "    # load weights\n",
    "    ckpt = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    state_dict = ckpt[\"classy_state_dict\"][\"base_model\"][\"model\"][\"trunk\"]\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"base_model\") and \"fc.\" not in k:\n",
    "            remainder = k.split(\"base_model.\")[-1]\n",
    "            new_state_dict[remainder] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    # freeze all weights\n",
    "    for param in model.parameters(): param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "# Image Occlusion test\n",
    "layer = \"layer41\" # targeted layer, change for different layers\n",
    "imgs_0index = np.load(folder_path + \"Fig4/TDANNrsp_1kimg_0idx.npz\")[layer] # (num_units, 1000), 0-indexed top 100 images for each unit\n",
    "Sigma = h5py.File(folder_path + \"Fig4/\" + layer + \"/Sigma.mat\")[\"Sigma\"] # 2D gaussian fitted receptive field parameters\n",
    "heatmaps25 = np.load(folder_path + \"Fig4/\" + layer + \"/heatmaps.npy\") # (unit_num, 26, 224, 224)\n",
    "# load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "checkpoint_path = folder_path + \"Fig4/TDANNfinal.torch\"\n",
    "model = load_model_from_checkpoint(checkpoint_path, device)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "# parameters\n",
    "num_topimgs = 25 # use top 25 images to do ON and OFF response test\n",
    "num_partial_imgs = 200 # for each top image, number of synthesis images to be used for the partial image test\n",
    "step = 0.04 # step size for the Gaussian mask, maybe a bit small\n",
    "if layer == \"layer10\" or layer == \"layer11\":\n",
    "    unit_num = 64\n",
    "    feature_map_size = 56\n",
    "    xy = 28\n",
    "elif layer == \"layer20\" or layer == \"layer21\":\n",
    "    unit_num = 128\n",
    "    feature_map_size = 28\n",
    "    xy = 14\n",
    "elif layer == \"layer30\" or layer == \"layer31\":\n",
    "    unit_num = 256\n",
    "    feature_map_size = 14\n",
    "    xy = 7\n",
    "elif layer == \"layer40\" or layer == \"layer41\":\n",
    "    unit_num = 512\n",
    "    feature_map_size = 7\n",
    "    xy = 3\n",
    "OnOff_activation = np.zeros((unit_num, num_topimgs, num_partial_imgs * 2))\n",
    "# iterate through all channels' feature map center units\n",
    "for k in tqdm(range(unit_num), desc=\"iterating through units...\", disable=True):\n",
    "    try:\n",
    "        # central unit localization\n",
    "        flat_index = k*feature_map_size*feature_map_size + xy*feature_map_size + xy # 0-indexed unit index\n",
    "        depth_index = flat_index // (feature_map_size * feature_map_size)        # Get index along the first dimension\n",
    "        row_col_index = flat_index % (feature_map_size * feature_map_size)       # Remaining index within the 7x7 matrix\n",
    "        row_index = row_col_index // feature_map_size             # Get index along the second dimension (7)\n",
    "        col_index = row_col_index % feature_map_size             # Get index along the third dimension (7)\n",
    "        print(\"Currently targeting at unit: \", flat_index, (0, depth_index, row_index, col_index)) # unit index indicates its original position in the 3rd layer response tensor\n",
    "        # most preferred images for the current unit\n",
    "        index1 = (1 + imgs_0index[k, :]).astype(int) # 0-indexed to 1-indexed to match image file name, obtain the top 1000 most preferred images for the current unit\n",
    "        # 2D Gaussian fit Parameters\n",
    "        area = np.pi * Sigma[0, k] * Sigma[1, k]\n",
    "        sig = np.sqrt(Sigma[0, k] * Sigma[1, k]) / 2\n",
    "        for i in range(num_topimgs):\n",
    "            # sort heatmap values from large to small for the current image\n",
    "            heatmap = heatmaps25[k, i+1, :, :] # (224, 224), averaged heatmap before 25 top images in the 2nd dim\n",
    "            heatmap = np.reshape(heatmap, (224 * 224,))\n",
    "            heatmap = np.argsort(-heatmap)\n",
    "            # space to store the synthesized images\n",
    "            imgs_on = np.zeros((num_partial_imgs, 224, 224, 3))\n",
    "            imgs_off = np.zeros((num_partial_imgs, 224, 224, 3))\n",
    "            # generate preserved and occluded images with an aperture\n",
    "            image = np.array(Image.open(folder_path + \"50K_Imgset/\" + str(index1[i]) + \".bmp\"))\n",
    "            for j in range(num_partial_imgs):\n",
    "                mask = np.zeros((224*224,))\n",
    "                mask[heatmap[:int(step*j*area)]] = 1\n",
    "                mask1 = np.repeat(np.reshape(mask, (224,224,1)),3,axis=2)\n",
    "                mask2 = cv2.GaussianBlur(mask1,(0,0),2.0)\n",
    "                # ON\n",
    "                img = image.copy()\n",
    "                img = resize(img, (224, 224, 3), anti_aliasing=True)\n",
    "                im3 = img - cv2.GaussianBlur(img, (0, 0), sig)\n",
    "                img[mask1 < 1] = 0.5\n",
    "                imgs_on[j, :, :, :] = cv2.GaussianBlur(img, (0, 0), sig) + im3 * mask2 # top-k preserved images\n",
    "                # OFF\n",
    "                img = image.copy()\n",
    "                img = resize(img, (224, 224, 3), anti_aliasing=True)\n",
    "                im3 = img - cv2.GaussianBlur(img, (0, 0), sig)\n",
    "                img[mask1 > 0] = 0.5\n",
    "                imgs_off[j, :, :, :] = cv2.GaussianBlur(img, (0, 0), sig) + im3 * (1 - mask2) # top-k removed images\n",
    "            # create image batch from shape (num_partial_imgs * 2, 224, 224, 3) to (num_partial_imgs * 2, 3, 224, 224) as a torch tensor input\n",
    "            imgs_batch = torch.tensor(np.concatenate((imgs_on, imgs_off), axis=0).astype(np.float32), device=device).permute(0, 3, 1, 2)\n",
    "            # model forward pass, obtain activation values of the target unit\n",
    "            activation = None # Placeholder for the activation\n",
    "            def hook_fn(module, input, output): # Define the hook function to capture the output\n",
    "                global activation\n",
    "                activation = output\n",
    "            # targeting at different layer\n",
    "            if layer == \"layer10\":\n",
    "                target_layer = model.layer1[0]\n",
    "            elif layer == \"layer11\":\n",
    "                target_layer = model.layer1[1]\n",
    "            elif layer == \"layer20\":\n",
    "                target_layer = model.layer2[0]\n",
    "            elif layer == \"layer21\":\n",
    "                target_layer = model.layer2[1]\n",
    "            elif layer == \"layer30\":\n",
    "                target_layer = model.layer3[0]\n",
    "            elif layer == \"layer31\":\n",
    "                target_layer = model.layer3[1]\n",
    "            elif layer == \"layer40\":\n",
    "                target_layer = model.layer4[0]\n",
    "            elif layer == \"layer41\":\n",
    "                target_layer = model.layer4[1]\n",
    "            hook = target_layer.register_forward_hook(hook_fn)\n",
    "            with torch.no_grad(): model(imgs_batch)\n",
    "            OnOff_activation[k, i, :] = activation[:, depth_index, row_index, col_index].detach().cpu().numpy()  # obtain the activations of the target unit\n",
    "            hook.remove()\n",
    "    except Exception as e: print(\"error message at k =\", k, e)\n",
    "    break # remove this break to run through all units\n",
    "# save the final results\n",
    "np.save(folder_path + \"Fig4/OnOff41.npy\", OnOff_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature dispersity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the feature dispersity value for V4 digital twin\n",
    "# Step 1: Define constants\n",
    "pnum = 3048\n",
    "snum = 25 # Number of image samples\n",
    "num_partial_imgs = 200 # Number of partial images\n",
    "step = 0.02 # Step size for the Gaussian mask\n",
    "S0 = 2 * np.log(2) / step # Constant for normalization\n",
    "# On, Off responses\n",
    "# Load MATLAB .mat files\n",
    "MRspS = scipy.io.loadmat(\"/Users/dunhan/Desktop/topoV4/V4DT/Dispersity_results/MRspS.mat\")['MRspS'] # OFF responses, Shape: (3048, 25, 200)\n",
    "TRspS = scipy.io.loadmat(\"/Users/dunhan/Desktop/topoV4/V4DT/Dispersity_results/TRspS.mat\")['TRspS'] # ON responses, Shape: (3048, 25, 200)\n",
    "ROI = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128) <class 'numpy.ndarray'>\n",
    "# Step 2: Initialize ERsp\n",
    "ERsp = np.zeros((2, pnum, snum, num_partial_imgs)) # Shape: (2, pnum, 25, 200)\n",
    "# Step 3: Normalize MRspS and TRspS\n",
    "for i in range(pnum):\n",
    "    for j in range(snum):\n",
    "        tmp1 = MRspS[i, j, :].squeeze()\n",
    "        tmp2 = TRspS[i, j, :].squeeze()\n",
    "        ERsp[0, i, j, :] = tmp1 / tmp1[0] # Normalize MRspS\n",
    "        ERsp[1, i, j, :] = tmp2 / tmp1[0] # Normalize TRspS\n",
    "# Step 4: Compute mean across samples (axis=2)\n",
    "MRsp = np.mean(ERsp, axis=2)  # Shape: (2, pnum, 200)\n",
    "# Step 5: Compute TRsp\n",
    "TRsp = np.zeros(pnum) # Raw feature dispersity values\n",
    "for ci in range(pnum):\n",
    "    tmp1 = MRsp[0, ci, :] - MRsp[1, ci, :]\n",
    "    tmp2 = tmp1 > 0\n",
    "    tmp3 = tmp1 < 0\n",
    "    tmp4 = tmp2[:-1] * tmp3[1:]  # Detect zero-crossings\n",
    "    tmp5 = np.where(tmp4 > 0)[0]  # Find crossing indices\n",
    "    if tmp5.size == 0: TRsp[ci] = (num_partial_imgs - 1) / S0\n",
    "    else:\n",
    "        ID = tmp5[0]\n",
    "        tmp6 = abs(tmp1[ID]) + abs(tmp1[ID + 1])\n",
    "        tmp7 = ID + abs(tmp1[ID]) / tmp6 - 1\n",
    "        TRsp[ci] = tmp7 / S0\n",
    "\n",
    "FD = np.zeros((128, 128)) # Feature dispersity map\n",
    "count = 0\n",
    "for i in range(128):\n",
    "    for j in range(128):\n",
    "        if ROI[i, j] == 1:\n",
    "            FD[i, j] = TRsp[count]\n",
    "            count += 1\n",
    "assert count == 3048 # Ensure all units are processed\n",
    "# Normalize FD and map to 100 levels\n",
    "Min, Max = 0.0, 1.0\n",
    "Imap = np.floor((FD - Min) / (Max - Min) * 100).astype(int) # (128, 128)\n",
    "Imap = np.clip(Imap, 1, 100) # (128, 128), Clamping values to [1, 100]\n",
    "# Get Parula colormap with 100 levels\n",
    "colA = scipy.io.loadmat(folder_path + \"V4DT/Dispersity_results/color_scheme.mat\")[\"colA\"] # (100, 3)\n",
    "cmap = np.zeros((128, 128, 3)) # Create an empty colormap image (initialized as black)\n",
    "mask = ROI == 1 # masked ROI region\n",
    "cmap[mask] = colA[Imap[mask] - 1]  # -1 because Python indexing starts at 0\n",
    "\n",
    "# Display the final colormap\n",
    "plt.imshow(cmap)\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "plt.title(\"Colormap with ROI Mask\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the feature dispersity value for TDANN layers\n",
    "LAYERS = [\"layer31\", \"layer40\"]\n",
    "for layer in LAYERS:\n",
    "    # Step 0: Load the OnOff activation values\n",
    "    OnOff_activation = np.load(folder_path + \"Fig4/\" + layer + \"/OnOff\" + str(layer[-2:]) + \".npy\") # (pnum, 25, 400)\n",
    "    # Step 1: Define constants\n",
    "    pnum = OnOff_activation.shape[0] # 256 for layer3.1 (V4), 512 for layer4.0 and layer4.1 (IT)\n",
    "    snum = 25 # Number of image samples\n",
    "    num_partial_imgs = 200 # Number of partial images\n",
    "    step = 0.04 # Step size for the Gaussian mask\n",
    "    S0 = 2 * np.log(2) / step # Constant for normalization\n",
    "    # On, Off responses\n",
    "    TRspS = OnOff_activation[:, :, :num_partial_imgs] # ON responses, Shape: (pnum, 25, 200)\n",
    "    MRspS = OnOff_activation[:, :, num_partial_imgs:] # OFF responses, Shape: (pnum, 25, 200)\n",
    "    # Step 2: Initialize ERsp\n",
    "    ERsp = np.zeros((2, pnum, snum, num_partial_imgs)) # Shape: (2, pnum, 25, 200)\n",
    "    # Step 3: Normalize MRspS and TRspS\n",
    "    for i in range(pnum):\n",
    "        for j in range(snum):\n",
    "            tmp1 = MRspS[i, j, :].squeeze()\n",
    "            tmp2 = TRspS[i, j, :].squeeze()\n",
    "            ERsp[0, i, j, :] = tmp1 / tmp1[0] # Normalize MRspS\n",
    "            ERsp[1, i, j, :] = tmp2 / tmp1[0] # Normalize TRspS\n",
    "    # Step 4: Compute mean across samples (axis=2)\n",
    "    MRsp = np.mean(ERsp, axis=2)  # Shape: (2, pnum, 200)\n",
    "    # Step 5: Compute TRsp\n",
    "    TRsp = np.zeros(pnum) # Raw feature dispersity values\n",
    "    for ci in range(pnum):\n",
    "        tmp1 = MRsp[0, ci, :] - MRsp[1, ci, :]\n",
    "        tmp2 = tmp1 > 0\n",
    "        tmp3 = tmp1 < 0\n",
    "        tmp4 = tmp2[:-1] * tmp3[1:]  # Detect zero-crossings\n",
    "        tmp5 = np.where(tmp4 > 0)[0]  # Find crossing indices\n",
    "        if tmp5.size == 0: TRsp[ci] = (num_partial_imgs - 1) / S0\n",
    "        else:\n",
    "            ID = tmp5[0]\n",
    "            tmp6 = abs(tmp1[ID]) + abs(tmp1[ID + 1])\n",
    "            tmp7 = ID + abs(tmp1[ID]) / tmp6 - 1\n",
    "            TRsp[ci] = tmp7 / S0\n",
    "    # save the feature dispersity values\n",
    "    np.save(folder_path + \"Fig4/\" + layer + \"/FD.npy\", TRsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram visualization\n",
    "for i, layer in enumerate([\"V4DT\", \"layer31\", \"layer40\"]):\n",
    "    if layer == \"V4DT\":\n",
    "        FD = scipy.io.loadmat(folder_path + \"V4DT/Dispersity_results/FDraw.mat\")[\"FD\"]\n",
    "        roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128) <class 'numpy.ndarray'>\n",
    "        FD = FD[roi == 1] # (3048,)\n",
    "        x_vals_v4 = np.linspace(FD.min(), FD.max(), 100)  # Continuous range of values\n",
    "        shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(FD, floc=0)  # Force location to 0\n",
    "        gamma_pdf_v4 = scipy.stats.gamma.pdf(x_vals_v4, a=shape_hat, loc=loc_hat, scale=scale_hat)\n",
    "    else:\n",
    "        FD = scipy.io.loadmat(folder_path + \"V4DT/Dispersity_results/FDraw.mat\")[\"FD\"]\n",
    "        roi = np.load(folder_path + \"V4DT/ROI.npy\").T # (128, 128) <class 'numpy.ndarray'>\n",
    "        FD = FD[roi == 1] # (3048,)\n",
    "        x_vals_v4 = np.linspace(FD.min(), FD.max(), 100)  # Continuous range of values\n",
    "        shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(FD, floc=0)  # Force location to 0\n",
    "        gamma_pdf_v4 = scipy.stats.gamma.pdf(x_vals_v4, a=shape_hat, loc=loc_hat, scale=scale_hat)\n",
    "\n",
    "        FD = np.load(folder_path + \"Fig4/\" + layer + \"/FD.npy\")\n",
    "        rsquares = scipy.io.loadmat(folder_path + \"Fig4/\" + layer + \"/rsquares.mat\")[\"rsquares\"]\n",
    "        rsquares = rsquares.squeeze()\n",
    "        # FD = FD[rsquares >= 0.9]\n",
    "        FD = FD[FD > 0]\n",
    "    fs = 16\n",
    "    x_vals = np.linspace(FD.min(), FD.max(), 100)  # Continuous range of values\n",
    "    # Fit a Gamma distribution to the data\n",
    "    shape_hat, loc_hat, scale_hat = scipy.stats.gamma.fit(FD, floc=0)  # Force location to 0\n",
    "    # Compute the Gamma PDF values\n",
    "    gamma_pdf = scipy.stats.gamma.pdf(x_vals, a=shape_hat, loc=loc_hat, scale=scale_hat)\n",
    "    fig = plt.figure(figsize=(4, 3))\n",
    "    plt.hist(FD, bins=20, density=True, alpha=0.6, color='b', edgecolor='black')\n",
    "    if layer == \"layer31\":\n",
    "        plt.plot(x_vals, gamma_pdf, 'r-', linewidth=2, label=f'TDANN V4 layer')\n",
    "    elif layer == \"layer40\":\n",
    "        plt.plot(x_vals, gamma_pdf, 'r-', linewidth=2, label=f'TDANN ITC layer')\n",
    "    else: plt.plot(x_vals, gamma_pdf, 'r-', linewidth=2, label=f'{layer} gamma fit (k={shape_hat:.2f}, Î¸={scale_hat:.2f})')\n",
    "    gamma_pdf_v4 = gamma_pdf_v4 / np.sum(gamma_pdf_v4) * np.sum(gamma_pdf) # re-scale the V4DT gamma pdf to match the other pdf sum\n",
    "    plt.plot(x_vals_v4, gamma_pdf_v4, 'k-', linewidth=1, label=f'V4 digital twin')\n",
    "    plt.xlabel(\"Dispersity\", fontsize=fs)\n",
    "    plt.ylabel(\"Density\", fontsize=fs)\n",
    "    plt.yticks([])\n",
    "    plt.legend()\n",
    "    # save the figure\n",
    "    plt.tight_layout()\n",
    "    if layer == \"V4DT\": fig.savefig(folder_path + \"V4DT/FDhist.png\", dpi=300)\n",
    "    else: fig.savefig(folder_path + \"Fig4/\" + layer + \"/FDhist.png\", dpi=300)\n",
    "    plt.close()\n",
    "    del fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature dispersity map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature dispersity map visualization / scatter plot\n",
    "LAYERS = [\"layer31\", \"layer40\"]\n",
    "for layer in LAYERS:\n",
    "    positions = np.load(folder_path + \"Fig4/TDANNfinal_positions/layer\" + str(layer[-2:-1]) + \".\" + str(layer[-1]) + \".npz\")[\"coordinates\"] # (num_units, 2)\n",
    "    FD = np.load(folder_path + \"Fig4/\" + layer + \"/FD.npy\")\n",
    "    if layer == \"layer41\" or layer == \"layer40\":\n",
    "        num_units = 25088\n",
    "        size = 7\n",
    "        s = 3\n",
    "    elif layer == \"layer31\" or layer == \"layer30\":\n",
    "        num_units = 50176\n",
    "        size = 14\n",
    "        s = 1.5\n",
    "    FD_all = np.zeros((num_units))\n",
    "    for i in range(num_units):\n",
    "        depth_index = i // (size * size)\n",
    "        FD_all[i] = FD[depth_index]\n",
    "\n",
    "    # Normalize FD and map to 100 levels\n",
    "    Min, Max = 0, 3\n",
    "    Imap = np.floor((FD_all - Min) / (Max - Min) * 100).astype(int)\n",
    "    Imap = np.clip(Imap, 1, 100) # Clamping values to [1, 100]\n",
    "    # Get Parula colormap with 100 levels\n",
    "    colA = scipy.io.loadmat(folder_path + \"V4DT/Dispersity_results/color_scheme.mat\")[\"colA\"] # (100, 3)\n",
    "    colA = mcolors.ListedColormap(colA)\n",
    "\n",
    "    # Display the final colormap\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(positions[:, 1], positions[:, 0], c=Imap, s=s, cmap=colA, marker='o', edgecolors='none')\n",
    "    plt.gca().invert_yaxis() # Move origin to the top-left\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    # plt.title(\"Dispersity\", fontsize=22)\n",
    "    # save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(folder_path + \"Fig4/\" + layer + \"/FDscatter03.png\", dpi=1000)\n",
    "    plt.close()\n",
    "    del fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a map corresponding domain number, remove the unconnected components smaller than a threshold\n",
    "def connected_components(matched, threshold, return_largest=False):\n",
    "    # Define the structure (connectivity) for connected components\n",
    "    structure = np.ones((3, 3), dtype=int)  # 4-connectivity, meaning that both adjacent and diagonal pixels are considered as neighbors\n",
    "    structure[0, 0] = 0\n",
    "    structure[0, 2] = 0\n",
    "    structure[2, 0] = 0\n",
    "    structure[2, 2] = 0\n",
    "    connected = np.copy(matched) # Initialize an array to store the output\n",
    "    if return_largest: largest = np.copy(matched) # Initialize an array to store the largest connected component\n",
    "    for class_num in np.unique(matched): # Process each class separately\n",
    "        if class_num == 0: continue # Skip the background\n",
    "        class_mask = (matched == class_num) # Create a binary mask for the current class\n",
    "        labeled_array, num_features = label(class_mask, structure=structure) # Find connected components in the binary mask\n",
    "        component_sizes = ndi_sum(class_mask, labeled_array, index=range(1, num_features + 1)) # Compute the size of each component\n",
    "        for i, size in enumerate(component_sizes): # Zero out components smaller than the threshold\n",
    "            if size < threshold: connected[labeled_array == (i + 1)] = 0\n",
    "        if return_largest:\n",
    "            for i, size in enumerate(component_sizes):\n",
    "                if size != np.max(component_sizes): largest[labeled_array == (i + 1)] = 0\n",
    "    for i in range(1, matched.shape[0]-1):\n",
    "        for j in range(1, matched.shape[1]-1):\n",
    "            if connected[i, j] == 0:\n",
    "                if connected[i-1, j] == 1 and connected[i+1, j] == 1 and connected[i, j-1] == 1 and connected[i, j+1] == 1:\n",
    "                    connected[i, j] = 1\n",
    "    if return_largest: return connected, largest\n",
    "    else: return connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature dispersity map visualization / aggregated into 60 by 60 gridded colormap\n",
    "LAYERS = [\"layer31\", \"layer40\"]\n",
    "for layer in LAYERS:\n",
    "    positions = np.load(folder_path + \"Fig4/TDANNfinal_positions/layer\" + str(layer[-2:-1]) + \".\" + str(layer[-1]) + \".npz\")[\"coordinates\"] # (num_units, 2)\n",
    "    FD = np.load(folder_path + \"Fig4/\" + layer + \"/FD.npy\")\n",
    "    if layer == \"layer41\" or layer == \"layer40\":\n",
    "        num_units = 25088\n",
    "        size = 7\n",
    "        s = 3\n",
    "    elif layer == \"layer31\" or layer == \"layer30\":\n",
    "        num_units = 50176\n",
    "        size = 14\n",
    "        s = 1.5\n",
    "    FD_all = np.zeros((num_units))\n",
    "    for i in range(num_units):\n",
    "        depth_index = i // (size * size)\n",
    "        FD_all[i] = FD[depth_index]\n",
    "    grid_num = 60\n",
    "    FD_map = np.zeros((grid_num, grid_num))\n",
    "    ROI = np.zeros((grid_num, grid_num))\n",
    "    cortical_size = max(positions[:, 0]) - min(positions[:, 0]) # define the length of the 2D plane\n",
    "    for i in tqdm(range(grid_num), desc=\"map initialization...\", disable=True):\n",
    "        for j in range(grid_num):\n",
    "            # first find all units in this current grid\n",
    "            xmin_cortex = cortical_size / grid_num * i\n",
    "            xmax_cortex = cortical_size / grid_num * (i + 1)\n",
    "            ymin_cortex = cortical_size / grid_num * j\n",
    "            ymax_cortex = cortical_size / grid_num * (j + 1)\n",
    "            # find all units in this current grid\n",
    "            units_within_grid_indeices = np.where((positions[:, 0] >= xmin_cortex) & (positions[:, 0] < xmax_cortex) & (positions[:, 1] >= ymin_cortex) & (positions[:, 1] < ymax_cortex))[0]\n",
    "            if len(units_within_grid_indeices) > 0:\n",
    "                FD_map[i, j] = np.mean(FD_all[units_within_grid_indeices])\n",
    "                ROI[i, j] = 1\n",
    "    \n",
    "    # Hierarchical clustering, perform hierarchical clustering\n",
    "    # Create a mask to ignore zero values\n",
    "    nonzero_mask = FD_map > 0\n",
    "    nonzero_values = FD_map[nonzero_mask]  # Extract nonzero values\n",
    "    # Perform hierarchical clustering only on nonzero values\n",
    "    Z = linkage(nonzero_values.reshape(-1, 1), method='ward')\n",
    "    cluster_labels = fcluster(Z, t=2, criterion='maxclust')\n",
    "    # Map cluster labels back to original shape\n",
    "    clusters = np.zeros_like(FD_map, dtype=int)  # Default 0 (ignored values)\n",
    "    clusters[nonzero_mask] = cluster_labels  # Assign labels only to nonzero positions\n",
    "    # Identify the high-value cluster\n",
    "    cluster_1_mean = nonzero_values[cluster_labels == 1].mean()\n",
    "    cluster_2_mean = nonzero_values[cluster_labels == 2].mean()\n",
    "    high_dispersity_cluster = 1 if cluster_1_mean > cluster_2_mean else 2\n",
    "    # Assign 1 only to grids in the high-dispersity cluster\n",
    "    clusters[clusters != high_dispersity_cluster] = 0\n",
    "    clusters[clusters == high_dispersity_cluster] = 1  # Convert high-value cluster to 1\n",
    "    # Remove unconnected components smaller than a threshold\n",
    "    clusters = connected_components(clusters, 10, return_largest=False)\n",
    "    contours = find_contours(clusters, level=0.5)  # Extracts boundaries of 1 clusters\n",
    "\n",
    "    # Normalize FD and map to 100 levels\n",
    "    Min, Max = 0, 1\n",
    "    Imap = np.floor((FD_map - Min) / (Max - Min) * 100).astype(int)\n",
    "    Imap = np.clip(Imap, 1, 100) # Clamping values to [1, 100], values outside the interval are clipped to the interval edges\n",
    "    # Get Parula colormap with 100 levels\n",
    "    colA = scipy.io.loadmat(folder_path + \"V4DT/Dispersity_results/color_scheme.mat\")[\"colA\"] # (100, 3)\n",
    "    cmap = np.zeros((grid_num, grid_num, 3)) # Create an empty colormap image (initialized as black); from top left to bottom right: [vertical, horizontal, rgb]\n",
    "    mask = ROI == 1 # masked ROI region\n",
    "    mask_off = ROI == 0 # masked OFF region\n",
    "    cmap[mask] = colA[Imap[mask] - 1]  # -1 because Python indexing starts at 0\n",
    "    cmap[mask_off] = np.ones((3)) # OFF region is white\n",
    "\n",
    "    # Display the final colormap\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(cmap)\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.title(\"Dispersity\", fontsize=22)\n",
    "    # save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(folder_path + \"Fig4/\" + layer + \"/FDmap01.png\", dpi=300)\n",
    "    plt.close()\n",
    "    del fig\n",
    "\n",
    "    # scatter plot with contours overlay\n",
    "    image = Image.open(folder_path + \"Fig4/\" + layer + \"/FDscatter01.png\")\n",
    "    image = np.array(image)\n",
    "    image = image[300:3700, 300:3700, :] # Crop the image to focus on the scatter plot\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(image)\n",
    "    for contour in contours:\n",
    "        contour[:, 1] *= image.shape[1] / grid_num\n",
    "        contour[:, 0] *= image.shape[0] / grid_num\n",
    "        plt.plot(contour[:, 1], contour[:,0], color=\"darkgoldenrod\", lw=4)  # Draw the contours\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Dispersity\", fontsize=22)\n",
    "    # save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(folder_path + \"Fig4/\" + layer + \"/FDscatter01_contour.png\", dpi=300)\n",
    "    plt.close()\n",
    "    del fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topoV4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
